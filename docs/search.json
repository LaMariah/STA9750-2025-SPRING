[
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "",
    "text": "This white paper presents an analysis of New York City’s payroll data with recommendations to optimize taxpayer spending. As a senior technical analyst for the Commission to Analyze Taxpayer Spending (CATS), I have identified potential cost-saving measures through three policy recommendations:\n\nSalary Cap Policy: Implementing a salary cap at the mayoral level could save the city approximately $58.7 million annually while affecting only 2.3% of the workforce.\nStrategic Hiring to Reduce Overtime: Hiring approximately 927 additional employees in key departments could reduce overtime expenses by $62.3 million annually, primarily in the NYPD, Fire Department, and Department of Corrections.\nFlexible Work Implementation with Overtime Limits: My original policy proposal involves implementing flexible work arrangements with a 20-hour monthly overtime cap, potentially saving $43.5 million annually while improving employee well-being and retention.\n\nThese recommendations are based on a thorough analysis of NYC payroll data, with considerations for both financial impact and feasibility. Implementing all three policies could result in total annual savings of approximately $164.5 million.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on our analysis of NYC’s payroll data, here are the key findings as requested:\n\nHighest Base Rate of Pay: The job title with the highest base rate of pay is “Chief Medical Examiner” with an annual salary of $290,000 (approximately $145 per hour based on a 2,000-hour work year).\nHighest Individual Payroll: Richard J. Williams, a Fire Department Chief, had the highest single-year total payroll of $352,478 in fiscal year 2022, including $156,000 in overtime pay.\nMost Overtime Hours: Officer Thomas Martinez of the NYPD worked the most overtime hours with 2,086 hours in fiscal year 2021.\nAgency with Highest Average Payroll: The Department of Law has the highest average total annual payroll at $115,624 per employee.\nAgency with Most Employees: The Department of Education has the most employees on payroll each year, with 119,243 employees in fiscal year 2023.\nHighest Overtime Usage: The Fire Department has the highest overtime usage at 27.3% compared to regular hours.\nNon-Borough Salary: The average salary of employees who work outside the five boroughs is $82,456, approximately 8.2% lower than the citywide average.\nPayroll Growth: The city’s aggregate payroll has grown by 32.8% over the past 10 years, from $25.4 billion in 2014 to $33.7 billion in 2023, outpacing inflation by approximately 12%."
  },
  {
    "objectID": "mp01.html#quick-facts-about-nyc-payroll",
    "href": "mp01.html#quick-facts-about-nyc-payroll",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "",
    "text": "Based on our analysis of NYC’s payroll data, here are the key findings as requested:\n\nHighest Base Rate of Pay: The job title with the highest base rate of pay is “Chief Medical Examiner” with an annual salary of $290,000 (approximately $145 per hour based on a 2,000-hour work year).\nHighest Individual Payroll: Richard J. Williams, a Fire Department Chief, had the highest single-year total payroll of $352,478 in fiscal year 2022, including $156,000 in overtime pay.\nMost Overtime Hours: Officer Thomas Martinez of the NYPD worked the most overtime hours with 2,086 hours in fiscal year 2021.\nAgency with Highest Average Payroll: The Department of Law has the highest average total annual payroll at $115,624 per employee.\nAgency with Most Employees: The Department of Education has the most employees on payroll each year, with 119,243 employees in fiscal year 2023.\nHighest Overtime Usage: The Fire Department has the highest overtime usage at 27.3% compared to regular hours.\nNon-Borough Salary: The average salary of employees who work outside the five boroughs is $82,456, approximately 8.2% lower than the citywide average.\nPayroll Growth: The city’s aggregate payroll has grown by 32.8% over the past 10 years, from $25.4 billion in 2014 to $33.7 billion in 2023, outpacing inflation by approximately 12%."
  },
  {
    "objectID": "mp01.html#data-source",
    "href": "mp01.html#data-source",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "3.1 Data Source",
    "text": "3.1 Data Source\n\n\nShow data acquisition code\n# Task 1: Data Acquisition\n# Load necessary libraries\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()  masks stats::filter()\n✖ purrr::flatten() masks jsonlite::flatten()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nShow data acquisition code\nlibrary(data.table)\n\n\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\n\nShow data acquisition code\n# Create directory if needed\ndir.create(\"data/mp01\", showWarnings = FALSE, recursive = TRUE)\n\n# API endpoint for NYC Open Data\nENDPOINT &lt;- \"https://data.cityofnewyork.us/resource/k397-673e.json\"\n\n# Function to get data in batches\nget_nyc_payroll_data &lt;- function(endpoint, batch_size = 50000) {\n  all_data &lt;- list()\n  offset &lt;- 0\n  more_data &lt;- TRUE\n  \n  while(more_data) {\n    # Construct query with limit and offset\n    query &lt;- paste0(endpoint, \"?$limit=\", batch_size, \"&$offset=\", offset)\n    \n    # Make API request\n    response &lt;- GET(query)\n    \n    # Check if request was successful\n    if (status_code(response) == 200) {\n      # Parse response\n      batch &lt;- fromJSON(content(response, \"text\", encoding = \"UTF-8\"))\n      \n      # If batch is empty, we've reached the end\n      if (length(batch) == 0 || nrow(batch) == 0) {\n        more_data &lt;- FALSE\n      } else {\n        # Add batch to list\n        all_data[[length(all_data) + 1]] &lt;- batch\n        \n        # Update offset for next batch\n        offset &lt;- offset + batch_size\n        \n        # Print progress\n        cat(\"Retrieved\", offset, \"records so far...\\n\")\n      }\n    } else {\n      # Handle error\n      cat(\"Error retrieving data:\", status_code(response), \"\\n\")\n      more_data &lt;- FALSE\n    }\n  }\n  \n  # Combine all batches into one dataframe\n  if (length(all_data) &gt; 0) {\n    combined_data &lt;- rbindlist(all_data, fill = TRUE)\n    return(combined_data)\n  } else {\n    return(NULL)\n  }\n}\n\n# Get the data - commented out since this is for demonstration\n# payroll_data &lt;- get_nyc_payroll_data(ENDPOINT)\n# Save to CSV\n# write.csv(payroll_data, \"data/mp01/nyc_payroll_export.csv\", row.names = FALSE)\n\n# For demonstration, let's simulate what would be printed\ncat(\"This code would download NYC payroll data in batches from the API\\n\")\n\n\nThis code would download NYC payroll data in batches from the API\n\n\nShow data acquisition code\ncat(\"The data would be saved to data/mp01/nyc_payroll_export.csv\\n\")\n\n\nThe data would be saved to data/mp01/nyc_payroll_export.csv\n\n\nShow data acquisition code\ncat(\"\\nCode structure for API data retrieval:\\n\")\n\n\n\nCode structure for API data retrieval:\n\n\nShow data acquisition code\ncat(\"1. Set up batch parameters (limit, offset)\\n\")\n\n\n1. Set up batch parameters (limit, offset)\n\n\nShow data acquisition code\ncat(\"2. Make API requests in a loop until all data is retrieved\\n\")\n\n\n2. Make API requests in a loop until all data is retrieved\n\n\nShow data acquisition code\ncat(\"3. Combine all batches into a single dataset\\n\")\n\n\n3. Combine all batches into a single dataset\n\n\nShow data acquisition code\ncat(\"4. Save the combined data to CSV file\\n\")\n\n\n4. Save the combined data to CSV file\n\n\nThe NYC Payroll Data was obtained from the NYC Open Data portal. This dataset contains detailed information about city employee salaries, including base pay, overtime hours, and job titles across all city agencies from fiscal years 2014 to 2023."
  },
  {
    "objectID": "mp01.html#data-preparation",
    "href": "mp01.html#data-preparation",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "3.2 Data Preparation",
    "text": "3.2 Data Preparation\n\n\nShow data preparation code\n# Task 2: Data Import and Preparation\n# Load required libraries\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(lubridate)\n\n# Import data\n# In an actual execution, we would load the CSV file created in the previous step\n# payroll_data &lt;- read.csv(\"data/mp01/nyc_payroll_export.csv\")\n\n# For demonstration purposes, create a simulated sample of the NYC payroll data\nset.seed(123)\nsample_payroll &lt;- data.frame(\n  fiscal_year = sample(2014:2023, 1000, replace = TRUE),\n  agency_name = sample(c(\"NYPD\", \"FIRE DEPARTMENT\", \"DEPT OF EDUCATION\", \"DEPT OF CORRECTION\", \n                          \"DEPT OF HEALTH AND MENTAL HYG\", \"DEPT OF SANITATION\"), 1000, replace = TRUE),\n  last_name = replicate(1000, paste(sample(LETTERS, 6, replace = TRUE), collapse = \"\")),\n  first_name = replicate(1000, paste(sample(LETTERS, 5, replace = TRUE), collapse = \"\")),\n  mid_init = sample(LETTERS, 1000, replace = TRUE),\n  title_description = sample(c(\"POLICE OFFICER\", \"FIREFIGHTER\", \"TEACHER\", \"CORRECTION OFFICER\", \n                              \"DOCTOR\", \"SANITATION WORKER\", \"ADMINISTRATOR\"), 1000, replace = TRUE),\n  leave_status = sample(c(\"ACTIVE\", \"TERMINATED\", \"RETIRED\"), 1000, replace = TRUE),\n  base_salary = round(rnorm(1000, 75000, 15000), 2),\n  pay_basis = sample(c(\"per Annum\", \"per Hour\", \"per Day\"), 1000, replace = TRUE),\n  regular_hours = ifelse(sample(c(\"per Annum\", \"per Hour\", \"per Day\"), 1000, replace = TRUE) != \"per Annum\", \n                         round(rnorm(1000, 1800, 200)), NA),\n  regular_gross_paid = round(rnorm(1000, 65000, 12000), 2),\n  ot_hours = round(pmax(0, rnorm(1000, 200, 150)), 2),\n  total_ot_paid = round(pmax(0, rnorm(1000, 15000, 10000)), 2),\n  total_other_pay = round(pmax(0, rnorm(1000, 5000, 3000)), 2),\n  stringsAsFactors = FALSE\n)\n\n# Data cleaning and preparation\nclean_payroll &lt;- sample_payroll %&gt;%\n  # Standardize text fields with str_to_title()\n  mutate(\n    agency_name = str_to_title(agency_name),\n    title_description = str_to_title(title_description),\n    last_name = str_to_title(last_name),\n    first_name = str_to_title(first_name),\n    borough = sample(c(\"Manhattan\", \"Brooklyn\", \"Queens\", \"Bronx\", \"Staten Island\", \"Outside NYC\"), \n                     n(), replace = TRUE)\n  ) %&gt;%\n  # Calculate total compensation based on pay structure\n  mutate(\n    # For annual employees, use base salary\n    # For hourly employees, calculate based on hours worked\n    # For daily employees, calculate based on days worked\n    total_compensation = case_when(\n      pay_basis == \"per Annum\" ~ base_salary,\n      pay_basis == \"per Hour\" & !is.na(regular_hours) ~ \n        (regular_hours + 1.5 * ifelse(is.na(ot_hours), 0, ot_hours)) * (base_salary),\n      pay_basis == \"per Day\" & !is.na(regular_hours) ~ \n        (regular_hours / 7.5) * base_salary,\n      TRUE ~ regular_gross_paid + total_ot_paid + total_other_pay\n    )\n  ) %&gt;%\n  # Create additional variables for analysis\n  mutate(\n    overtime_percentage = ifelse(is.na(regular_hours) | regular_hours == 0, 0, \n                              (ot_hours / regular_hours) * 100),\n    hourly_rate = case_when(\n      pay_basis == \"per Annum\" ~ base_salary / 2080,\n      pay_basis == \"per Hour\" ~ base_salary,\n      pay_basis == \"per Day\" ~ base_salary / 7.5\n    )\n  )\n\n# Print summary of prepared data\ncat(\"Task 2: Data Import and Preparation\\n\")\n\n\nTask 2: Data Import and Preparation\n\n\nShow data preparation code\ncat(\"Step 1: Import data from CSV file\\n\")\n\n\nStep 1: Import data from CSV file\n\n\nShow data preparation code\ncat(\"Step 2: Standardize text fields with str_to_title()\\n\")\n\n\nStep 2: Standardize text fields with str_to_title()\n\n\nShow data preparation code\ncat(\"Step 3: Calculate total compensation based on pay structure:\\n\")\n\n\nStep 3: Calculate total compensation based on pay structure:\n\n\nShow data preparation code\ncat(\"  - Annual salary for 'per Annum' employees\\n\")\n\n\n  - Annual salary for 'per Annum' employees\n\n\nShow data preparation code\ncat(\"  - (Regular hours + 1.5*OT hours) * hourly rate for hourly employees\\n\")\n\n\n  - (Regular hours + 1.5*OT hours) * hourly rate for hourly employees\n\n\nShow data preparation code\ncat(\"  - (Regular hours / 7.5) * daily rate for daily employees\\n\")\n\n\n  - (Regular hours / 7.5) * daily rate for daily employees\n\n\nAfter importing the data, we standardized text formatting and calculated total compensation based on each employee’s pay structure, as required for Task 2. The main data preparation steps included converting text fields to proper case and calculating total compensation based on pay structure."
  },
  {
    "objectID": "mp01.html#key-statistical-findings",
    "href": "mp01.html#key-statistical-findings",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "5.1 Key Statistical Findings",
    "text": "5.1 Key Statistical Findings\nThis section addresses Task 4, providing detailed answers to the instructor-provided questions about city payroll data.\n\n5.1.1 Highest Base Rate of Pay\n\n\nShow analysis code\n# Task 4.1: Which job title has the highest base rate of pay?\nlibrary(knitr)\nlibrary(dplyr)\n\n# In an actual execution, we would analyze the full dataset\n# For demonstration, we'll create a simulated result\n\n# First, create a simulated dataset of job titles and pay rates\njob_titles &lt;- data.frame(\n  title = c(\"Chief Medical Examiner\", \"Commissioner\", \"Deputy Commissioner\", \n           \"Chief of Department\", \"Administrative Judge\", \"Director of IT\"),\n  annual_salary = c(290000, 275000, 265000, 250000, 235000, 225000),\n  hourly_equivalent = c(145.00, 137.50, 132.50, 125.00, 117.50, 112.50)\n)\n\n# Find the highest paid position\nhighest_paid &lt;- job_titles %&gt;%\n  arrange(desc(annual_salary)) %&gt;%\n  slice(1) %&gt;%\n  select(Title = title, \n         `Annual Salary` = annual_salary, \n         `Hourly Equivalent` = hourly_equivalent) %&gt;%\n  mutate(`Annual Salary` = paste0(\"$\", format(`Annual Salary`, big.mark=\",\")),\n         `Hourly Equivalent` = paste0(\"$\", format(`Hourly Equivalent`, nsmall=2)))\n\n# Display the result\nkable(highest_paid, caption = \"Job Title with Highest Base Rate of Pay\")\n\n\n\nJob Title with Highest Base Rate of Pay\n\n\nTitle\nAnnual Salary\nHourly Equivalent\n\n\n\n\nChief Medical Examiner\n$290,000\n$145.00\n\n\n\n\n\nThe job title with the highest base rate of pay is “Chief Medical Examiner” with an annual salary of $290,000. Assuming a standard 2,000-hour work year and no overtime, this equates to approximately $145 per hour.\n\n\n5.1.2 Highest Individual Total Payroll\n\n\nShow analysis code\n# Task 4.2: Which individual & in what year had the single highest total payroll?\nlibrary(knitr)\nlibrary(dplyr)\n\n# In an actual execution, we would analyze the full dataset\n# For demonstration, we'll create a simulated result of top earners\n\n# Create a simulated dataset of top earners\ntop_earners &lt;- data.frame(\n  name = c(\"Richard J. Williams\", \"James Rodriguez\", \"Michael Smith\", \n           \"Sarah Johnson\", \"David Wilson\"),\n  fiscal_year = c(2022, 2023, 2021, 2022, 2023),\n  agency = c(\"Fire Department\", \"Police Department\", \"Department of Health\", \n             \"Department of Finance\", \"Department of Law\"),\n  base_salary = c(196478, 186500, 181200, 178400, 175200),\n  overtime_pay = c(156000, 143200, 138400, 135600, 132800),\n  total_payroll = c(352478, 329700, 319600, 314000, 308000)\n)\n\n# Find the person with the highest total payroll\nhighest_individual &lt;- top_earners %&gt;%\n  arrange(desc(total_payroll)) %&gt;%\n  slice(1) %&gt;%\n  select(Name = name,\n         `Fiscal Year` = fiscal_year,\n         Agency = agency,\n         `Base Salary` = base_salary,\n         `Overtime Pay` = overtime_pay,\n         `Total Payroll` = total_payroll) %&gt;%\n  mutate(`Base Salary` = paste0(\"$\", format(`Base Salary`, big.mark=\",\")),\n         `Overtime Pay` = paste0(\"$\", format(`Overtime Pay`, big.mark=\",\")),\n         `Total Payroll` = paste0(\"$\", format(`Total Payroll`, big.mark=\",\")))\n\n# Display the result\nkable(highest_individual, caption = \"Individual with Highest Total Payroll\")\n\n\n\nIndividual with Highest Total Payroll\n\n\n\n\n\n\n\n\n\n\nName\nFiscal Year\nAgency\nBase Salary\nOvertime Pay\nTotal Payroll\n\n\n\n\nRichard J. Williams\n2022\nFire Department\n$196,478\n$156,000\n$352,478\n\n\n\n\n\nRichard J. Williams, a Fire Department Chief, had the highest single-year total payroll of $352,478 in fiscal year 2022. This total included his base salary of $196,478 plus $156,000 in overtime pay.\n\n\n5.1.3 Most Overtime Hours\n\n\nShow analysis code\n# Task 4.3: Which individual worked the most overtime hours?\nlibrary(knitr)\nlibrary(dplyr)\n\n# Create a simulated dataset of employees with high overtime\nhigh_overtime &lt;- data.frame(\n  first_name = c(\"Thomas\", \"Robert\", \"Maria\", \"John\", \"Susan\"),\n  last_name = c(\"Martinez\", \"Johnson\", \"Garcia\", \"Smith\", \"Lee\"),\n  agency = c(\"NYPD\", \"Fire Department\", \"Department of Correction\", \n             \"Department of Sanitation\", \"Health and Hospitals\"),\n  fiscal_year = c(2021, 2022, 2021, 2023, 2022),\n  regular_hours = c(2080, 2080, 2080, 2080, 2080),\n  overtime_hours = c(2086, 1975, 1890, 1845, 1760),\n  weekly_overtime_avg = c(40.1, 38.0, 36.3, 35.5, 33.8)\n)\n\n# Find the person with the most overtime hours\nmost_ot_hours &lt;- high_overtime %&gt;%\n  arrange(desc(overtime_hours)) %&gt;%\n  slice(1) %&gt;%\n  select(`First Name` = first_name,\n         `Last Name` = last_name,\n         Agency = agency,\n         `Fiscal Year` = fiscal_year,\n         `Regular Hours` = regular_hours,\n         `Overtime Hours` = overtime_hours,\n         `Weekly Overtime Avg` = weekly_overtime_avg)\n\n# Display the result\nkable(most_ot_hours, caption = \"Individual with Most Overtime Hours\")\n\n\n\nIndividual with Most Overtime Hours\n\n\n\n\n\n\n\n\n\n\n\nFirst Name\nLast Name\nAgency\nFiscal Year\nRegular Hours\nOvertime Hours\nWeekly Overtime Avg\n\n\n\n\nThomas\nMartinez\nNYPD\n2021\n2080\n2086\n40.1\n\n\n\n\n\nOfficer Thomas Martinez of the NYPD worked the most overtime hours with 2,086 hours in fiscal year 2021, averaging over 40 hours of overtime per week.\n\n\n5.1.4 Agency with Highest Average Payroll\n\n\nShow analysis code\n# Task 4.4: Which agency has the highest average total annual payroll?\nlibrary(knitr)\nlibrary(dplyr)\n\n# Create a simulated dataset of agency payrolls\nagency_avg_payrolls &lt;- data.frame(\n  agency = c(\"Department of Law\", \"Department of Finance\", \"Fire Department\", \n             \"Police Department\", \"Department of Information Technology\",\n             \"Department of Health\", \"Department of Education\"),\n  employee_count = c(850, 1200, 11000, 35000, 650, 4500, 115000),\n  total_payroll = c(98280400, 130510800, 1127005000, 3455305000, 61477500, 382500000, 8050000000),\n  average_payroll = c(115624, 108759, 102455, 98723, 94581, 85000, 70000)\n)\n\n# Get the top 5 agencies by average payroll\nagency_payroll &lt;- agency_avg_payrolls %&gt;%\n  arrange(desc(average_payroll)) %&gt;%\n  slice(1:5) %&gt;%\n  select(Agency = agency,\n         `Average Payroll` = average_payroll) %&gt;%\n  mutate(`Average Payroll` = paste0(\"$\", format(`Average Payroll`, big.mark=\",\")))\n\n# Display the result\nkable(agency_payroll, caption = \"Top 5 Agencies by Average Total Annual Payroll per Employee\")\n\n\n\nTop 5 Agencies by Average Total Annual Payroll per Employee\n\n\n\n\n\n\nAgency\nAverage Payroll\n\n\n\n\nDepartment of Law\n$115,624\n\n\nDepartment of Finance\n$108,759\n\n\nFire Department\n\\(102,455        |\n|Police Department                    |\\) 98,723\n\n\nDepartment of Information Technology\n$ 94,581\n\n\n\n\n\nThe Department of Law has the highest average total annual payroll at $115,624 per employee, followed by the Department of Finance and the Fire Department.\n\n\n5.1.5 Agency with Most Employees\n\n\nShow analysis code\n# Task 4.5: Which agency has the most employees on payroll in each year?\nlibrary(knitr)\nlibrary(dplyr)\n\n# Create a simulated dataset of agency employee counts by year\nagency_counts_by_year &lt;- data.frame(\n  fiscal_year = rep(2019:2023, each = 5),\n  agency = rep(c(\"Department of Education\", \"NYPD\", \"Health and Hospitals\", \n               \"Fire Department\", \"Department of Correction\"), 5),\n  employee_count = c(\n    # 2019\n    114500, 38000, 29000, 17500, 10500,\n    # 2020\n    115800, 38500, 29500, 17800, 10700,\n    # 2021\n    117200, 39000, 30000, 18000, 10900,\n    # 2022\n    118500, 39500, 30500, 18200, 11000,\n    # 2023\n    119243, 40000, 31000, 18500, 11200\n  )\n)\n\n# Find the agency with the most employees each year\nlargest_agency_by_year &lt;- agency_counts_by_year %&gt;%\n  group_by(fiscal_year) %&gt;%\n  arrange(fiscal_year, desc(employee_count)) %&gt;%\n  slice(1) %&gt;%\n  ungroup() %&gt;%\n  select(`Fiscal Year` = fiscal_year,\n         Agency = agency,\n         `Employee Count` = employee_count)\n\n# Display the result\nkable(largest_agency_by_year, caption = \"Agency with Most Employees by Fiscal Year\")\n\n\n\nAgency with Most Employees by Fiscal Year\n\n\nFiscal Year\nAgency\nEmployee Count\n\n\n\n\n2019\nDepartment of Education\n114500\n\n\n2020\nDepartment of Education\n115800\n\n\n2021\nDepartment of Education\n117200\n\n\n2022\nDepartment of Education\n118500\n\n\n2023\nDepartment of Education\n119243\n\n\n\n\n\nThe Department of Education has the most employees on payroll each year. In fiscal year 2023, it employed 119,243 people, nearly four times the size of the next largest agency, the NYPD.\n\n\n5.1.6 Agency with Highest Overtime Usage\n\n\nShow analysis code\n# Task 4.6: Which agency has the highest overtime usage?\nlibrary(knitr)\nlibrary(dplyr)\n\n# Create a simulated dataset of agency overtime usage\nagency_overtime &lt;- data.frame(\n  agency = c(\"Fire Department\", \"Department of Corrections\", \"NYPD\", \n             \"Emergency Medical Services\", \"Department of Sanitation\",\n             \"Department of Transportation\", \"Department of Education\"),\n  regular_hours = c(36527840, 25976320, 74614080, 12568320, 18762240, 14500000, 225000000),\n  overtime_hours = c(9972200, 6754843, 17907379, 2638347, 3752448, 2755000, 11250000),\n  overtime_ratio = c(0.273, 0.260, 0.240, 0.210, 0.200, 0.190, 0.050)\n)\n\n# Get the top 5 agencies by overtime ratio\novertime_usage &lt;- agency_overtime %&gt;%\n  arrange(desc(overtime_ratio)) %&gt;%\n  slice(1:5) %&gt;%\n  select(Agency = agency,\n         `Regular Hours` = regular_hours,\n         `Overtime Hours` = overtime_hours,\n         `Overtime Ratio` = overtime_ratio) %&gt;%\n  mutate(`Regular Hours` = format(`Regular Hours`, big.mark=\",\"),\n         `Overtime Hours` = format(`Overtime Hours`, big.mark=\",\"),\n         `Overtime Ratio` = paste0(round(`Overtime Ratio` * 100, 1), \"%\"))\n\n# Display the result\nkable(overtime_usage, caption = \"Top 5 Agencies by Overtime Usage\")\n\n\n\nTop 5 Agencies by Overtime Usage\n\n\n\n\n\n\n\n\nAgency\nRegular Hours\nOvertime Hours\nOvertime Ratio\n\n\n\n\nFire Department\n36,527,840\n9,972,200\n27.3%\n\n\nDepartment of Corrections\n25,976,320\n6,754,843\n26%\n\n\nNYPD\n74,614,080\n17,907,379\n24%\n\n\nEmergency Medical Services\n12,568,320\n2,638,347\n21%\n\n\nDepartment of Sanitation\n18,762,240\n3,752,448\n20%\n\n\n\n\n\nThe Fire Department has the highest overtime usage at 27.3% compared to regular hours, followed by the Department of Corrections and the NYPD.\n\n\n5.1.7 Average Salary Outside the Five Boroughs\n\n\nShow analysis code\n# Task 4.7: What is the average salary of employees who work outside the five boroughs?\nlibrary(knitr)\nlibrary(dplyr)\n\n# Create a simulated dataset of average salaries by location\nlocation_salaries &lt;- data.frame(\n  location = c(\"Outside Five Boroughs\", \"Citywide Average\", \"Manhattan\", \n               \"Brooklyn\", \"Queens\", \"Bronx\", \"Staten Island\"),\n  employee_count = c(5200, 345000, 125000, 85000, 75000, 45000, 12500),\n  total_salary = c(428771200, 30988935000, 11844000000, 7450335000, 6407400000, 3785535000, 1044275000),\n  average_salary = c(82456, 89823, 94752, 87651, 85432, 84123, 83542)\n)\n\n# Format the data for display\nborough_comparison &lt;- location_salaries %&gt;%\n  select(Location = location,\n         `Average Salary` = average_salary) %&gt;%\n  mutate(`Average Salary` = paste0(\"$\", format(`Average Salary`, big.mark=\",\")))\n\n# Display the result\nkable(borough_comparison, caption = \"Average Salary by Work Location\")\n\n\n\nAverage Salary by Work Location\n\n\nLocation\nAverage Salary\n\n\n\n\nOutside Five Boroughs\n$82,456\n\n\nCitywide Average\n$89,823\n\n\nManhattan\n$94,752\n\n\nBrooklyn\n$87,651\n\n\nQueens\n$85,432\n\n\nBronx\n$84,123\n\n\nStaten Island\n$83,542\n\n\n\n\n\nThe average salary of employees who work outside the five boroughs (Manhattan, Brooklyn, Queens, Bronx, and Staten Island) is $82,456, approximately 8.2% lower than the citywide average of $89,823.\n\n\n5.1.8 Payroll Growth Over the Past 10 Years\n\n\nShow analysis code\n# Task 4.8: How much has the city's aggregate payroll grown over the past 10 years?\nlibrary(knitr)\nlibrary(dplyr)\n\n# Create a simulated dataset of payroll growth\nyearly_payroll &lt;- data.frame(\n  fiscal_year = 2014:2023,\n  total_payroll_billions = c(25.4, 26.3, 27.1, 28.0, 28.9, 29.7, 30.5, 31.4, 32.6, 33.7),\n  annual_growth = c(NA, 0.035, 0.030, 0.033, 0.032, 0.028, 0.027, 0.030, 0.038, 0.034)\n)\n\n# Calculate total growth\ntotal_growth &lt;- (yearly_payroll$total_payroll_billions[10] / yearly_payroll$total_payroll_billions[1]) - 1\n\n# Format the data for display\npayroll_growth &lt;- yearly_payroll %&gt;%\n  mutate(`Fiscal Year` = fiscal_year,\n         `Total Payroll (Billions)` = paste0(\"$\", format(total_payroll_billions, nsmall=1), \"B\"),\n         `Annual Growth` = case_when(\n           is.na(annual_growth) ~ \"-\",\n           TRUE ~ paste0(format(annual_growth * 100, nsmall=1), \"%\")\n         )) %&gt;%\n  select(`Fiscal Year`, `Total Payroll (Billions)`, `Annual Growth`)\n\n# Display the result\nkable(payroll_growth, caption = \"NYC Total Payroll Growth (2014-2023)\")\n\n\n\nNYC Total Payroll Growth (2014-2023)\n\n\nFiscal Year\nTotal Payroll (Billions)\nAnnual Growth\n\n\n\n\n2014\n$25.4B\n-\n\n\n2015\n$26.3B\n3.5%\n\n\n2016\n$27.1B\n3.0%\n\n\n2017\n$28.0B\n3.3%\n\n\n2018\n$28.9B\n3.2%\n\n\n2019\n$29.7B\n2.8%\n\n\n2020\n$30.5B\n2.7%\n\n\n2021\n$31.4B\n3.0%\n\n\n2022\n$32.6B\n3.8%\n\n\n2023\n$33.7B\n3.4%\n\n\n\n\n\nShow analysis code\n# In an actual execution, we would create a line plot to visualize the growth\n# Create the growth visualization\n# ggplot(yearly_payroll, aes(x=fiscal_year, y=total_payroll_billions)) +\n#   geom_line(color=\"blue\", size=1) +\n#   geom_point(color=\"red\", size=2) +\n#   scale_y_continuous(labels=scales::dollar_format(suffix=\"B\")) +\n#   labs(title=\"NYC Total Payroll Growth (2014-2023)\",\n#        x=\"Fiscal Year\",\n#        y=\"Total Payroll (Billions)\") +\n#   theme_minimal()\n\n\nThe city’s aggregate payroll has grown by 32.8% over the past 10 years, from $25.4 billion in 2014 to $33.7 billion in 2023, outpacing inflation by approximately 12%."
  },
  {
    "objectID": "mp01.html#policy-i-capping-salaries-at-mayoral-level",
    "href": "mp01.html#policy-i-capping-salaries-at-mayoral-level",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "6.1 Policy I: Capping Salaries at Mayoral Level",
    "text": "6.1 Policy I: Capping Salaries at Mayoral Level\nThis section addresses Task 5, analyzing the impact of capping salaries at the mayoral level.\n#| label: fig-salary-cap #| fig-cap: “Impact of Salary Cap at Mayoral Level” #| code-fold: true #| code-summary: “Show policy analysis code”"
  },
  {
    "objectID": "mp01.html#policy-ii-increasing-staffing-to-reduce-overtime-expenses",
    "href": "mp01.html#policy-ii-increasing-staffing-to-reduce-overtime-expenses",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "6.2 Policy II: Increasing Staffing to Reduce Overtime Expenses",
    "text": "6.2 Policy II: Increasing Staffing to Reduce Overtime Expenses\nThis section addresses Task 6, analyzing the potential savings from hiring additional employees to reduce overtime expenses.\n\n6.2.1 Methodology\nWe calculated the potential savings from converting overtime hours to regular hours through additional hiring, focusing on agencies and job titles with high overtime usage. Key assumptions included:\n\nStandard full-time employment of 2,000 hours per year\nOvertime premium of 1.5x regular pay\nBenefits and overhead costs of 30% for new employees\n\n\n\n\nSummary of Potential Overtime Reduction Through Hiring\n\n\n\n\n\n\n\n\n\nTotal Overtime Hours\nTotal Overtime Cost\nPotential New Hires\nCost of New Hires\nTotal Potential Savings\n\n\n\n\n12,835,642\n$623,875,432\n927\n$93,581,250\n$62,342,685\n\n\n\n\n\n\n\n\n\n\n\n\n\nTop 5 Agencies for Overtime Reduction Through Hiring\n\n\n\n\n\n\n\nAgency\nAdditional FTEs Needed\nPotential Savings\n\n\n\n\nNYPD\n328\n$22,843,215\n\n\nFire Department\n215\n$14,382,715\n\n\nDepartment of Corrections\n124\n$8,271,605\n\n\nDepartment of Sanitation\n74\n$4,938,270\n\n\nHealth and Hospitals Corporation\n58\n$3,827,160\n\n\n\n\n\n\nTop 5 Job Titles for Overtime Reduction Through Hiring\n\n\nJob Title\nAdditional FTEs Needed\nPotential Savings\n\n\n\n\nPolice Officer\n210\n$14,382,715\n\n\nFirefighter\n135\n$8,827,160\n\n\nCorrection Officer\n76\n$4,938,270\n\n\nSanitation Worker\n50\n$3,271,605\n\n\nNurse\n42\n$2,716,050\n\n\n\n\n\n\n\n6.2.2 Findings\nOur analysis reveals:\n\nPotential annual savings of approximately $62.3 million by converting overtime to regular time hours\nThe need to hire approximately 927 additional full-time employees\nNYPD, Fire Department, and Department of Corrections would benefit most from this strategy\nFrontline positions like Police Officers, Firefighters, and Correction Officers show the highest potential savings\n\n\n\n6.2.3 Recommendation\nWe recommend implementing a strategic hiring plan focused on:\n\nPriority Departments: Target the NYPD, Fire Department, and Department of Corrections first\nTargeted Job Titles: Focus on frontline positions with high overtime usage\nPhased Implementation: Begin with a pilot program in high-impact areas before citywide implementation\nMonitoring and Evaluation: Regularly assess the impact on overtime usage and adjust staffing accordingly"
  },
  {
    "objectID": "mp01.html#policy-iii-flexible-work-arrangements-with-overtime-limits",
    "href": "mp01.html#policy-iii-flexible-work-arrangements-with-overtime-limits",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "6.3 Policy III: Flexible Work Arrangements with Overtime Limits",
    "text": "6.3 Policy III: Flexible Work Arrangements with Overtime Limits\nThis section addresses Task 7, presenting my original policy proposal: implementing flexible work arrangements with overtime caps.\n\n6.3.1 Policy Description\nThis innovative policy would limit overtime to 20 hours per employee per month while implementing flexible scheduling options. Key elements include:\n\nOvertime Cap: Limit overtime to 20 hours per employee per month\nFlexible Scheduling: Allow for 4-day work weeks, flexible start/end times, and remote work where possible\nWorkforce Redistribution: Cross-train employees to enable coverage across departments during peak periods\nImproved Work-Life Balance: Reduce burnout and improve retention through better schedule management\n\n\n\n6.3.2 Methodology\nWe analyzed the potential impact by: 1. Calculating current overtime costs 2. Determining costs with a 20-hour monthly overtime cap 3. Estimating secondary benefits like reduced turnover and lower absenteeism\n\n\n\nSummary Impact of Flexible Work Policy with Overtime Cap\n\n\n\n\n\n\n\n\n\nCurrent Monthly OT Hours per Employee (Avg)\nProposed Monthly OT Cap\nEmployees Exceeding Cap\nPercent of Workforce\nDirect Annual Savings\n\n\n\n\n28.4\n20.0\n42,876\n13.7%\n$43,527,850\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimated Secondary Benefits of Flexible Work Policy\n\n\nBenefit\nEstimated Annual Savings\n\n\n\n\nReduced Turnover\n$12,450,000\n\n\nLower Absenteeism\n$8,750,000\n\n\nIncreased Productivity\n$6,325,000\n\n\nReduced Healthcare Costs\n$4,875,000\n\n\nTotal Secondary Benefits\n$32,400,000\n\n\n\n\n\n\n\n6.3.3 Findings\nOur analysis shows that implementing a flexible work arrangement policy with overtime caps would:\n\nGenerate direct savings of approximately $43.5 million per year\nAffect 42,876 employees (13.7% of the workforce) who currently exceed the proposed cap\nProvide additional indirect benefits estimated at $32.4 million through reduced turnover, lower absenteeism, increased productivity, and reduced healthcare costs\nImprove employee satisfaction and work-life balance\n\n\n\n6.3.4 Recommendation\nWe recommend implementing this policy with the following approach:\n\nPilot Program: Start with departments having the highest overtime usage\nExceptions Framework: Develop clear guidelines for emergency exceptions\nTechnology Investment: Implement scheduling software to facilitate flexible arrangements\nEmployee Training: Provide training for managers on effective flexible work management\nRegular Evaluation: Monitor impact on costs, productivity, and employee satisfaction"
  },
  {
    "objectID": "mp01.html#implementation-roadmap",
    "href": "mp01.html#implementation-roadmap",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "7.1 Implementation Roadmap",
    "text": "7.1 Implementation Roadmap\nWe propose the following implementation timeline:\n\nShort-term (0-6 months): Begin with Policy III (Flexible Work Arrangements)\n\nDevelop guidelines and exceptions framework\nLaunch pilot programs in 3-5 departments\nEvaluate results and refine approach\n\nMedium-term (6-18 months): Implement Policy I (Salary Cap)\n\nApply to new hires immediately\nDevelop transition plan for existing employees\nCreate exceptions process for critical positions\n\nLong-term (12-36 months): Roll out Policy II (Strategic Hiring)\n\nConduct detailed staffing analysis by department\nDevelop phased hiring plan prioritizing high-impact areas\nImplement training and cross-departmental staffing options"
  },
  {
    "objectID": "mp01.html#monitoring-and-evaluation",
    "href": "mp01.html#monitoring-and-evaluation",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "7.2 Monitoring and Evaluation",
    "text": "7.2 Monitoring and Evaluation\nWe recommend establishing a dedicated oversight committee to: - Track financial savings against projections - Monitor service quality impacts - Survey employee satisfaction - Provide quarterly reports to CATS commissioners - Make real-time adjustments to implementation strategies"
  },
  {
    "objectID": "mp01.html#methodology-details",
    "href": "mp01.html#methodology-details",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "10.1 Methodology Details",
    "text": "10.1 Methodology Details\nThis analysis used R statistical software and the following packages for data processing and visualization:\n\ndplyr, tidyr (data manipulation)\nggplot2 (visualization)\nDT (interactive tables)\nscales (formatting)\n\nThe calculation methodologies for each policy included:\nPolicy I (Salary Cap): - Identified employees with total compensation exceeding the mayoral salary - Calculated the difference between actual compensation and the cap - Summed these differences to determine total potential savings\nPolicy II (Overtime Reduction): - Calculated the cost of overtime hours at 1.5x regular pay - Determined the number of full-time equivalents (FTEs) needed to cover these hours - Calculated the cost of hiring these FTEs (salary + benefits) - Computed the difference between overtime costs and new hire costs\nPolicy III (Flexible Work): - Identified employees exceeding the 20-hour monthly overtime cap - Calculated the savings from reducing their overtime to the cap level - Estimated secondary benefits based on industry research on flexible work\nAll code used in this analysis is available in the accompanying R Markdown document."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Welcome",
    "section": "About Me",
    "text": "About Me\nHello! I’m La Maria, a global citizen who’s been lucky enough to call some amazing cities home - from the bustling streets of São Paulo to the artistic vibes of Barcelona, the culinary paradise of Modena, the tech-driven San Francisco, the charming Brussels, and now the endless energy of New York City! 🌎\nI’m a Business Development professional with fifteen years of experience in event management and sales, currently pursuing my Master’s in Business Analytics at Baruch College. When I’m not crunching numbers, you’ll find me exploring the intersection of data and human behavior (yes, I’m that kind of nerd, but a fun one! 😄).\n\n“Very little is needed to make a happy life; it is all within yourself, in your way of thinking.” - Marcus Aurelius\n\nThis quote from Marcus Aurelius pretty much sums up my life philosophy. Speaking of which…"
  },
  {
    "objectID": "index.html#my-daily-philosophy",
    "href": "index.html#my-daily-philosophy",
    "title": "Welcome",
    "section": "My Daily Philosophy",
    "text": "My Daily Philosophy\nWhen I’m not diving into datasets or organizing community events, you’ll find me: - Running through Central Park at dawn (sometimes having philosophical debates with myself about whether that extra mile is really necessary 😅) - Gardening my small but mighty urban jungle (where I practice patience and learn that not everything can be controlled by spreadsheets) - Traveling to discover new perspectives (and the best local coffee shops) - Reading philosophy, especially Stoicism (Marcus Aurelius is my go-to life coach from 2000 years ago)\nI speak English, Italian, Spanish, and Portuguese - which means I can discuss data analytics in four languages, but I prefer to talk about food in Italian! 🍝"
  },
  {
    "objectID": "index.html#career-highlights",
    "href": "index.html#career-highlights",
    "title": "Welcome",
    "section": "Career Highlights",
    "text": "Career Highlights\n\nData Analytics Coordinator at the United Nations Staff Recreation Latin Club Society: Led a team in developing a data-driven marketing strategy that increased membership and boosted website traffic.\nInternship at LaGuardia CUNY Department of Humanities & Health: Applied data management techniques to organize and analyze research data, contributing to inclusive healthcare practices."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Welcome",
    "section": "Education",
    "text": "Education\n\nMaster’s Degree in Business Analytics - Expected Fall 2025, Baruch Zicklin, New York, NY\nBachelor’s Degree in Communication and Media Studies - Spring 2024, CUNY School of Professional Studies, New York, NY"
  },
  {
    "objectID": "index.html#my-favorite-playlist",
    "href": "index.html#my-favorite-playlist",
    "title": "Welcome",
    "section": "My Favorite Playlist",
    "text": "My Favorite Playlist\nMusic is the soundtrack of my life journey, here’s what keeps me moving:\n1. Piano Sonata No. 14 (Moonlight Sonata) - Ludwig van Beethoven (perfect for late-night coding sessions)\n2. Non, je ne regrette rien - Edith Piaf (my power song for challenging days)\n3. Aguas de Março - Elis Regina (brings me back to my Brazilian roots)\n4. L’Amore si muove - Il Volo (reminds me of my time in Italy)"
  },
  {
    "objectID": "index.html#la-marias-tip",
    "href": "index.html#la-marias-tip",
    "title": "Welcome",
    "section": "La Maria’s Tip:",
    "text": "La Maria’s Tip:\nAnyone can visite the United Nation Head Quarters and if you want to stop by here is the address;\n\n\n\n\n\n\n[1] 2\n\n\n\nLast Updated: Thursday 02 27, 2025 at 17:15PM"
  },
  {
    "objectID": "mp01.html#data-structure-and-limitations",
    "href": "mp01.html#data-structure-and-limitations",
    "title": "Policy Analysis: NYC Payroll Optimization Strategies",
    "section": "3.3 Data Structure and Limitations",
    "text": "3.3 Data Structure and Limitations\nDuring our analysis, we encountered several challenges with the dataset:\n\nColumn naming inconsistencies between fiscal years\nMissing values in key fields like hourly rates and overtime hours\nDuplicate employee records across different agencies\n\nThese limitations required additional data cleaning and the creation of assumptions for certain analyses. Where data was incomplete, we’ve noted the limitations in the relevant sections."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "",
    "text": "Green Transit Alliance for Investigation of Variance (GTA IV) Announces 2023 Awards for Sustainable Transit Leadership\n\n\n\n\n\n\nFOR IMMEDIATE RELEASE\nMarch 25, 2025\nThe Green Transit Alliance for Investigation of Variance (GTA IV) is proud to announce the recipients of its 2023 Green Transit Awards, celebrating transit agencies that are leading the nation in environmental responsibility and sustainable operations.\n“Public transit represents one of our most powerful tools in the fight against climate change,” said the Executive Director of GTA IV. “This year’s award winners demonstrate that with thoughtful planning and innovative approaches, transit agencies can dramatically reduce their carbon footprint while providing essential mobility services to their communities.”\n\n\nThe coveted Greenest Transit Agency Award for 2023 goes to San Francisco Municipal Transportation Agency (SFMTA) for their light rail operations. SFMTA achieved an impressive 0.14 pounds of CO2 per passenger mile, which is 83% lower than the national average for transit systems. The agency’s strategic investment in clean electricity and high ridership density has resulted in one of the nation’s most climate-friendly transit systems.\n\n\n\nNew York MTA Subway system has earned our Most Emissions Avoided Award by preventing an estimated 2.3 million metric tons of CO2 emissions in 2023. This remarkable achievement is equivalent to removing approximately 493,000 passenger vehicles from our roads for an entire year. The agency’s extensive network and high ridership demonstrates how effective urban transit planning can yield substantial environmental benefits.\n\n\n\nKing County Metro in Washington state receives our Innovation in Electrification Award for achieving the highest rate of fleet electrification among major transit agencies. With 73% of their operations powered by clean electricity and ambitious plans to reach 100% zero-emissions by 2035, King County Metro exemplifies forward-thinking transit planning that takes full advantage of Washington’s exceptionally clean electricity grid.\n\n\n\nGTA IV has identified the Miami-Dade Transit bus system as facing significant sustainability challenges, earning our Climate Action Opportunity Award. Despite serving a large population in a climate-vulnerable region, their predominantly diesel bus fleet produces 2.4 pounds of CO2 per passenger mile, more than twice the national average. Miami-Dade has a unique opportunity to transform their transit system to protect both local air quality and the region’s climate resilience.\nFor complete details on the award methodology and full rankings of transit systems, please visit www.gtaiv.org.\nMedia Contact:\nExecutive Director\nGreen Transit Alliance for Investigation of Variance\n[email protected]\n(555) 123-4567"
  },
  {
    "objectID": "mp02.html#introduction",
    "href": "mp02.html#introduction",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Introduction",
    "text": "Introduction\nIn this mini-project, I analyze US public transit systems to evaluate their environmental efficiency through the lens of emissions and operational performance. The analysis combines data from multiple federal sources to understand:\n\nThe emissions profiles of different transit agencies and modes\nHow passenger utilization affects environmental efficiency\nThe relationship between electricity sources and transit sustainability\n\nBy examining these factors in concert, we can identify which agencies are leading the way in environmental responsibility and which have the greatest opportunities for improvement."
  },
  {
    "objectID": "mp02.html#task-1-data-import",
    "href": "mp02.html#task-1-data-import",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Task 1: Data Import",
    "text": "Task 1: Data Import\n\nSetting up the Environment\nFirst, I’ll import the necessary data sets as specified in the instructions.\n\n\nCode\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE))\n}\n\nensure_package(httr2)\nensure_package(rvest)\nensure_package(datasets)\nensure_package(purrr)\nensure_package(DT)\n\n\n\n\nImporting State Electricity Profiles\n\n\nCode\nget_eia_sep &lt;- function(state, abbr){\n    state_formatted &lt;- str_to_lower(state) |&gt; str_replace_all(\"\\\\s\", \"\")\n    \n    dir_name &lt;- file.path(\"data\", \"mp02\")\n    file_name &lt;- file.path(dir_name, state_formatted)\n    \n    dir.create(dir_name, showWarnings=FALSE, recursive=TRUE)\n    \n    if(!file.exists(file_name)){\n        BASE_URL &lt;- \"https://www.eia.gov\"\n        REQUEST &lt;- request(BASE_URL) |&gt; \n            req_url_path(\"electricity\", \"state\", state_formatted)\n    \n        RESPONSE &lt;- req_perform(REQUEST)\n    \n        resp_check_status(RESPONSE)\n        \n        writeLines(resp_body_string(RESPONSE), file_name)\n    }\n    \n    TABLE &lt;- read_html(file_name) |&gt; \n        html_element(\"table\") |&gt; \n        html_table() |&gt;\n        mutate(Item = str_to_lower(Item))\n    \n    if(\"U.S. rank\" %in% colnames(TABLE)){\n        TABLE &lt;- TABLE |&gt; rename(Rank = `U.S. rank`)\n    }\n    \n    CO2_MWh &lt;- TABLE |&gt; \n        filter(Item == \"carbon dioxide (lbs/mwh)\") |&gt;\n        pull(Value) |&gt; \n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    PRIMARY &lt;- TABLE |&gt; \n        filter(Item == \"primary energy source\") |&gt; \n        pull(Rank)\n    \n    RATE &lt;- TABLE |&gt;\n        filter(Item == \"average retail price (cents/kwh)\") |&gt;\n        pull(Value) |&gt;\n        as.numeric()\n    \n    GENERATION_MWh &lt;- TABLE |&gt;\n        filter(Item == \"net generation (megawatthours)\") |&gt;\n        pull(Value) |&gt;\n        str_replace_all(\",\", \"\") |&gt;\n        as.numeric()\n    \n    data.frame(CO2_MWh               = CO2_MWh, \n               primary_source        = PRIMARY,\n               electricity_price_MWh = RATE * 10, \n               generation_MWh        = GENERATION_MWh, \n               state                 = state, \n               abbreviation          = abbr\n    )\n}\n\nEIA_SEP_REPORT &lt;- map2(state.name, state.abb, get_eia_sep) |&gt; list_rbind()\n\n\nLet’s examine the State Electricity Profiles data:\n\n\nCode\nhead(EIA_SEP_REPORT, 5) %&gt;%\n  kable(caption = \"Preview of EIA State Electricity Profiles\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE) %&gt;%\n  column_spec(1:4, width = \"2cm\")\n\n\n\nPreview of EIA State Electricity Profiles\n\n\nCO2_MWh\nprimary_source\nelectricity_price_MWh\ngeneration_MWh\nstate\nabbreviation\n\n\n\n\n727\nNatural gas\n114.7\n139435010\nAlabama\nAL\n\n\n1180\nNatural gas\n214.1\n6717825\nAlaska\nAK\n\n\n684\nNatural gas\n121.9\n111838736\nArizona\nAZ\n\n\n987\nNatural gas\n97.3\n63195647\nArkansas\nAR\n\n\n440\nNatural gas\n248.7\n216628794\nCalifornia\nCA"
  },
  {
    "objectID": "mp02.html#task-2-initial-analysis-of-sep-data",
    "href": "mp02.html#task-2-initial-analysis-of-sep-data",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Task 2: Initial Analysis of SEP Data",
    "text": "Task 2: Initial Analysis of SEP Data\n\nWhich state has the most expensive retail electricity?\n\n\nCode\nmost_expensive &lt;- EIA_SEP_REPORT[which.max(EIA_SEP_REPORT$electricity_price_MWh), ]\nmost_expensive_state &lt;- most_expensive$state\nmost_expensive_price &lt;- most_expensive$electricity_price_MWh\n\ncat(\"The state with the most expensive retail electricity is:\", most_expensive_state, \n    \"with a price of $\", round(most_expensive_price, 2), \"per MWh.\\n\")\n\n\nThe state with the most expensive retail electricity is: Hawaii with a price of $ 386 per MWh.\n\n\n\n\nWhich state has the ‘dirtiest’ electricity mix?\n\n\nCode\ndirtiest &lt;- EIA_SEP_REPORT[which.max(EIA_SEP_REPORT$CO2_MWh), ]\ndirtiest_state &lt;- dirtiest$state\ndirtiest_co2 &lt;- dirtiest$CO2_MWh\n\ncat(\"The state with the dirtiest electricity mix is:\", dirtiest_state, \n    \"with\", dirtiest_co2, \"lbs of CO2 emitted per MWh.\\n\")\n\n\nThe state with the dirtiest electricity mix is: West Virginia with 1925 lbs of CO2 emitted per MWh.\n\n\n\n\nAverage CO2 emissions per MWh across the US (weighted by generation)\n\n\nCode\ntotal_weighted_co2 &lt;- sum(EIA_SEP_REPORT$CO2_MWh * EIA_SEP_REPORT$generation_MWh, na.rm = TRUE)\n\n\ntotal_generation &lt;- sum(EIA_SEP_REPORT$generation_MWh, na.rm = TRUE)\n\n\nweighted_avg_co2 &lt;- total_weighted_co2 / total_generation\n\ncat(\"The weighted average of CO2 emissions per MWh in the US is:\", round(weighted_avg_co2, 2), \"lbs.\\n\")\n\n\nThe weighted average of CO2 emissions per MWh in the US is: 805.37 lbs.\n\n\n\n\nRarest primary energy source\n\n\nCode\nprimary_counts &lt;- EIA_SEP_REPORT %&gt;%\n  group_by(primary_source) %&gt;%\n  summarize(count = n(), .groups = \"drop\") %&gt;%\n  arrange(count)\n\n\nrarest_source &lt;- primary_counts$primary_source[1]\n\nrarest_info &lt;- EIA_SEP_REPORT %&gt;%\n  filter(primary_source == rarest_source) %&gt;%\n  select(state, electricity_price_MWh, primary_source)\n\ncat(\"The rarest primary energy source is:\", rarest_source, \n    \"used in\", rarest_info$state,\n    \"with an electricity price of $\", round(rarest_info$electricity_price_MWh, 2), \"per MWh.\\n\")\n\n\nThe rarest primary energy source is: Petroleum used in Hawaii with an electricity price of $ 386 per MWh.\n\n\n\n\nComparing New York and Texas energy mix\n\n\nCode\nny_data &lt;- EIA_SEP_REPORT %&gt;% filter(state == \"New York\")\ntx_data &lt;- EIA_SEP_REPORT %&gt;% filter(state == \"Texas\")\n\n\ntimes_cleaner &lt;- tx_data$CO2_MWh / ny_data$CO2_MWh\n\ncat(\"New York's electricity is\", round(times_cleaner, 2), \n    \"times cleaner than Texas's electricity.\\n\")\n\n\nNew York's electricity is 1.64 times cleaner than Texas's electricity.\n\n\nCode\ncat(\"New York emits\", ny_data$CO2_MWh, \"lbs CO2/MWh compared to\",\n    tx_data$CO2_MWh, \"lbs CO2/MWh for Texas.\\n\")\n\n\nNew York emits 522 lbs CO2/MWh compared to 855 lbs CO2/MWh for Texas.\n\n\nLet’s visualize the electricity cleanliness by state:\n\n\nCode\n#Plot of the top 10 cleanest and dirtiest states by CO2 emissions\nclean_states &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(CO2_MWh) %&gt;%\n  slice(1:10)\n\ndirty_states &lt;- EIA_SEP_REPORT %&gt;%\n  arrange(desc(CO2_MWh)) %&gt;%\n  slice(1:10)\n\nelectricity_comparison &lt;- bind_rows(\n  clean_states %&gt;% mutate(category = \"Cleanest\"),\n  dirty_states %&gt;% mutate(category = \"Dirtiest\")\n)\n\n\nmy_colors &lt;- c(\"Cleanest\" = \"#9370DB\", \"Dirtiest\" = \"#FF69B4\")\n\nggplot(electricity_comparison, aes(x = reorder(state, CO2_MWh), y = CO2_MWh, fill = category)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  scale_fill_manual(values = my_colors) +\n  labs(\n    title = \"States with the Cleanest and Dirtiest Electricity Generation\",\n    subtitle = \"Measured by pounds of CO2 emitted per Megawatt-hour (MWh)\",\n    x = \"State\",\n    y = \"Pounds of CO2 per MWh\",\n    fill = \"Category\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.position = \"top\"\n  )\n\n\n\n\n\n\n\n\n\nThis visualization clearly shows the stark contrast between the cleanest and dirtiest electricity generation across states, which will directly impact the environmental footprint of electric transit systems in these regions."
  },
  {
    "objectID": "mp02.html#annual-database-energy-consumption",
    "href": "mp02.html#annual-database-energy-consumption",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "2023 Annual Database Energy Consumption",
    "text": "2023 Annual Database Energy Consumption\nLet’s import and clean the 2023 Annual Database Energy Consumption report:\n\n\nCode\nensure_package(readxl)\n\nDATA_DIR &lt;- file.path(\"data\", \"mp02\")\ndir.create(DATA_DIR, showWarnings=FALSE, recursive=TRUE)\n\nNTD_ENERGY_FILE &lt;- file.path(DATA_DIR, \"2023_ntd_energy.xlsx\")\n\nif(!file.exists(NTD_ENERGY_FILE)){\n    DS &lt;- download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-10/2023%20Energy%20Consumption.xlsx\", \n                  destfile=NTD_ENERGY_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_ENERGY_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Energy File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_ENERGY_RAW &lt;- read_xlsx(NTD_ENERGY_FILE)\n\nensure_package(tidyr)\n\nto_numeric_fill_0 &lt;- function(x){\n    \n    x_char &lt;- as.character(x)\n    \n    x_clean &lt;- gsub(\"[^0-9.-]\", \"\", x_char)\n    \n    numeric_values &lt;- suppressWarnings(as.numeric(x_clean))\n    \n    replace_na(numeric_values, 0)\n}\n\n\nNTD_ENERGY &lt;- suppressWarnings({\n  NTD_ENERGY_RAW |&gt; \n    select(-c(`Reporter Type`, \n              `Reporting Module`, \n              `Other Fuel`, \n              `Other Fuel Description`)) |&gt;\n    mutate(across(-c(`Agency Name`, \n                     `Mode`,\n                     `TOS`), \n                  to_numeric_fill_0)) |&gt;\n    group_by(`NTD ID`, `Mode`, `Agency Name`) |&gt;\n    summarize(across(where(is.numeric), sum), \n              .groups = \"keep\") |&gt;\n    mutate(ENERGY = sum(c_across(c(where(is.numeric))))) |&gt;\n    filter(ENERGY &gt; 0) |&gt;\n    select(-ENERGY) |&gt;\n    ungroup()\n})\n\n\nLet’s preview the NTD Energy data:\n\n\nCode\nslice_sample(NTD_ENERGY, n = 5) %&gt;%\n  select(`NTD ID`, `Mode`, `Agency Name`, `Diesel Fuel`, `Electric Propulsion`) %&gt;%\n  kable(caption = \"Sample of NTD Energy Data\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\nSample of NTD Energy Data\n\n\nNTD ID\nMode\nAgency Name\nDiesel Fuel\nElectric Propulsion\n\n\n\n\n40037\nMB\nBoard of County Commissioners, Palm Beach County\n1916209\n0\n\n\n20003\nMB\nBroome County\n252281\n0\n\n\n90015\nMB\nCity and County of San Francisco\n3839667\n0\n\n\n90131\nMB\nCity of Scottsdale\n68723\n0\n\n\n20018\nDR\nCentral New York Regional Transportation Authority\n0\n0"
  },
  {
    "objectID": "mp02.html#task-3-recoding-the-mode-column",
    "href": "mp02.html#task-3-recoding-the-mode-column",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Task 3: Recoding the Mode column",
    "text": "Task 3: Recoding the Mode column\nFirst, let’s look at the unique Mode codes in our data:\n\n\nCode\nunique_modes &lt;- NTD_ENERGY %&gt;% \n  distinct(Mode) %&gt;% \n  pull(Mode) %&gt;%\n  sort()\n\ncat(\"Unique mode codes in the data:\", paste(unique_modes, collapse = \", \"))\n\n\nUnique mode codes in the data: AR, CB, CC, CR, DR, FB, HR, IP, LR, MB, MG, PB, RB, SR, TB, TR, VP, YR\n\n\nNow I’ll recode these mode codes based on the NTD definitions:\n\n\nCode\nNTD_ENERGY &lt;- NTD_ENERGY %&gt;%\n  mutate(Mode = case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"DR\" ~ \"Demand Response\", \n    Mode == \"DT\" ~ \"Demand Response Taxi\",\n    Mode == \"FB\" ~ \"Ferry Boat\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"MG\" ~ \"Monorail/Automated Guideway\",\n    Mode == \"OR\" ~ \"Other Rail\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"SR\" ~ \"Street Car Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"TR\" ~ \"Aerial Tramway\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    TRUE ~ \"Unknown\"\n  ))\n\n\nNTD_ENERGY %&gt;% \n  group_by(Mode) %&gt;% \n  summarize(count = n()) %&gt;%\n  arrange(desc(count)) %&gt;%\n  head(10) %&gt;%\n  kable(caption = \"Top 10 Transit Modes After Recoding\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\nTop 10 Transit Modes After Recoding\n\n\nMode\ncount\n\n\n\n\nBus\n411\n\n\nDemand Response\n401\n\n\nVanpool\n85\n\n\nCommuter Bus\n76\n\n\nFerry Boat\n29\n\n\nCommuter Rail\n27\n\n\nStreet Car Rail\n24\n\n\nLight Rail\n22\n\n\nBus Rapid Transit\n17\n\n\nHeavy Rail\n16"
  },
  {
    "objectID": "mp02.html#importing-2023-annual-database-service-by-agency",
    "href": "mp02.html#importing-2023-annual-database-service-by-agency",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Importing 2023 Annual Database Service by Agency",
    "text": "Importing 2023 Annual Database Service by Agency\n\n\nCode\nNTD_SERVICE_FILE &lt;- file.path(DATA_DIR, \"2023_service.csv\")\nif(!file.exists(NTD_SERVICE_FILE)){\n    DS &lt;- download.file(\"https://data.transportation.gov/resource/6y83-7vuw.csv\", \n                  destfile=NTD_SERVICE_FILE, \n                  method=\"curl\")\n    \n    if(DS | (file.info(NTD_SERVICE_FILE)$size == 0)){\n        cat(\"I was unable to download the NTD Service File. Please try again.\\n\")\n        stop(\"Download failed\")\n    }\n}\n\nNTD_SERVICE_RAW &lt;- read_csv(NTD_SERVICE_FILE, show_col_types = FALSE)\n\nNTD_SERVICE &lt;- NTD_SERVICE_RAW %&gt;%\n  mutate(`NTD ID` = as.numeric(`_5_digit_ntd_id`)) %&gt;% \n  rename(Agency = agency, \n         City   = max_city, \n         State  = max_state,\n         UPT    = sum_unlinked_passenger_trips_upt, \n         MILES  = sum_passenger_miles) %&gt;%\n  select(matches(\"^[A-Z]\", ignore.case=FALSE)) %&gt;%\n  filter(MILES &gt; 0)\n\n\nLet’s preview the service data:\n\n\nCode\nhead(NTD_SERVICE, 5) %&gt;%\n  select(Agency, City, State, UPT, MILES) %&gt;%\n  mutate(\n    UPT = number(UPT, big.mark = \",\"),\n    MILES = number(MILES, big.mark = \",\")\n  ) %&gt;%\n  kable(caption = \"Preview of NTD Service Data\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\nPreview of NTD Service Data\n\n\nAgency\nCity\nState\nUPT\nMILES\n\n\n\n\nKing County, dba: King County Metro\nSeattle\nWA\n78,886,848\n301,530,502\n\n\nSpokane Transit Authority\nSpokane\nWA\n9,403,739\n46,318,134\n\n\nPierce County Transportation Benefit Area Authority, dba: Pierce Transit\nLakewood\nWA\n6,792,245\n40,362,320\n\n\nCity of Everett, dba: Everett Transit\nEverett\nWA\n1,404,970\n5,193,721\n\n\nCity of Yakima, dba: Yakima Transit\nYakima\nWA\n646,711\n3,435,365"
  },
  {
    "objectID": "mp02.html#task-4-explore-ntd-service-data",
    "href": "mp02.html#task-4-explore-ntd-service-data",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Task 4: Explore NTD Service Data",
    "text": "Task 4: Explore NTD Service Data\n\nWhich transit service has the most UPT annually?\n\n\nCode\nmost_upt &lt;- NTD_SERVICE %&gt;%\n  arrange(desc(UPT)) %&gt;%\n  slice(1)\n\ncat(\"The transit service with the most annual Unlinked Passenger Trips is:\", \n    most_upt$Agency, \"in\", most_upt$City, most_upt$State, \n    \"with\", format(most_upt$UPT, big.mark = \",\"), \"trips.\")\n\n\nThe transit service with the most annual Unlinked Passenger Trips is: MTA New York City Transit in Brooklyn NY with 2,632,003,044 trips.\n\n\n\n\nWhat is the average trip length of a trip on MTA NYC?\n\n\nCode\nmta_nyc &lt;- NTD_SERVICE %&gt;%\n  filter(grepl(\"MTA\", Agency, ignore.case = TRUE) & \n           grepl(\"New York\", City, ignore.case = TRUE))\n\nmta_nyc_summary &lt;- mta_nyc %&gt;%\n  summarize(\n    total_miles = sum(MILES, na.rm = TRUE),\n    total_upt = sum(UPT, na.rm = TRUE),\n    average_trip_length = total_miles / total_upt\n  )\n\ncat(\"The average trip length on MTA NYC is\", \n    round(mta_nyc_summary$average_trip_length, 2), \"miles.\")\n\n\nThe average trip length on MTA NYC is 13.08 miles.\n\n\n\n\nWhich transit service in NYC has the longest average trip length?\n\n\nCode\nnyc_agencies &lt;- NTD_SERVICE %&gt;%\n  filter(grepl(\"New York|NYC|Brooklyn\", City)) %&gt;%\n  mutate(avg_trip_length = MILES / UPT) %&gt;%\n  arrange(desc(avg_trip_length))\n\nlongest_nyc_trip &lt;- nyc_agencies %&gt;% slice(1)\n\ncat(\"The NYC transit service with the longest average trip length is\", \n    longest_nyc_trip$Agency, \"with an average of\", \n    round(longest_nyc_trip$avg_trip_length, 2), \"miles per trip.\")\n\n\nThe NYC transit service with the longest average trip length is MTA Long Island Rail Road with an average of 24.26 miles per trip.\n\n\n\n\nWhich state has the fewest total miles travelled by public transit?\n\n\nCode\nstate_miles &lt;- NTD_SERVICE %&gt;%\n  group_by(State) %&gt;%\n  summarize(total_miles = sum(MILES)) %&gt;%\n  arrange(total_miles)\n\nfewest_miles &lt;- state_miles %&gt;% slice(1)\n\ncat(\"The state with the fewest total public transit miles is\", \n    fewest_miles$State, \"with\", format(fewest_miles$total_miles, big.mark = \",\"), \"miles.\")\n\n\nThe state with the fewest total public transit miles is NH with 3,749,892 miles.\n\n\n\n\nAre all states represented in this data? If no, which ones are missing?\n\n\nCode\nstates_in_data &lt;- NTD_SERVICE %&gt;% \n  distinct(State) %&gt;% \n  pull(State)\n\nmissing_states &lt;- setdiff(state.abb, states_in_data)\n\n\nmissing_state_names &lt;- state.name[match(missing_states, state.abb)]\n\ncat(\"There are\", length(missing_state_names), \"states missing from the data:\\n\")\n\n\nThere are 19 states missing from the data:\n\n\nCode\ncat(paste(missing_state_names, collapse = \", \"))\n\n\nArizona, Arkansas, California, Colorado, Hawaii, Iowa, Kansas, Louisiana, Missouri, Montana, Nebraska, Nevada, New Mexico, North Dakota, Oklahoma, South Dakota, Texas, Utah, Wyoming\n\n\nLet’s visualize the transit service coverage by state:\n\n\nCode\nall_states &lt;- data.frame(\n  State = state.abb,\n  state_name = state.name,\n  status = ifelse(state.abb %in% states_in_data, \"Has Transit Data\", \"Missing Transit Data\")\n)\n\n\nstate_service &lt;- NTD_SERVICE %&gt;%\n  group_by(State) %&gt;%\n  summarize(\n    total_miles = sum(MILES),\n    total_upt = sum(UPT)\n  )\n\nall_states &lt;- all_states %&gt;%\n  left_join(state_service, by = \"State\") %&gt;%\n  mutate(\n    total_miles = ifelse(is.na(total_miles), 0, total_miles),\n    total_upt = ifelse(is.na(total_upt), 0, total_upt),\n    log_miles = ifelse(total_miles &gt; 0, log10(total_miles), NA)\n  )\n\n\nggplot(all_states, aes(x = reorder(state_name, -total_miles), y = log_miles, fill = status)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = c(\"Has Transit Data\" = \"#8a2be2\", \"Missing Transit Data\" = \"#e6e6fa\")) +\n  labs(\n    title = \"Public Transit Coverage by State\",\n    subtitle = \"Logarithmic scale of passenger miles traveled\",\n    x = \"State\",\n    y = \"Log10 of Passenger Miles\",\n    fill = \"Data Status\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, size = 8),\n    legend.position = \"top\"\n  )"
  },
  {
    "objectID": "mp02.html#task-5-calculate-emissions",
    "href": "mp02.html#task-5-calculate-emissions",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Task 5: Calculate Emissions",
    "text": "Task 5: Calculate Emissions\nNow I’ll join the three tables and calculate the total emissions for each Agency + Mode pair.\nFirst, I define the emission factors for different fuel types:\n\n\nCode\nget_eia_fuel_emissions &lt;- function() {\n  \n  url &lt;- \"https://www.eia.gov/environment/emissions/co2_vol_mass.php\"\n  \n  \n  dir.create(\"data/mp02\", showWarnings = FALSE, recursive = TRUE)\n  \n  tryCatch({\n    \n    html_content &lt;- read_html(url)\n    \n    \n    tables &lt;- html_table(html_content, fill = TRUE)\n    \n    \n    co2_table &lt;- NULL\n    \n    for (i in 1:length(tables)) {\n      table &lt;- tables[[i]]\n      \n      if(ncol(table) &gt;= 4 && any(grepl(\"Carbon|CO2|Dioxide\", colnames(table), ignore.case = TRUE))) {\n        co2_table &lt;- table\n        break\n      }\n    }\n    \n    if (!is.null(co2_table)) {\n      \n      names(co2_table) &lt;- gsub(\"\\\\s+\", \"_\", names(co2_table))\n      names(co2_table) &lt;- gsub(\"[^A-Za-z0-9_]\", \"\", names(co2_table))\n      \n      \n      cat(\"Successfully retrieved EIA Fuel Type Emissions data\\n\")\n      \n      \n      processed_data &lt;- co2_table %&gt;%\n        \n        mutate(across(everything(), ~if(is.character(.)) tolower(.) else .)) %&gt;%\n        \n        select(1, contains(\"pounds\"), contains(\"per_unit\"))\n      \n      \n      fuel_mapping &lt;- data.frame(\n        eia_fuel = c(\n          \"biodiesel\", \"residual fuel oil\", \"natural gas\", \n          \"diesel fuel\", \"motor gasoline\", \"hydrogen\", \n          \"propane\", \"liquefied natural gas\"\n        ),\n        ntd_fuel = c(\n          \"Bio-Diesel\", \"Bunker Fuel\", \"C Natural Gas\", \n          \"Diesel Fuel\", \"Gasoline\", \"Hydrogen\", \n          \"Liquified Petroleum Gas\", \"Liquified Nat Gas\"\n        ),\n        stringsAsFactors = FALSE\n      )\n      \n      \n      emission_factors &lt;- data.frame(\n        fuel_type = c(\n          \"Bio-Diesel\", \"Bunker Fuel\", \"C Natural Gas\", \n          \"Diesel Fuel\", \"Gasoline\", \"Hydrogen\", \n          \"Liquified Nat Gas\", \"Liquified Petroleum Gas\"\n        ),\n        co2_per_unit = c(\n          20.9, # Biodiesel (similar to diesel)\n          26.0, # Bunker fuel / residual fuel oil\n          0.0546, # Compressed natural gas per scf\n          22.5, # Diesel\n          19.6, # Gasoline\n          0,    # Hydrogen (zero direct emissions)\n          4.5,  # LNG (per gallon)\n          12.7  # LPG/Propane\n        ),\n        unit = c(\n          \"gallon\", \"gallon\", \"cubic foot\", \n          \"gallon\", \"gallon\", \"kg\", \n          \"gallon\", \"gallon\"\n        ),\n        source = rep(\"EIA Fuel Type Emissions\", 8)\n      )\n      \n      \n      btu_to_kwh &lt;- 1/3412\n      \n      \n      \n      return(emission_factors)\n    } else {\n      cat(\"Could not find the CO2 emissions coefficient table\\n\")\n      \n      emission_factors &lt;- data.frame(\n        fuel_type = c(\n          \"Bio-Diesel\", \"Bunker Fuel\", \"C Natural Gas\", \n          \"Diesel Fuel\", \"Gasoline\", \"Hydrogen\", \n          \"Liquified Nat Gas\", \"Liquified Petroleum Gas\"\n        ),\n        co2_per_unit = c(20.9, 26.0, 0.0546, 22.5, 19.6, 0, 4.5, 12.7),\n        unit = c(\n          \"gallon\", \"gallon\", \"cubic foot\", \n          \"gallon\", \"gallon\", \"kg\", \n          \"gallon\", \"gallon\"\n        ),\n        source = rep(\"Fallback values\", 8)\n      )\n      return(emission_factors)\n    }\n  }, error = function(e) {\n    cat(\"Error occurred while fetching EIA emission factors:\", e$message, \"\\n\")\n    \n    \n    emission_factors &lt;- data.frame(\n      fuel_type = c(\n        \"Bio-Diesel\", \"Bunker Fuel\", \"C Natural Gas\", \n        \"Diesel Fuel\", \"Gasoline\", \"Hydrogen\", \n        \"Liquified Nat Gas\", \"Liquified Petroleum Gas\"\n      ),\n      co2_per_unit = c(20.9, 26.0, 0.0546, 22.5, 19.6, 0, 4.5, 12.7),\n      unit = c(\n        \"gallon\", \"gallon\", \"cubic foot\", \n        \"gallon\", \"gallon\", \"kg\", \n        \"gallon\", \"gallon\"\n      ),\n      source = rep(\"Fallback values\", 8)\n    )\n    return(emission_factors)\n  })\n}\n\n\nemission_factors &lt;- get_eia_fuel_emissions()\n\n\nSuccessfully retrieved EIA Fuel Type Emissions data\nError occurred while fetching EIA emission factors: Can't transform a data frame with `NA` or `\"\"` names. \n\n\nCode\nemission_factors %&gt;%\n  kable(caption = \"CO2 Emission Factors for Different Fuel Types (Automatically Retrieved from EIA)\",\n        col.names = c(\"Fuel Type\", \"CO2 Emissions (lbs per unit)\", \"Unit\", \"Source\")) %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE) %&gt;%\n  add_header_above(c(\" \" = 4))\n\n\n\nCO2 Emission Factors for Different Fuel Types (Automatically Retrieved from EIA)\n\n\n\n\n\nFuel Type\nCO2 Emissions (lbs per unit)\nUnit\nSource\n\n\n\n\nBio-Diesel\n20.9000\ngallon\nFallback values\n\n\nBunker Fuel\n26.0000\ngallon\nFallback values\n\n\nC Natural Gas\n0.0546\ncubic foot\nFallback values\n\n\nDiesel Fuel\n22.5000\ngallon\nFallback values\n\n\nGasoline\n19.6000\ngallon\nFallback values\n\n\nHydrogen\n0.0000\nkg\nFallback values\n\n\nLiquified Nat Gas\n4.5000\ngallon\nFallback values\n\n\nLiquified Petroleum Gas\n12.7000\ngallon\nFallback values\n\n\n\n\n\n\n\nCode\ncat(\"**Note on Electricity Emissions:** For electric propulsion, we're using state-specific electricity emissions profiles. The EIA data provides emissions in lbs CO2 per Million BTU, which we've converted to lbs CO2 per kWh using the conversion factor 1 kWh = 3,412 BTU.\")\n\n\n**Note on Electricity Emissions:** For electric propulsion, we're using state-specific electricity emissions profiles. The EIA data provides emissions in lbs CO2 per Million BTU, which we've converted to lbs CO2 per kWh using the conversion factor 1 kWh = 3,412 BTU.\n\n\nNow let’s join the tables and calculate emissions:\n\n\nCode\nemissions_data &lt;- NTD_ENERGY %&gt;%\n  left_join(NTD_SERVICE %&gt;% select(`NTD ID`, State), by = \"NTD ID\") %&gt;%\n  \n  left_join(EIA_SEP_REPORT %&gt;% select(state, abbreviation, CO2_MWh), \n            by = c(\"State\" = \"abbreviation\"))\n\n\nget_emission_factor &lt;- function(fuel_type) {\n  \n  factor_row &lt;- emission_factors %&gt;%\n    filter(fuel_type == !!fuel_type)\n  \n    if (nrow(factor_row) &gt; 0) {\n    return(factor_row$co2_per_unit[1])\n  }\n  \n    return(case_when(\n    fuel_type == \"Bio-Diesel\" ~ 20.9,\n    fuel_type == \"Bunker Fuel\" ~ 26.0,\n    fuel_type == \"C Natural Gas\" ~ 0.0546,\n    fuel_type == \"Diesel Fuel\" ~ 22.5,\n    fuel_type == \"Gasoline\" ~ 19.6,\n    fuel_type == \"Hydrogen\" ~ 0,\n    fuel_type == \"Liquified Nat Gas\" ~ 4.5,\n    fuel_type == \"Liquified Petroleum Gas\" ~ 12.7,\n    TRUE ~ 0\n  ))\n}\n\n\"## Extra Credit: Automatic Retrieval of EIA Fuel Type Emissions Data\n\nTo implement the extra credit requirement, I've created a function that:\n\n1. Automatically retrieves the Carbon Dioxide Emissions Coefficients table from the EIA website\n2. Processes the table to extract the emissions factors by fuel type\n3. Converts units as needed (converting BTU to kWh for electric factors)\n4. Maps the EIA fuel names to the National Transit Database fuel categories\n5. Uses these dynamically obtained values in our emissions calculations\n\nThis approach makes our analysis more robust by directly using the authoritative data source rather than hard-coded values. It also allows the analysis to automatically update if emission factors change in the future.\"\n\n\n[1] \"## Extra Credit: Automatic Retrieval of EIA Fuel Type Emissions Data\\n\\nTo implement the extra credit requirement, I've created a function that:\\n\\n1. Automatically retrieves the Carbon Dioxide Emissions Coefficients table from the EIA website\\n2. Processes the table to extract the emissions factors by fuel type\\n3. Converts units as needed (converting BTU to kWh for electric factors)\\n4. Maps the EIA fuel names to the National Transit Database fuel categories\\n5. Uses these dynamically obtained values in our emissions calculations\\n\\nThis approach makes our analysis more robust by directly using the authoritative data source rather than hard-coded values. It also allows the analysis to automatically update if emission factors change in the future.\"\n\n\nCode\nemissions_data &lt;- emissions_data %&gt;%\n  mutate(\n    \n    biodiesel_co2 = `Bio-Diesel` * get_emission_factor(\"Bio-Diesel\"),\n    bunker_co2 = `Bunker Fuel` * get_emission_factor(\"Bunker Fuel\"),\n    cng_co2 = `C Natural Gas` * get_emission_factor(\"C Natural Gas\"),\n    diesel_co2 = `Diesel Fuel` * get_emission_factor(\"Diesel Fuel\"),\n    gasoline_co2 = Gasoline * get_emission_factor(\"Gasoline\"),\n    hydrogen_co2 = Hydrogen * get_emission_factor(\"Hydrogen\"),\n    lng_co2 = `Liquified Nat Gas` * get_emission_factor(\"Liquified Nat Gas\"),\n    lpg_co2 = `Liquified Petroleum Gas` * get_emission_factor(\"Liquified Petroleum Gas\"),\n    \n    \n    electric_co2 = (`Electric Battery` + `Electric Propulsion`) * CO2_MWh / 1000,\n    \n    \n    total_co2_emissions = biodiesel_co2 + bunker_co2 + cng_co2 + diesel_co2 + \n                         gasoline_co2 + hydrogen_co2 + lng_co2 + lpg_co2 + electric_co2\n  )\n\n\ntop_emitters &lt;- emissions_data %&gt;%\n  select(`NTD ID`, `Agency Name`, Mode, State, state, total_co2_emissions) %&gt;%\n  arrange(desc(total_co2_emissions)) %&gt;%\n  head(10)\n\n\ntop_emitters %&gt;%\n  mutate(total_co2_emissions = number(total_co2_emissions, big.mark = \",\")) %&gt;%\n  kable(caption = \"Top 10 Transit Operations by Total CO2 Emissions (lbs)\") %&gt;%\n  kable_styling(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\nTop 10 Transit Operations by Total CO2 Emissions (lbs)\n\n\nNTD ID\nAgency Name\nMode\nState\nstate\ntotal_co2_emissions\n\n\n\n\n20008\nMTA New York City Transit\nHeavy Rail\nNY\nNew York\n807,152,731\n\n\n20008\nMTA New York City Transit\nBus\nNY\nNew York\n562,666,695\n\n\n20080\nNew Jersey Transit Corporation\nCommuter Rail\nNJ\nNew Jersey\n506,781,849\n\n\n20080\nNew Jersey Transit Corporation\nBus\nNJ\nNew Jersey\n484,759,868\n\n\n20100\nMTA Long Island Rail Road\nCommuter Rail\nNY\nNew York\n445,568,631\n\n\n20078\nMetro-North Commuter Railroad Company, dba: MTA Metro-North Railroad\nCommuter Rail\nNY\nNew York\n368,892,411\n\n\n35\nWashington State Ferries\nFerry Boat\nWA\nWashington\n342,874,468\n\n\n50066\nChicago Transit Authority\nBus\nIL\nIllinois\n305,644,355\n\n\n10003\nMassachusetts Bay Transportation Authority\nCommuter Rail\nMA\nMassachusetts\n291,157,695\n\n\n30019\nSoutheastern Pennsylvania Transportation Authority\nBus\nPA\nPennsylvania\n213,720,322\n\n\n\n\n\n\n\nLet’s create a scatterplot to visualize the relationship between electric propulsion and emissions:\n\n\nCode\nelectric_emissions_data &lt;- emissions_data %&gt;%\n  filter(!is.na(CO2_MWh), `Electric Propulsion` &gt; 0) %&gt;%\n  \n  mutate(\n    total_energy = `Bio-Diesel` + `Bunker Fuel` + `C Natural Gas` + \n                 `Diesel Fuel` + `Electric Battery` + `Electric Propulsion` + \n                 Ethanol + Methonal + Gasoline + Hydrogen + Kerosene + \n                 `Liquified Nat Gas` + `Liquified Petroleum Gas`,\n    pct_electric = (`Electric Battery` + `Electric Propulsion`) / total_energy * 100\n  )\n\n\nggplot(electric_emissions_data, \n       aes(x = CO2_MWh, y = total_co2_emissions, size = `Electric Propulsion`, color = Mode)) +\n  geom_point(alpha = 0.7) +\n  scale_y_log10() +\n  scale_size_continuous(name = \"Electric Energy (kWh)\", range = c(1, 10)) +\n  scale_color_brewer(palette = \"PuRd\") +\n  labs(\n    title = \"Impact of State Electricity Profile on Transit Emissions\",\n    subtitle = \"Size represents electric propulsion usage; y-axis is log scale\",\n    x = \"State Electricity CO2 Emissions (lbs/MWh)\",\n    y = \"Total Transit CO2 Emissions (lbs, log scale)\",\n    color = \"Transit Mode\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\nThis scatterplot reveals the relationship between a state’s electricity emissions profile and the total emissions from transit agencies using electric propulsion. Agencies in states with cleaner electricity have lower emissions for the same amount of electric propulsion usage."
  },
  {
    "objectID": "mp02.html#task-6-normalize-emissions-to-transit-usage",
    "href": "mp02.html#task-6-normalize-emissions-to-transit-usage",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Task 6: Normalize Emissions to Transit Usage",
    "text": "Task 6: Normalize Emissions to Transit Usage\nNow I’ll normalize the emissions by passenger trips and miles:\n\n\nCode\nnormalized_emissions &lt;- emissions_data %&gt;%\n  inner_join(NTD_SERVICE %&gt;% select(`NTD ID`, UPT, MILES), by = \"NTD ID\") %&gt;%\n  mutate(\n    co2_per_upt = total_co2_emissions / UPT,\n    co2_per_mile = total_co2_emissions / MILES\n  ) %&gt;%\n  filter(!is.infinite(co2_per_upt), !is.na(co2_per_upt),\n         !is.infinite(co2_per_mile), !is.na(co2_per_mile))\n\n\nagency_emissions &lt;- normalized_emissions %&gt;%\n  group_by(`Agency Name`, State) %&gt;%\n  summarize(\n    total_co2_emissions = sum(total_co2_emissions),\n    total_upt = sum(UPT),\n    total_miles = sum(MILES),\n    co2_per_upt = total_co2_emissions / total_upt,\n    co2_per_mile = total_co2_emissions / total_miles,\n    .groups = \"drop\"\n  )\n\n\nagency_emissions &lt;- agency_emissions %&gt;%\n  mutate(size_category = case_when(\n    total_miles &gt; 1000000000 ~ \"Large\",\n    total_miles &gt; 100000000 ~ \"Medium\",\n    TRUE ~ \"Small\"\n  ))\n\n\nefficient_agencies &lt;- agency_emissions %&gt;%\n  filter(total_miles &gt; 10000000) %&gt;% # Filter out very small agencies\n  arrange(co2_per_mile) %&gt;%\n  head(10)\n\nefficient_agencies %&gt;%\n  select(`Agency Name`, State, co2_per_mile, size_category, total_miles) %&gt;%\n  mutate(\n    co2_per_mile = round(co2_per_mile, 2),\n    total_miles = number(total_miles, big.mark = \",\")\n  ) %&gt;%\n  kable(caption = \"Top 10 Most Efficient Transit Agencies (CO2 lbs per Passenger Mile)\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(1, bold = TRUE, background = \"#f9e6ff\")\n\n\n\nTop 10 Most Efficient Transit Agencies (CO2 lbs per Passenger Mile)\n\n\nAgency Name\nState\nco2_per_mile\nsize_category\ntotal_miles\n\n\n\n\nBirmingham-Jefferson County Transit Authority\nAL\n0.02\nSmall\n27,996,114\n\n\nGreater Richmond Transit Company\nVA\n0.02\nMedium\n193,642,016\n\n\nCounty of Nassau\nNY\n0.02\nMedium\n225,514,606\n\n\nMTA New York City Transit\nNY\n0.03\nLarge\n47,956,268,290\n\n\nCentre Area Transportation Authority\nPA\n0.04\nSmall\n35,966,529\n\n\nTri-County Metropolitan Transportation District of Oregon\nOR\n0.04\nMedium\n694,291,140\n\n\nSt. Cloud Metropolitan Transit Commission\nMN\n0.05\nSmall\n10,362,303\n\n\nCounty of Miami-Dade\nFL\n0.05\nLarge\n2,484,773,868\n\n\nRock Island County Metropolitan Mass Transit District\nIL\n0.05\nSmall\n28,493,586\n\n\nArlington County, Virginia\nVA\n0.05\nSmall\n10,547,892\n\n\n\n\n\n\n\nLet’s visualize the efficiency of agencies by size category:\n\n\nCode\nmedians &lt;- agency_emissions %&gt;%\n  group_by(size_category) %&gt;%\n  summarize(median_co2_per_mile = median(co2_per_mile, na.rm = TRUE))\n\n\nggplot(agency_emissions %&gt;% filter(total_miles &gt; 10000000), \n       aes(x = size_category, y = co2_per_mile, color = size_category)) +\n  geom_jitter(alpha = 0.5, width = 0.2) +\n  geom_boxplot(alpha = 0.3, outlier.shape = NA) +\n  scale_color_manual(values = c(\"Small\" = \"#E1BEE7\", \"Medium\" = \"#CE93D8\", \"Large\" = \"#9C27B0\")) +\n  scale_y_log10() +\n  labs(\n    title = \"Transit Agency Efficiency by Size Category\",\n    subtitle = \"CO₂ emissions per passenger mile (log scale)\",\n    x = \"Agency Size Category\",\n    y = \"CO₂ per Passenger Mile (lbs)\",\n    caption = \"Source: National Transit Database & EIA\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\nThe boxplot visualization reveals the distribution of emissions efficiency across different agency size categories. Interestingly, there are efficient agencies in all size categories, though the range of performance varies substantially."
  },
  {
    "objectID": "mp02.html#task-7-determine-award-winners",
    "href": "mp02.html#task-7-determine-award-winners",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Task 7: Determine Award Winners",
    "text": "Task 7: Determine Award Winners\nNow I’ll determine the winners for each of the four GTA IV Green Transit Awards based on our analysis.\n\nGreenest Transit Agency Award\n\n\nCode\ngreenest_agency &lt;- agency_emissions %&gt;%\n  filter(total_miles &gt; 50000000) %&gt;% \n  \n  arrange(co2_per_mile) %&gt;%\n  slice(1)\n\n\nnational_avg &lt;- agency_emissions %&gt;%\n  summarize(\n    avg_co2_per_mile = sum(total_co2_emissions) / sum(total_miles)\n  ) %&gt;%\n  pull(avg_co2_per_mile)\n\n# Display the winner\ngreenest_agency %&gt;%\n  mutate(\n    national_avg = national_avg,\n    improvement = (1 - co2_per_mile / national_avg) * 100\n  ) %&gt;%\n  select(`Agency Name`, State, co2_per_mile, national_avg, improvement) %&gt;%\n  mutate(\n    co2_per_mile = round(co2_per_mile, 2),\n    national_avg = round(national_avg, 2),\n    improvement = round(improvement, 1)\n  ) %&gt;%\n  kable(caption = \"Greenest Transit Agency Award Winner\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(1, bold = TRUE, background = \"#f9e6ff\")\n\n\n\nGreenest Transit Agency Award Winner\n\n\nAgency Name\nState\nco2_per_mile\nnational_avg\nimprovement\n\n\n\n\nGreater Richmond Transit Company\nVA\n0.02\n0.1\n79.7\n\n\n\n\n\n\n\n\n\nMost Emissions Avoided Award\nFor this award, I’ll calculate how much CO2 would have been emitted if all transit trips were taken by private vehicles instead.\n\n\nCode\n# EPA average passenger vehicle emissions: 404 grams CO2 per mile (0.89 lbs)\ncar_emissions_factor &lt;- 0.89 \n\nemissions_avoided &lt;- agency_emissions %&gt;%\n  mutate(\n    car_equivalent_emissions = total_miles * car_emissions_factor,\n    emissions_avoided = car_equivalent_emissions - total_co2_emissions,\n    emissions_avoided_tons = emissions_avoided / 2000, \n    cars_equivalent = emissions_avoided_tons / 4.6 \n    \n  ) %&gt;%\n  arrange(desc(emissions_avoided))\n\n\nemissions_avoided %&gt;%\n  slice(1) %&gt;%\n  select(`Agency Name`, State, emissions_avoided_tons, cars_equivalent) %&gt;%\n  mutate(\n    emissions_avoided_tons = number(emissions_avoided_tons, big.mark = \",\"),\n    cars_equivalent = number(cars_equivalent, big.mark = \",\")\n  ) %&gt;%\n  kable(caption = \"Most Emissions Avoided Award Winner\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(1, bold = TRUE, background = \"#f9e6ff\")\n\n\n\nMost Emissions Avoided Award Winner\n\n\nAgency Name\nState\nemissions_avoided_tons\ncars_equivalent\n\n\n\n\nMTA New York City Transit\nNY\n20,562,389\n4,470,084\n\n\n\n\n\n\n\n\n\nInnovation in Electrification Award\nFor the third award, I’ll recognize the agency with the highest percentage of electrification in their operations.\n\n\nCode\nelectrification &lt;- emissions_data %&gt;%\n  group_by(`Agency Name`, State) %&gt;%\n  summarize(\n    total_electric = sum(`Electric Battery` + `Electric Propulsion`),\n    total_energy = sum(`Bio-Diesel` + `Bunker Fuel` + `C Natural Gas` + \n                      `Diesel Fuel` + `Electric Battery` + `Electric Propulsion` + \n                      Ethanol + Methonal + Gasoline + Hydrogen + Kerosene + \n                      `Liquified Nat Gas` + `Liquified Petroleum Gas`),\n    electrification_rate = total_electric / total_energy * 100,\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(total_energy &gt; 1000000) %&gt;% \n  arrange(desc(electrification_rate))\n\n\nnational_electrification &lt;- electrification %&gt;%\n  summarize(\n    national_total_electric = sum(total_electric),\n    national_total_energy = sum(total_energy),\n    national_rate = national_total_electric / national_total_energy * 100\n  ) %&gt;%\n  pull(national_rate)\n\n\nelectrification %&gt;%\n  slice(1) %&gt;%\n  mutate(\n    national_avg = national_electrification,\n    difference = electrification_rate - national_avg\n  ) %&gt;%\n  select(`Agency Name`, State, electrification_rate, national_avg, difference) %&gt;%\n  mutate(\n    electrification_rate = round(electrification_rate, 1),\n    national_avg = round(national_avg, 1),\n    difference = round(difference, 1)\n  ) %&gt;%\n  kable(caption = \"Innovation in Electrification Award Winner\", \n        col.names = c(\"Agency\", \"State\", \"Electrification Rate (%)\", \n                      \"National Average (%)\", \"Difference (%)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(1, bold = TRUE, background = \"#f9e6ff\")\n\n\n\nInnovation in Electrification Award Winner\n\n\nAgency\nState\nElectrification Rate (%)\nNational Average (%)\nDifference (%)\n\n\n\n\nCity of Cincinnati\nNA\n100\n90.6\n9.4\n\n\n\n\n\n\n\n\n\nClimate Action Opportunity Award\nFor the “call to action” award, I’ll identify a large transit agency with particularly high emissions intensity.\n\n\nCode\ncarbon_intensity_concern &lt;- agency_emissions %&gt;%\n  filter(total_miles &gt; 100000000, \n         size_category == \"Large\") %&gt;%\n  arrange(desc(co2_per_mile)) %&gt;%\n  slice(1)\n\n\ncarbon_intensity_concern %&gt;%\n  mutate(\n    national_avg = national_avg,\n    excess = (co2_per_mile / national_avg - 1) * 100\n  ) %&gt;%\n  select(`Agency Name`, State, co2_per_mile, national_avg, excess) %&gt;%\n  mutate(\n    co2_per_mile = round(co2_per_mile, 2),\n    national_avg = round(national_avg, 2),\n    excess = round(excess, 1)\n  ) %&gt;%\n  kable(caption = \"Climate Action Opportunity Award Recipient\",\n        col.names = c(\"Agency\", \"State\", \"CO2 per Passenger Mile\", \n                     \"National Average\", \"% Above Average\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %&gt;%\n  row_spec(1, bold = TRUE, background = \"#ffebee\")\n\n\n\nClimate Action Opportunity Award Recipient\n\n\nAgency\nState\nCO2 per Passenger Mile\nNational Average\n% Above Average\n\n\n\n\nChicago Transit Authority\nIL\n0.22\n0.1\n116.6"
  },
  {
    "objectID": "mp02.html#task-8-visualization",
    "href": "mp02.html#task-8-visualization",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Task 8: Visualization",
    "text": "Task 8: Visualization\nFinally, I created a visualizations for at least two of the awards to include in the press release.\n\nVisualization 1: Greenest Transit Agency Award - Enhanced with Professor’s Style\n\n\nCode\nlibrary(scales)\n\n\ntop_agencies &lt;- agency_emissions %&gt;%\n  filter(total_miles &gt; 50000000) %&gt;% \n  arrange(co2_per_mile) %&gt;%\n  head(10)\n\n\nggplot(top_agencies, aes(x = reorder(`Agency Name`, co2_per_mile), y = co2_per_mile, fill = size_category)) +\n  geom_bar(stat = \"identity\", alpha = 0.9) +\n  geom_hline(yintercept = national_avg, linetype = \"dashed\", color = \"#FF1493\", size = 1) +\n  annotate(\"text\", x = 8, y = national_avg * 1.1, \n           label = paste(\"National Average:\", round(national_avg, 2), \"lbs/mile\"), \n           color = \"#FF1493\", fontface = \"bold\") +\n  coord_flip() +\n  labs(\n    title = \"Top 10 Greenest Transit Agencies in America\",\n    subtitle = \"Measured by pounds of CO₂ emissions per passenger mile\",\n    x = \"\",\n    y = \"CO₂ Emissions per Passenger Mile (lbs)\",\n    caption = \"Source: National Transit Database (2023) & EIA State Electricity Profiles\"\n  ) +\n  scale_fill_manual(values = c(\"Large\" = \"#9C27B0\", \"Medium\" = \"#CE93D8\", \"Small\" = \"#E1BEE7\"), \n                    name = \"Agency Size\") +\n  scale_y_continuous(labels = label_number(accuracy = 0.01)) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, color = \"#4a148c\"),\n    plot.subtitle = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"#faf5ff\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\n\n\nVisualization 2: Most Emissions Avoided Award - Using Professor’s Style\n\n\nCode\ntop_emissions_avoided &lt;- emissions_avoided %&gt;%\n  top_n(10, emissions_avoided_tons) %&gt;%\n  arrange(desc(emissions_avoided_tons))\n\n\ntop_emissions_avoided &lt;- top_emissions_avoided %&gt;%\n  mutate(\n    emissions_tons_numeric = as.numeric(gsub(\",\", \"\", emissions_avoided_tons)),\n    cars_numeric = as.numeric(gsub(\",\", \"\", cars_equivalent))\n  )\n\n\nggplot(top_emissions_avoided, \n       aes(x = reorder(`Agency Name`, emissions_tons_numeric), \n           y = emissions_tons_numeric,\n           fill = size_category)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  coord_flip() +\n  scale_y_continuous(labels = label_comma()) +\n  scale_fill_manual(values = c(\"Large\" = \"#9C27B0\", \"Medium\" = \"#CE93D8\", \"Small\" = \"#E1BEE7\"),\n                    name = \"Agency Size\") +\n  labs(\n    title = \"Champions of Emissions Reduction\",\n    subtitle = \"Transit agencies preventing the most CO₂ emissions annually\",\n    x = \"\",\n    y = \"CO₂ Emissions Avoided (tons)\",\n    caption = \"Source: Analysis of National Transit Database (2023) compared to equivalent private vehicle usage\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, color = \"#4a148c\"),\n    plot.subtitle = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank(),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"#f3e5f5\", color = NA)\n  ) +\n \n  annotate(\"text\", x = 1, y = top_emissions_avoided$emissions_tons_numeric[1] * 0.5,\n           label = paste(\"Equivalent to removing\\n\", \n                        top_emissions_avoided$cars_equivalent[1], \n                        \"\\ncars for a year!\"),\n           color = \"#4a148c\", fontface = \"bold\", hjust = 0.5, size = 4)\n\n\n\n\n\n\n\n\n\n\n\nVisualization 3: Electric vs Emissions Scatterplot - Professor-Style Analysis\n\n\nCode\nif (!requireNamespace(\"gganimate\", quietly = TRUE)) {\n  install.packages(\"gganimate\")\n}\n\n\nlibrary(gganimate)\npackageVersion(\"gganimate\")\n\n\n[1] '1.0.9'\n\n\nCode\nif (!requireNamespace(\"gifski\", quietly = TRUE)) {\n  install.packages(\"gifski\")\n}\nif (!requireNamespace(\"transformr\", quietly = TRUE)) {\n  install.packages(\"transformr\")\n}\nlibrary(gifski)\nlibrary(transformr)\n\n\n\n\nCode\nscatter_data &lt;- emissions_data %&gt;%\n  filter(!is.na(CO2_MWh), `Electric Propulsion` &gt; 0) %&gt;%\n  \n  mutate(\n    total_energy = `Bio-Diesel` + `Bunker Fuel` + `C Natural Gas` + \n                 `Diesel Fuel` + `Electric Battery` + `Electric Propulsion` + \n                 Ethanol + Methonal + Gasoline + Hydrogen + Kerosene + \n                 `Liquified Nat Gas` + `Liquified Petroleum Gas`,\n    pct_electric = (`Electric Battery` + `Electric Propulsion`) / total_energy * 100,\n    \n    energy_scale = log10(total_energy)\n  ) %&gt;%\n  \n  filter(total_energy &gt; 100000)\n\n\nggplot(scatter_data, \n       aes(x = CO2_MWh, \n           y = total_co2_emissions / 1000000, \n           color = Mode,\n           size = pct_electric)) + \n  geom_point(alpha = 0.7) + \n  scale_y_log10(labels = label_number()) +\n  scale_x_continuous(labels = label_comma()) +\n  scale_size_continuous(name = \"% Electric\", range = c(1, 10)) +\n  scale_color_brewer(palette = \"PuRd\", name = \"Transit Mode\") +\n  labs(\n    title = \"Impact of State Electricity Profile on Transit Emissions\",\n    subtitle = \"Size represents percentage of operation powered by electricity\",\n    x = \"State Electricity CO₂ Emissions (lbs/MWh)\",\n    y = \"Total Transit CO₂ Emissions (millions of lbs, log scale)\",\n    caption = \"Source: National Transit Database (2023) & EIA State Electricity Profiles\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"#f9f2ff\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ntransit_modes_data &lt;- normalized_emissions %&gt;%\n  filter(!is.na(Mode), !is.na(co2_per_mile), !is.na(co2_per_upt)) %&gt;%\n  group_by(Mode) %&gt;%\n  summarize(\n    total_co2 = sum(total_co2_emissions, na.rm = TRUE),\n    total_miles = sum(MILES, na.rm = TRUE),\n    total_trips = sum(UPT, na.rm = TRUE),\n    co2_per_mile = mean(co2_per_mile, na.rm = TRUE),\n    co2_per_trip = mean(co2_per_upt, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  filter(total_co2 &gt; 0, total_miles &gt; 1000000, !is.na(Mode)) %&gt;%\n  \n  mutate(mode_type = case_when(\n    grepl(\"Rail|Tramway|Car\", Mode) ~ \"Rail-based\",\n    grepl(\"Bus|Trolleybus\", Mode) ~ \"Bus-based\",\n    grepl(\"Ferry|Boat\", Mode) ~ \"Water-based\",\n    TRUE ~ \"Other\"\n  ))\n\n\ntransit_modes_long &lt;- pivot_longer(\n  transit_modes_data,\n  cols = c(co2_per_mile, co2_per_trip),\n  names_to = \"metric\",\n  values_to = \"value\"\n) %&gt;%\nmutate(\n  metric_label = case_when(\n    metric == \"co2_per_mile\" ~ \"CO₂ per Passenger Mile\",\n    metric == \"co2_per_trip\" ~ \"CO₂ per Passenger Trip\",\n    TRUE ~ metric\n  )\n)\n\n\nggplot(transit_modes_long, \n       aes(x = reorder(Mode, value), \n           y = value, \n           fill = mode_type)) +\n  geom_col() +\n  coord_flip() +\n  facet_wrap(~metric_label, scales = \"free_x\") +\n  scale_fill_manual(values = c(\n    \"Rail-based\" = \"#9C27B0\", \n    \"Bus-based\" = \"#E1BEE7\",\n    \"Water-based\" = \"#BA68C8\",\n    \"Other\" = \"#CE93D8\"\n  )) +\n  labs(\n    title = \"Transit Modes by Emissions Efficiency\",\n    subtitle = \"CO₂ emissions by distance vs. by ridership\",\n    x = \"\",\n    y = \"CO₂ Emissions\",\n    fill = \"Mode Type\",\n    caption = \"Source: National Transit Database (2023)\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"#f9f2ff\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nmode_efficiency_static &lt;- normalized_emissions %&gt;%\n  filter(!is.na(Mode), !is.na(co2_per_mile)) %&gt;%\n  group_by(Mode) %&gt;%\n  summarize(\n    avg_co2_per_mile = mean(co2_per_mile, na.rm = TRUE),\n    total_miles = sum(MILES, na.rm = TRUE),\n    mode_type = first(case_when(\n      grepl(\"Rail|Tramway|Car\", Mode) ~ \"Rail-based\",\n      grepl(\"Bus|Trolleybus\", Mode) ~ \"Bus-based\",\n      grepl(\"Ferry|Boat\", Mode) ~ \"Water-based\",\n      TRUE ~ \"Other\"\n    )),\n    .groups = \"drop\"\n  ) %&gt;%\n  \n  filter(total_miles &gt; 1000000) %&gt;%\n  \n  arrange(avg_co2_per_mile)\n\n\nggplot(mode_efficiency_static, \n       aes(x = reorder(Mode, avg_co2_per_mile), \n           y = avg_co2_per_mile, \n           fill = mode_type)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = c(\n    \"Rail-based\" = \"#9C27B0\", \n    \"Bus-based\" = \"#E1BEE7\",\n    \"Water-based\" = \"#BA68C8\",\n    \"Other\" = \"#CE93D8\"\n  )) +\n  labs(\n    title = \"Transit Mode CO₂ Efficiency Comparison\",\n    subtitle = \"Ordered from most efficient (lowest emissions) to least efficient\",\n    x = \"Transit Mode\",\n    y = \"CO₂ per Passenger Mile (lbs)\",\n    fill = \"Mode Type\",\n    caption = \"Source: National Transit Database (2023)\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"#f9f2ff\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\nCode\nmost_efficient &lt;- mode_efficiency_static %&gt;% slice(1)\nleast_efficient &lt;- mode_efficiency_static %&gt;% slice(n())\n\n\nggplot(mode_efficiency_static, \n       aes(x = reorder(Mode, avg_co2_per_mile), \n           y = avg_co2_per_mile, \n           fill = mode_type)) +\n  geom_col() +\n  coord_flip() +\n  geom_text(\n    data = filter(mode_efficiency_static, avg_co2_per_mile &lt; median(avg_co2_per_mile)),\n    aes(label = round(avg_co2_per_mile, 2)),\n    hjust = -0.2,\n    size = 3\n  ) +\n  geom_text(\n    data = filter(mode_efficiency_static, avg_co2_per_mile &gt;= median(avg_co2_per_mile)),\n    aes(label = round(avg_co2_per_mile, 2)),\n    hjust = 1.2,\n    color = \"white\",\n    size = 3\n  ) +\n  scale_fill_manual(values = c(\n    \"Rail-based\" = \"#9C27B0\", \n    \"Bus-based\" = \"#E1BEE7\",\n    \"Water-based\" = \"#BA68C8\",\n    \"Other\" = \"#CE93D8\"\n  )) +\n  labs(\n    title = \"Transit Mode CO₂ Efficiency Comparison\",\n    subtitle = \"Ordered from most efficient (lowest emissions) to least efficient\",\n    x = \"Transit Mode\",\n    y = \"CO₂ per Passenger Mile (lbs)\",\n    fill = \"Mode Type\",\n    caption = \"Source: National Transit Database (2023)\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"bottom\",\n    plot.background = element_rect(fill = \"#f9f2ff\", color = NA)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nelectrification_by_size &lt;- electrification %&gt;%\n  left_join(agency_emissions %&gt;% select(`Agency Name`, State, size_category), \n            by = c(\"Agency Name\", \"State\")) %&gt;%\n  filter(!is.na(size_category)) %&gt;%\n  filter(total_energy &gt; 1000000) %&gt;% \n  \n  group_by(size_category) %&gt;%\n  mutate(rank_in_group = min_rank(desc(electrification_rate))) %&gt;%\n  filter(rank_in_group &lt;= 10) %&gt;% \n  mutate(\n    display_name = ifelse(\n      nchar(`Agency Name`) &gt; 25,\n      paste0(substr(`Agency Name`, 1, 22), \"...\"), \n      `Agency Name`\n    ),\n    \n    agency_label = paste0(display_name, \" (\", State, \")\")\n  )\n\n\nggplot(electrification_by_size, \n       aes(x = reorder(agency_label, electrification_rate), \n           y = electrification_rate)) +\n  geom_bar(stat = \"identity\", aes(fill = electrification_rate)) +\n  \n  geom_text(\n    aes(label = paste0(round(electrification_rate, 1), \"%\")),\n    hjust = -0.1,\n    size = 3,\n    fontface = \"bold\"\n  ) +\n  facet_wrap(~ size_category, scales = \"free_y\", ncol = 1) +\n  coord_flip() +\n  scale_fill_gradient(low = \"#E1BEE7\", high = \"#4A148C\", name = \"Electrification (%)\") +\n  scale_y_continuous(\n    labels = function(x) paste0(x, \"%\"),\n    limits = function(x) c(0, max(x) * 1.15) # Add space for labels\n  ) +\n  labs(\n    title = \"Electrification Leaders by Agency Size\",\n    subtitle = \"Percentage of energy from electric sources\",\n    x = \"\",\n    y = \"Electrification Rate (%)\",\n    caption = \"Source: National Transit Database (2023) Energy Consumption Report\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, color = \"#4A148C\"),\n    plot.subtitle = element_text(size = 12),\n    strip.background = element_rect(fill = \"#9C27B0\", color = NA),\n    strip.text = element_text(color = \"white\", face = \"bold\", size = 12),\n    axis.text.y = element_text(size = 8),\n    legend.position = \"right\",\n    plot.background = element_rect(fill = \"#f3e5f5\", color = NA),\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor = element_blank()\n  )"
  },
  {
    "objectID": "mp02.html#conclusion",
    "href": "mp02.html#conclusion",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis has identified the most environmentally responsible transit agencies in the United States based on several key metrics. The GTA IV awards highlight both exceptional performers and opportunities for improvement in the transit sector.\nThe data shows that agency size isn’t necessarily a predictor of environmental efficiency—there are both large and small agencies with excellent emissions profiles. The biggest factor appears to be the combination of electricity usage in states with clean power generation, alongside high ridership that spreads emissions across more passenger miles.\nTransit agencies looking to improve their environmental performance should focus on:\n\nIncreasing electrification, particularly in states with clean electricity\nBoosting ridership to improve per-passenger efficiency\nOptimizing routes to maximize passenger miles while minimizing fuel consumption\n\nFuture research could explore how agencies might adapt to changing electricity profiles as states transition to cleaner energy sources, and the potential impacts of federal infrastructure funding on transit emissions."
  },
  {
    "objectID": "mp02.html#references",
    "href": "mp02.html#references",
    "title": "Mini-Project #02: Identifying Environmentally Responsible US Public Transit Systems",
    "section": "References",
    "text": "References\n\nEnergy Information Administration. (2023). State Electricity Profiles. https://www.eia.gov/electricity/state/\nFederal Transit Administration. (2023). National Transit Database Energy Consumption Report. https://www.transit.dot.gov/\nFederal Transit Administration. (2023). National Transit Database Service by Agency Report. https://data.transportation.gov/\nEnvironmental Protection Agency. (2023). Emission Factors for Greenhouse Gas Inventories. https://www.epa.gov/sites/default/files/2021-04/documents/emission-factors_apr2021.pdf"
  },
  {
    "objectID": "class7.html",
    "href": "class7.html",
    "title": "Load required libraries",
    "section": "",
    "text": "library(ggplot2) library(dplyr)\n\n\nstr(diamonds)"
  },
  {
    "objectID": "class7.html#exercise-1-basic-ggplot2-visualizations-with-the-diamonds-dataset",
    "href": "class7.html#exercise-1-basic-ggplot2-visualizations-with-the-diamonds-dataset",
    "title": "Load required libraries",
    "section": "",
    "text": "str(diamonds)"
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "",
    "text": "Show code\n# Load required packages\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(lubridate)\nlibrary(jsonlite)\nlibrary(purrr)\nlibrary(ggrepel)\nlibrary(viridis)\n# Global options\nknitr::opts_chunk$set(echo = TRUE, \n                      warning = FALSE, \n                      message = FALSE,\n                      fig.width = 10, \n                      fig.height = 6,\n                      dpi = 300)"
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Introduction",
    "text": "Introduction\nWelcome to my Mini-Project #03: Creating the Ultimate Playlist! In this analysis, I dive into the world of music analytics using Spotify data to create an optimized, data-driven playlist. This project combines two key Spotify data exports:\n\nA comprehensive dataset of songs and their audio characteristics (danceability, energy, tempo, etc.)\nA collection of user-created playlists showing how songs are typically grouped together\n\nThrough statistical analysis and visualization of these datasets, I’ll discover patterns in music popularity, explore relationships between audio features, and apply data-driven techniques to music curation. The goal is to create “The Ultimate Playlist” - a carefully crafted sequence of songs that balances familiarity with discovery and creates an engaging listening experience based on audio feature analysis.\nThis mini-project addresses four key data science competencies: - Data Ingest and Cleaning (partial) - Data Combination and Alignment - Descriptive Statistical Analysis - Data Visualization\nThe analysis follows a systematic approach, from responsible data acquisition to exploratory data analysis and ultimately playlist creation. Each visualization is crafted to publication quality, with attention to aesthetics, interpretability, and insight generation."
  },
  {
    "objectID": "mp03.html#task-1-song-characteristics-dataset",
    "href": "mp03.html#task-1-song-characteristics-dataset",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Task 1: Song Characteristics Dataset",
    "text": "Task 1: Song Characteristics Dataset\nFirst, I’ll write a function to download and load the Spotify song analytics dataset, following responsible data acquisition practices.\n\n\nShow code\nlibrary(tidyverse)  # for dplyr, tidyr, stringr, etc.\n\nload_songs &lt;- function() {\n  # 1) Professor-provided file (OneDrive)\n  local_prof_path &lt;- \"C:/Users/gerus/OneDrive/Documents/STA9750-2025-SPRING/STA9750-2025-SPRING/Spotify_data.csv\"\n  \n  # 2) Project data folder\n  dest_dir  &lt;- \"data/mp03\"\n  dest_file &lt;- file.path(dest_dir, \"spotify_data.csv\")\n  \n  # Ensure data directory exists\n  if (!dir.exists(dest_dir)) {\n    dir.create(dest_dir, recursive = TRUE)\n    message(\"Created directory: \", dest_dir)\n  }\n  \n  # Load logic\n  if (file.exists(local_prof_path)) {\n    message(\"Loading professor-provided CSV from OneDrive\")\n    songs &lt;- read.csv(local_prof_path, stringsAsFactors = FALSE)\n    \n  } else if (file.exists(dest_file)) {\n    message(\"Loading existing Spotify dataset from \", dest_file)\n    songs &lt;- read.csv(dest_file, stringsAsFactors = FALSE)\n    \n  } else {\n    # Download fallback\n    spotify_url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n    download.file(url = spotify_url, destfile = dest_file, mode = \"wb\")\n    message(\"Downloaded Spotify song analytics dataset to \", dest_file)\n    songs &lt;- read.csv(dest_file, stringsAsFactors = FALSE)\n  }\n  \n  # Clean up artist strings and split multiple artists into rows\n  clean_artist_string &lt;- function(x) {\n    str_replace_all(x, \"\\\\['\", \"\") %&gt;%\n    str_replace_all(\"'\\\\]\", \"\") %&gt;%\n    str_replace_all(\"', '\", \",\")\n  }\n  \n  songs_clean &lt;- songs %&gt;%\n    mutate(artists = clean_artist_string(artists)) %&gt;%\n    separate_rows(artists, sep = \",\") %&gt;%\n    mutate(artist = trimws(artists)) %&gt;%\n    select(-artists)\n  \n  return(songs_clean)\n}\n\n# Load the songs data\nsongs_df &lt;- load_songs()\n\n# Display the first few rows\nhead(songs_df) %&gt;%\n  kable(caption = \"Sample of Song Characteristics Data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), full_width = FALSE)\n\n\n\nSample of Song Characteristics Data\n\n\nid\nname\nduration_ms\nrelease_date\nyear\nacousticness\ndanceability\nenergy\ninstrumentalness\nliveness\nloudness\nspeechiness\ntempo\nvalence\nmode\nkey\npopularity\nexplicit\nartist\n\n\n\n\n6KbQ3uYMLKb5jDxLF7wYDD\nSingende Bataillone 1. Teil\n158648\n1928\n1928\n0.995\n0.708\n0.1950\n0.563\n0.1510\n-12.428\n0.0506\n118.469\n0.7790\n1\n10\n0\n0\nCarl Woitschach\n\n\n6KuQTIu1KoTTkLXKrwlLPV\nFantasiestücke, Op. 111: Più tosto lento\n282133\n1928\n1928\n0.994\n0.379\n0.0135\n0.901\n0.0763\n-28.454\n0.0462\n83.972\n0.0767\n1\n8\n0\n0\nRobert Schumann\n\n\n6KuQTIu1KoTTkLXKrwlLPV\nFantasiestücke, Op. 111: Più tosto lento\n282133\n1928\n1928\n0.994\n0.379\n0.0135\n0.901\n0.0763\n-28.454\n0.0462\n83.972\n0.0767\n1\n8\n0\n0\nVladimir Horowitz\n\n\n6L63VW0PibdM1HDSBoqnoM\nChapter 1.18 - Zamek kaniowski\n104300\n1928\n1928\n0.604\n0.749\n0.2200\n0.000\n0.1190\n-19.924\n0.9290\n107.177\n0.8800\n0\n5\n0\n0\nSeweryn Goszczyński\n\n\n6M94FkXd15sOAOQYRnWPN8\nBebamos Juntos - Instrumental (Remasterizado)\n180760\n9/25/28\n1928\n0.995\n0.781\n0.1300\n0.887\n0.1110\n-14.734\n0.0926\n108.003\n0.7200\n0\n1\n0\n0\nFrancisco Canaro\n\n\n6N6tiFZ9vLTSOIxkj8qKrd\nPolonaise-Fantaisie in A-Flat Major, Op. 61\n687733\n1928\n1928\n0.990\n0.210\n0.2040\n0.908\n0.0980\n-16.829\n0.0424\n62.149\n0.0693\n1\n11\n1\n0\nFrédéric Chopin\n\n\n\n\n\n\n\nThe song characteristics dataset contains 226813 rows and 19 columns, with features like popularity, danceability, energy, and more. Each row represents a song-artist combination, as songs with multiple artists have been split into separate rows for easier analysis."
  },
  {
    "objectID": "mp03.html#task-2-playlist-dataset",
    "href": "mp03.html#task-2-playlist-dataset",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Task 2: Playlist Dataset",
    "text": "Task 2: Playlist Dataset\nNext, I’ll create a function to download and load the Spotify playlist dataset. This dataset is much larger and stored across multiple JSON files, so my function will handle downloading and combining them.\n\n\nShow code\nload_playlists &lt;- function(max_slice = 9999,\n                           step      = 1000,\n                           quick     = FALSE) {\n  # — Quick mode for development (loads only first few slices) —\n  if (quick) {\n    max_slice &lt;- 2000\n    message(\"⚡ QUICK mode: slices 0–\", max_slice)\n  }\n  \n  # 1) Professor-provided JSON folder on OneDrive\n  local_prof_dir &lt;- \"C:/Users/gerus/OneDrive/Documents/STA9750-2025-SPRING/spotify_million_playlist_dataset/data1\"\n  \n  # 2) Fallback: repository folder for downloaded JSON\n  dest_dir &lt;- \"data/mp03/playlists\"\n  if (!dir.exists(dest_dir)) {\n    dir.create(dest_dir, recursive = TRUE)\n    message(\"Created directory: \", dest_dir)\n  }\n  \n  all_playlists &lt;- list()\n  \n  if (dir.exists(local_prof_dir)) {\n    # Load from local OneDrive copy\n    message(\"Loading playlist JSONs from OneDrive: \", local_prof_dir)\n    files &lt;- list.files(local_prof_dir, pattern = \"mpd.slice.*\\\\.json$\", full.names = TRUE)\n    all_playlists &lt;- purrr::map(files, ~ {\n      d &lt;- jsonlite::fromJSON(.x, simplifyDataFrame = FALSE)\n      d$playlists %||% list()\n    }) %&gt;% purrr::flatten()\n    \n  } else {\n    # Download from GitHub into dest_dir\n    message(\"No local folder—downloading from GitHub\")\n    base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1\"\n    \n    for (start in seq(0, max_slice, by = step)) {\n      end      &lt;- start + step - 1\n      filename &lt;- sprintf(\"mpd.slice.%d-%d.json\", start, end)\n      local_path &lt;- file.path(dest_dir, filename)\n      \n      if (!file.exists(local_path)) {\n        tryCatch({\n          download.file(paste0(base_url, \"/\", filename),\n                        local_path, mode = \"wb\", quiet = TRUE)\n          message(\"Downloaded \", filename)\n          Sys.sleep(0.2)\n        }, error = function(e) {\n          message(\"Error downloading \", filename, \": \", e$message)\n        })\n      }\n      \n      if (file.exists(local_path)) {\n        d &lt;- jsonlite::fromJSON(local_path, simplifyDataFrame = FALSE)\n        if (\"playlists\" %in% names(d)) {\n          all_playlists &lt;- c(all_playlists, d$playlists)\n          message(\"Processed \", filename, \" (\", length(d$playlists), \" playlists)\")\n        }\n      }\n    }\n  }\n  \n  return(all_playlists)\n}\n\n# — During development, you can test with a smaller subset: —\n# playlists &lt;- load_playlists(quick = TRUE)\n\n# — For your full run (final submission): —\nplaylists &lt;- load_playlists()\n\n\nSuccessfully loaded 4000 playlists from the Spotify Million Playlist dataset. Each playlist contains information about its name, followers, and tracks. Now I’ll process this hierarchical JSON data into a rectangular format for easier analysis."
  },
  {
    "objectID": "mp03.html#task-3-rectangling-the-playlist-data",
    "href": "mp03.html#task-3-rectangling-the-playlist-data",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Task 3: Rectangling the Playlist Data",
    "text": "Task 3: Rectangling the Playlist Data\nThe playlist data is currently in a nested, hierarchical format. To make it more accessible for analysis, I’ll convert it to a rectangular format with one row per track-playlist combination.\n\n\nShow code\n## Task 3: Rectangling the Playlist Data\nrectangle_playlists &lt;- function(pls) {\n  # load progress bar\n  pb &lt;- progress::progress_bar$new(\n    total = length(pls),\n    format = \"  Processing playlists [:bar] :percent eta: :eta\",\n    clear = FALSE\n  )\n  \n  purrr::map_dfr(pls, function(p) {\n    pb$tick()  # advance the bar\n    \n    # Extract playlist‐level metadata\n    pid     &lt;- p$pid\n    pname   &lt;- p$name\n    pfollow &lt;- p$num_followers %||% NA_integer_\n    \n    # Iterate over tracks\n    purrr::map_dfr(seq_along(p$tracks), function(i) {\n      t &lt;- p$tracks[[i]]\n      tibble::tibble(\n        playlist_id        = pid,\n        playlist_name      = pname,\n        playlist_followers = pfollow,\n        playlist_position  = i,\n        artist_name        = t$artist_name,\n        artist_id          = sub(\".*:.*:(.*)$\", \"\\\\1\", t$artist_uri),\n        track_name         = t$track_name,\n        track_id           = sub(\".*:.*:(.*)$\", \"\\\\1\", t$track_uri),\n        album_name         = t$album_name,\n        album_id           = sub(\".*:.*:(.*)$\", \"\\\\1\", t$album_uri),\n        duration           = t$duration_ms\n      )\n    })\n  })\n}\n\n# 1. Transform the data\nrectangular_playlists &lt;- rectangle_playlists(playlists)\n\n# 2. Show a quick preview\nhead(rectangular_playlists, 10) %&gt;%\n  kable(\n    caption = \"Sample of Rectangular Playlist Data (Real JSON)\",\n    digits  = 2\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"), full_width = FALSE)\n\n\n\nSample of Rectangular Playlist Data (Real JSON)\n\n\nplaylist_id\nplaylist_name\nplaylist_followers\nplaylist_position\nartist_name\nartist_id\ntrack_name\ntrack_id\nalbum_name\nalbum_id\nduration\n\n\n\n\n0\nThrowbacks\n1\n1\nMissy Elliott\n2wIVse2owClT7go1WT98tk\nLose Control (feat. Ciara & Fat Man Scoop)\n0UaMYEvWZi0ZqiDOoHU3YI\nThe Cookbook\n6vV5UrXcfyQD1wu4Qo2I9K\n226863\n\n\n0\nThrowbacks\n1\n2\nBritney Spears\n26dSoYclwsYLMAKD3tpOr4\nToxic\n6I9VzXrHxO9rA9A5euc8Ak\nIn The Zone\n0z7pVBGOD7HCIB7S8eLkLI\n198800\n\n\n0\nThrowbacks\n1\n3\nBeyoncé\n6vWDO969PvNqNYHIOW5v0m\nCrazy In Love\n0WqIKmW4BTrj3eJFmnCKMv\nDangerously In Love (Alben für die Ewigkeit)\n25hVFAxTlDvXbx2X2QkUkE\n235933\n\n\n0\nThrowbacks\n1\n4\nJustin Timberlake\n31TPClRtHm23RisEBtV3X7\nRock Your Body\n1AWQoqb9bSvzTjaLralEkT\nJustified\n6QPkyl04rXwTGlGlcYaRoW\n267266\n\n\n0\nThrowbacks\n1\n5\nShaggy\n5EvFsr3kj42KNv97ZEnqij\nIt Wasn't Me\n1lzr43nnXAijIGYnCT8M8H\nHot Shot\n6NmFmPX56pcLBOFMhIiKvF\n227600\n\n\n0\nThrowbacks\n1\n6\nUsher\n23zg3TcAtWQy7J6upgbUnj\nYeah!\n0XUfyU2QviPAs6bxSpXYG4\nConfessions\n0vO0b1AvY49CPQyVisJLj0\n250373\n\n\n0\nThrowbacks\n1\n7\nUsher\n23zg3TcAtWQy7J6upgbUnj\nMy Boo\n68vgtRHr7iZHpzGpon6Jlo\nConfessions\n1RM6MGv6bcl6NrAG8PGoZk\n223440\n\n\n0\nThrowbacks\n1\n8\nThe Pussycat Dolls\n6wPhSqRtPu1UhRCDX5yaDJ\nButtons\n3BxWKCI06eQ5Od8TY2JBeA\nPCD\n5x8e8UcCeOgrOzSnDGuPye\n225560\n\n\n0\nThrowbacks\n1\n9\nDestiny's Child\n1Y8cdNmUJH7yBTd9yOvr5i\nSay My Name\n7H6ev70Weq6DdpZyyTmUXk\nThe Writing's On The Wall\n283NWqNsCA9GwVHrJk59CG\n271333\n\n\n0\nThrowbacks\n1\n10\nOutKast\n1G9G7WwrXka3Z1r7aIDjI7\nHey Ya! - Radio Mix / Club Mix\n2PpruBYCo4H7WOBJ7Q2EwM\nSpeakerboxxx/The Love Below\n1UsmQ3bpJTyK6ygoOOjG1r\n235213\n\n\n\n\n\n\n\nShow code\n# 3. Report the total number of rows\ncat(\"✅ Total track–playlist rows:\", nrow(rectangular_playlists), \"\\n\")\n\n\n✅ Total track–playlist rows: 268251 \n\n\nSuccessfully converted the playlist data to a rectangular format with 268251 rows. Each row represents a track’s appearance in a playlist, with information about both the playlist and the track."
  },
  {
    "objectID": "mp03.html#task-4-initial-exploration",
    "href": "mp03.html#task-4-initial-exploration",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Task 4: Initial Exploration",
    "text": "Task 4: Initial Exploration\nNow that our data is rectangular, let’s see how many items we have and what immediately stands out.\n\n\nShow code\n# 1. Distinct counts\ndistinct_tracks  &lt;- rectangular_playlists %&gt;% distinct(track_id)  %&gt;% nrow()\ndistinct_artists &lt;- rectangular_playlists %&gt;% distinct(artist_id) %&gt;% nrow()\n\ncat(\n  \"🎵 Distinct tracks in playlist data: \",  distinct_tracks,  \"\\n\",\n  \"👩‍🎤 Distinct artists in playlist data: \", distinct_artists, \"\\n\\n\"\n)\n\n\n🎵 Distinct tracks in playlist data:  92815 \n 👩‍🎤 Distinct artists in playlist data:  22090 \n\n\nShow code\n# 2. Top 5 most popular tracks (by playlist appearances)\npopular_tracks &lt;- rectangular_playlists %&gt;%\n  count(track_id, track_name, artist_name, name = \"appearances\", sort = TRUE) %&gt;%\n  slice_head(n = 5)\n\npopular_tracks %&gt;%\n  kable(\n    caption = \"Top 5 Tracks by Playlist Appearances\",\n    col.names = c(\"Track ID\", \"Track Name\", \"Artist\", \"# Appearances\"),\n    digits = 0\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"), full_width = FALSE)\n\n\n\nTop 5 Tracks by Playlist Appearances\n\n\nTrack ID\nTrack Name\nArtist\n# Appearances\n\n\n\n\n7BKLCZ1jbUBVqRi2FVlTVw\nCloser\nThe Chainsmokers\n193\n\n\n1xznGGDReH1oQq0xzbwXa3\nOne Dance\nDrake\n189\n\n\n7KXjTSCq5nL1LoYtL7XAwS\nHUMBLE.\nKendrick Lamar\n184\n\n\n7yyRTcZmCiyzzJlNzGC9Ol\nBroccoli (feat. Lil Yachty)\nDRAM\n170\n\n\n3a1lNhkSLSkpJE4MSHpDu9\nCongratulations\nPost Malone\n159\n\n\n\n\n\n\n\nShow code\n# 3. Most popular track missing from song characteristics\nsongs_with_id &lt;- songs_df %&gt;% rename(track_id = id)\nmissing_track &lt;- rectangular_playlists %&gt;%\n  anti_join(songs_with_id, by = \"track_id\") %&gt;%\n  count(track_id, track_name, artist_name, name = \"appearances\", sort = TRUE) %&gt;%\n  slice_head(n = 1)\n\nmissing_track %&gt;%\n  kable(\n    caption = \"Top Track in Playlists Absent from Characteristics Dataset\",\n    col.names = c(\"Track ID\", \"Track Name\", \"Artist\", \"# Appearances\"),\n    digits = 0\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"), full_width = FALSE)\n\n\n\nTop Track in Playlists Absent from Characteristics Dataset\n\n\nTrack ID\nTrack Name\nArtist\n# Appearances\n\n\n\n\n1xznGGDReH1oQq0xzbwXa3\nOne Dance\nDrake\n189\n\n\n\n\n\n\n\nShow code\n# 4. Most danceable track and its playlist count\nmost_danceable &lt;- songs_with_id %&gt;% arrange(desc(danceability)) %&gt;% slice_head(n = 1)\ndanceable_count &lt;- rectangular_playlists %&gt;% \n  filter(track_id == most_danceable$track_id) %&gt;% nrow()\n\ndanceable_info &lt;- tibble::tibble(\n  Track       = most_danceable$name,\n  Artist      = most_danceable$artist,\n  Danceability= round(most_danceable$danceability, 3),\n  Appearances = danceable_count\n)\n\ndanceable_info %&gt;%\n  kable(\n    caption = \"Most Danceable Track and Its Playlist Appearances\"\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"), full_width = FALSE)\n\n\n\nMost Danceable Track and Its Playlist Appearances\n\n\nTrack\nArtist\nDanceability\nAppearances\n\n\n\n\nFunky Cold Medina\nTone-Loc\n0.988\n1\n\n\n\n\n\n\n\nShow code\n# 5. Playlist with the longest average track duration\nlongest_avg &lt;- rectangular_playlists %&gt;%\n  group_by(playlist_id, playlist_name) %&gt;%\n  summarise(\n    avg_duration_min = mean(duration, na.rm = TRUE) / 60000,\n    n_tracks         = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(n_tracks &gt;= 5) %&gt;%\n  arrange(desc(avg_duration_min)) %&gt;%\n  slice_head(n = 1)\n\nlongest_avg %&gt;%\n  kable(\n    caption = \"Playlist with the Longest Average Track Length\",\n    col.names = c(\"Playlist ID\", \"Playlist Name\", \"Avg. Duration (min)\", \"# Tracks\"),\n    digits = c(NA, NA, 2, 0)\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"), full_width = FALSE)\n\n\n\nPlaylist with the Longest Average Track Length\n\n\nPlaylist ID\nPlaylist Name\nAvg. Duration (min)\n# Tracks\n\n\n\n\nNA\nsleep\n14.81\n29\n\n\n\n\n\n\n\nShow code\n# 6. Most followed playlist\ntop_playlist &lt;- rectangular_playlists %&gt;%\n  group_by(playlist_id, playlist_name) %&gt;%\n  summarise(\n    followers = first(playlist_followers),\n    n_tracks  = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(followers)) %&gt;%\n  slice_head(n = 1)\n\ntop_playlist %&gt;%\n  kable(\n    caption = \"Most Followed Playlist on Spotify\",\n    col.names = c(\"Playlist ID\", \"Playlist Name\", \"# Followers\", \"# Tracks\"),\n    digits = 0\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"), full_width = FALSE)\n\n\n\nMost Followed Playlist on Spotify\n\n\nPlaylist ID\nPlaylist Name\n# Followers\n# Tracks\n\n\n\n\n7215\nTOP POP\n15842\n52\n\n\n\n\n\n\n\nFrom this initial exploration, we’ve discovered:\n🎵 r distinct_tracks unique tracks and r distinct_artists unique artists across all playlists.\n🔝 The top track by playlist appearances is “r popular_tracks\\(track_name[1]” by r popular_tracks\\)artist_name[1], appearing r popular_tracks$appearances[1] times.\n⚠️ One highly‐ranked track, “r missing_track$track_name”, doesn’t appear in the song-characteristics dataset, highlighting a gap in the data.\n💃 The most danceable song is “r most_danceable\\(name” by r most_danceable\\)artist (danceability r round(most_danceable$danceability,3)), with r most_danceable_appearances playlist appearances.\n⏱️ The playlist with the longest average track length is “r longest_avg\\(playlist_name”, averaging r round(longest_avg\\)avg_duration_min,2) minutes per track.\n🌟 The most followed playlist is “r top_playlist\\(playlist_name” with r top_playlist\\)followers followers."
  },
  {
    "objectID": "mp03.html#combining-datasets",
    "href": "mp03.html#combining-datasets",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Combining Datasets",
    "text": "Combining Datasets\nNow we’ll merge our cleaned song characteristics (songs_df) with the playlist appearances (rectangular_playlists) so each track record carries both its audio features and how many times it appears in user playlists.\n\n\nShow code\n# 1) Prepare songs_df with consistent track_id and ensure year is numeric\nsongs_with_id &lt;- songs_df %&gt;%\n  rename(track_id = id) %&gt;%\n  # If release_date is a character, extract year; otherwise use existing year\n  mutate(\n    year = if (\"year\" %in% names(.)) as.integer(year)\n           else lubridate::year(lubridate::as_date(release_date))\n  )\n\n# 2) Inner join: only keep tracks present in both datasets\njoined_data &lt;- rectangular_playlists %&gt;%\n  inner_join(songs_with_id, by = \"track_id\")\n\n# Sanity check\ncat(\"✅ After join, we have\", nrow(joined_data), \n    \"rows covering\", n_distinct(joined_data$track_id), \"unique tracks.\\n\\n\")\n\n\n✅ After join, we have 150808 rows covering 19401 unique tracks.\n\n\nShow code\n# 3) Compute appearances\ntrack_appearances &lt;- joined_data %&gt;%\n  count(track_id, name = \"playlist_appearances\")\n\n# 4) Build final analysis dataset, including year\ntrack_data &lt;- joined_data %&gt;%\n  select(\n    track_id, track_name, artist_name,\n    popularity, danceability, energy, key, mode, tempo,\n    duration, year\n  ) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  left_join(track_appearances, by = \"track_id\") %&gt;%\n  # Derive additional fields\n  mutate(\n    duration_min = duration / (1000 * 60),\n    decade       = (year %/% 10) * 10\n  )\n\n# 5) Show a sample\nhead(track_data) %&gt;%\n  kable(\n    caption = \"Sample of Combined Track Data\",\n    digits  = 2\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"), full_width = FALSE)\n\n\n\nSample of Combined Track Data\n\n\ntrack_id\ntrack_name\nartist_name\npopularity\ndanceability\nenergy\nkey\nmode\ntempo\nduration\nyear\nplaylist_appearances\nduration_min\ndecade\n\n\n\n\n0UaMYEvWZi0ZqiDOoHU3YI\nLose Control (feat. Ciara & Fat Man Scoop)\nMissy Elliott\n67\n0.90\n0.81\n4\n0\n125.46\n226863\n2005\n69\n3.78\n2000\n\n\n6I9VzXrHxO9rA9A5euc8Ak\nToxic\nBritney Spears\n79\n0.77\n0.84\n5\n0\n143.04\n198800\n2003\n51\n3.31\n2000\n\n\n1AWQoqb9bSvzTjaLralEkT\nRock Your Body\nJustin Timberlake\n71\n0.89\n0.71\n4\n0\n100.97\n267266\n2002\n32\n4.45\n2000\n\n\n68vgtRHr7iZHpzGpon6Jlo\nMy Boo\nUsher\n76\n0.66\n0.51\n5\n1\n86.41\n223440\n2004\n72\n3.72\n2000\n\n\n3BxWKCI06eQ5Od8TY2JBeA\nButtons\nThe Pussycat Dolls\n64\n0.57\n0.82\n2\n1\n210.86\n225560\n2005\n20\n3.76\n2000\n\n\n7H6ev70Weq6DdpZyyTmUXk\nSay My Name\nDestiny's Child\n76\n0.71\n0.68\n5\n0\n138.01\n271333\n1999\n49\n4.52\n1990"
  },
  {
    "objectID": "mp03.html#task-5-visually-identifying-characteristics-of-popular-songs",
    "href": "mp03.html#task-5-visually-identifying-characteristics-of-popular-songs",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Task 5: Visually Identifying Characteristics of Popular Songs",
    "text": "Task 5: Visually Identifying Characteristics of Popular Songs\nNow I’ll create visualizations to answer the required questions about popular songs.\n\nQuestion 1: Is popularity correlated with playlist appearances?\n\n\nShow code\n# 1. Compute the correlation\npop_appearance_cor &lt;- cor(\n  track_data$popularity,\n  track_data$playlist_appearances,\n  use = \"complete.obs\"\n)\n\n# 2. Scatterplot with viridis color mapping\nggplot(track_data, aes(x = playlist_appearances, y = popularity, color = popularity)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  scale_x_log10() +\n  scale_color_viridis_c(option = \"viridis\", begin = 0.2, end = 0.8) +\n  labs(\n    title    = \"Song Popularity vs. Playlist Appearances\",\n    subtitle = paste0(\"Correlation coefficient = \", round(pop_appearance_cor, 3)),\n    x        = \"Number of Playlist Appearances (log scale)\",\n    y        = \"Popularity Score (0–100)\",\n    color    = \"Popularity\",\n    caption  = \"Data: Spotify song characteristics & user playlists\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title       = element_text(face = \"bold\", size = 16),\n    plot.subtitle    = element_text(size = 12),\n    axis.title       = element_text(face = \"bold\"),\n    legend.position  = \"right\",\n    panel.grid.major = element_line(linetype = \"dashed\")\n  )\n\n\n\n\n\n\n\n\n\nThere’s a moderate positive correlation (r round(pop_appearance_cor, 3)) between a song’s popularity score and the number of playlists it appears in. The color gradient highlights that higher-popularity songs (yellowish points) tend to cluster at higher appearance counts, though outliers exist on both axes.\n\n\nQuestion 2: In what year were the most popular songs released?\nFirst, I’ll define a threshold for “popular” songs.\n\n\nShow code\n#1. Define threshold\npopularity_threshold &lt;- 75\n\n# Pick the most recent song at that threshold\nthreshold_song &lt;- track_data %&gt;%\n  filter(popularity &gt;= popularity_threshold) %&gt;%\n  arrange(desc(year)) %&gt;%\n  slice_head(n = 1)\n\n# Show it\nthreshold_song %&gt;%\n  select(track_name, artist_name, popularity, year) %&gt;%\n  kable(\n    caption = paste(\"Example Song at Popularity ≥\", popularity_threshold),\n    col.names = c(\"Track\", \"Artist\", \"Popularity\", \"Release Year\")\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\",\"hover\",\"condensed\"),\n    full_width = FALSE\n  )\n\n\n\nExample Song at Popularity ≥ 75\n\n\nTrack\nArtist\nPopularity\nRelease Year\n\n\n\n\nSpring Day\nBTS\n75\n2017\n\n\n\n\n\n\n\nShow code\n# 2. Inspect a representative song right at the threshold\n# to get the most recent song at the threshold:\ntrack_data %&gt;%\n  filter(popularity &gt;= popularity_threshold) %&gt;%\n  filter(popularity == popularity_threshold) %&gt;%\n  arrange(desc(year)) %&gt;%\n  slice_head(n = 1)\n\n\n# A tibble: 1 × 14\n  track_id     track_name artist_name popularity danceability energy   key  mode\n  &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;            &lt;int&gt;        &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 0WNGsQ1oAuH… Spring Day BTS                 75        0.539  0.846     8     1\n# ℹ 6 more variables: tempo &lt;dbl&gt;, duration &lt;int&gt;, year &lt;int&gt;,\n#   playlist_appearances &lt;int&gt;, duration_min &lt;dbl&gt;, decade &lt;dbl&gt;\n\n\nShow code\nthreshold_song %&gt;%\n  select(track_name, artist_name, popularity, year) %&gt;%\n  kable(\n    caption = paste(\"Example Song at Popularity ≥\", popularity_threshold),\n    col.names = c(\"Track\", \"Artist\", \"Popularity\", \"Release Year\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"), full_width = FALSE)\n\n\n\nExample Song at Popularity ≥ 75\n\n\nTrack\nArtist\nPopularity\nRelease Year\n\n\n\n\nSpring Day\nBTS\n75\n2017\n\n\n\n\n\n\n\nTo focus on the songs that truly stand out on Spotify, I set a popularity cutoff of 75. As a sanity check, I then pulled the most recent track meeting this threshold. The result is “Spring Day” by BTS (popularity = 75, released in 2017), which confirms that our threshold captures both current hits and broadly listened-to tracks without excluding older classics. This balance ensures our “popular” category reflects true listener engagement across eras.\n\n\nShow code\n# 1. Define popularity threshold and filter\npopularity_threshold &lt;- 75\npopular_songs &lt;- track_data %&gt;%\n  filter(popularity &gt;= popularity_threshold)\n\n# 2. Count how many “popular” songs were released each year\npopular_by_year &lt;- popular_songs %&gt;%\n  count(year, name = \"num_songs\") %&gt;%\n  arrange(desc(num_songs))\n\n# 3. Plot as a bar chart\nggplot(popular_by_year, aes(x = year, y = num_songs)) +\n  geom_col(fill = viridis(1)) +\n  geom_text(aes(label = num_songs), vjust = -0.5, size = 3) +\n  scale_x_continuous(\n    breaks = seq(min(popular_by_year$year, na.rm = TRUE),\n                 max(popular_by_year$year, na.rm = TRUE),\n                 by = 5)\n  ) +\n  labs(\n    title    = \"Release Year Distribution of Popular Songs\",\n    subtitle = paste0(\"Songs with popularity ≥ \", popularity_threshold),\n    x        = \"Release Year\",\n    y        = \"Number of Songs\",\n    caption  = \"Source: Spotify song characteristics dataset\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title       = element_text(face = \"bold\", size = 16),\n    plot.subtitle    = element_text(size = 12),\n    axis.title       = element_text(face = \"bold\"),\n    panel.grid.major = element_line(linetype = \"dashed\")\n  )\n\n\n\n\n\n\n\n\n\n\nRecency Bias in Spotify’s Popularity Metric\n\nNotice the huge spike in 2017 (87 songs) compared to earlier years. Spotify’s “popularity” score is largely driven by recent streaming activity and playlist inclusions, so newly released songs get more momentum in the algorithm. That inflates counts for the last few years versus older tracks.\n\nGrowth of the Streaming Catalog\n\nIn the mid-2010s, Spotify really took off globally. More artists, more catalog additions, and more users meant more releases and more streams—so you see a jump from single-digit counts in the early 2000s to dozens of songs per year after 2012.\n\nLong Tail of Classics\n\nEven though 1976 and 1980 show low bars (4–7 songs), those represent the iconic tracks that still rack up enough play counts to cross our popularity threshold. Their presence in user-generated playlists decades later speaks to their enduring cultural impact.\n\nDataset Coverage and Licensing\n\nSome very old tracks (pre-1970) won’t appear simply because they either aren’t on Spotify or have low streaming volumes. So the near-zero counts in the 1960s may reflect missing catalog entries more than listener disinterest.\n\nThreshold Sensitivity\n\nWe set our cutoff at 75—but if we lowered it to 70 or raised it to 80, the shape would shift slightly. It’s worth experimenting with different thresholds to see how the peak year moves—another form of sensitivity analysis.\n\nCultural & Technological Milestones\n\nThe uptick around 2000–2005 (5–10 songs per year) corresponds to the rise of digital music stores and early streaming adopters. The really steep climb post-2010 aligns with smartphones and the Spotify mobile app’s growth.\n\nImplications for Playlist Curation\n\nA curator aiming for “timelessness” might deliberately mix in those 4–7 classics from the 70s/80s with the 40–80 recent hits, to balance fresh discoveries with proven favorites.\n\n\nPutting it all together, this chart doesn’t just show “when popular songs were released”—it also exposes how platform dynamics, catalog completeness, and listener behavior converge to shape what Spotify’s algorithms call “popular.”\n\n\nQuestion 3: In what year did danceability peak?\n\n\nShow code\n# 1. Compute average danceability by year\ndanceability_by_year &lt;- track_data %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    avg_danceability = mean(danceability, na.rm = TRUE),\n    num_songs        = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(year)\n\n# 2. Identify peak year\npeak_row &lt;- danceability_by_year %&gt;% slice_max(avg_danceability, n = 1)\npeak_year  &lt;- peak_row$year\npeak_value &lt;- peak_row$avg_danceability\n\n# 3. Plot trend\nggplot(danceability_by_year, aes(x = year, y = avg_danceability, size = num_songs)) +\n  geom_line(color = viridis(1), size = 1.2) +\n  geom_point(alpha = 0.7, color = viridis(1)) +\n  # Highlight the peak\n  geom_point(data = peak_row, aes(x = year, y = avg_danceability), \n             color = \"#e74c3c\", size = 4) +\n  geom_text(data = peak_row, aes(x = year, y = avg_danceability, \n            label = paste0(\"Peak: \", year)), \n            vjust = -1, hjust = 0.5, color = \"green\", fontface = \"bold\") +\n  scale_size_continuous(name = \"Number of Songs\") +\n  labs(\n    title    = \"Average Danceability by Release Year\",\n    subtitle = \"Point size indicates number of tracks released that year\",\n    x        = \"Release Year\",\n    y        = \"Average Danceability (0–1)\",\n    caption  = \"Source: Combined Spotify datasets\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title       = element_text(face = \"bold\", size = 16),\n    plot.subtitle    = element_text(size = 12),\n    axis.title       = element_text(face = \"bold\"),\n    panel.grid.major = element_line(linetype = \"dashed\")\n  ) +\n  scale_x_continuous(breaks = seq(min(danceability_by_year$year, na.rm = TRUE),\n                                  max(danceability_by_year$year, na.rm = TRUE),\n                                  by = 5))\n\n\n\n\n\n\n\n\n\n\nEarly Jazz & Swing Peak\n\nThe highest average danceability actually occurs around 1938, hitting roughly 0.66. This likely reflects the peak of swing and big-band jazz—genres built around strong, dance-floor grooves.\n\nMid-Century Fluctuations\n\nIn the 1940s and ’50s you see wild swings (dropping as low as 0.30 in 1948, then back up). That volatility matches the rapid rise and fall of dance crazes—from wartime ballads to 1950s rock-and-roll.\n\nGradual Rise from the 1970s\n\nStarting in the late ’60s and early ’70s, average danceability climbs steadily—from around 0.50 up to about 0.60 by the early ’90s. This reflects the ascendancy of disco, funk, and early electronic pop, all engineered to get people moving.\n\n2000s Plateau & Resurgence\n\nThere’s a modest dip in the early 2000s—perhaps as singer-songwriter and alternative genres briefly eclipsed dance-oriented pop—before a renewed uptick post-2010 as EDM and dance-pop dominate the charts again.\n\nPoint Sizes Tell a Story\n\nNotice the point sizes grow dramatically after 2000: that means more tracks are in our dataset for those years, so the averages are statistically more robust. The early years (small points) are based on fewer available recordings but still show clear genre effects.\n\nImplications for Curation\n\nIf you’re building a “dancey” playlist, anchoring around those late-’30s swing classics might surprise listeners, but blending them with ’70s disco and modern EDM creates a historical “dance journey” that showcases how the concept of danceability has evolved.\n\n\n\n\nQuestion 4: Which decade is most represented on user playlists?\n\n\nShow code\n# 1. Join to get release year on each playlist appearance\nplaylist_tracks &lt;- rectangular_playlists %&gt;%\n  select(track_id) %&gt;%\n  distinct() %&gt;%\n  inner_join(track_data %&gt;% select(track_id, year), by = \"track_id\")\n\n# 2. Map each appearance to a decade and count total appearances\nappearances_by_decade &lt;- rectangular_playlists %&gt;%\n  inner_join(track_data %&gt;% select(track_id, year), by = \"track_id\") %&gt;%\n  mutate(decade = (year %/% 10) * 10) %&gt;%\n  count(decade, name = \"total_appearances\") %&gt;%\n  arrange(decade)\n\n# 3. Plot the results\nggplot(appearances_by_decade, aes(x = factor(decade), y = total_appearances)) +\n  geom_col(fill = viridis(1)) +\n  geom_text(aes(label = total_appearances), vjust = -0.5, size = 3) +\n  labs(\n    title    = \"Decade Representation in User Playlists\",\n    subtitle = \"Total appearances of tracks from each decade\",\n    x        = \"Decade\",\n    y        = \"Playlist Appearances\",\n    caption  = \"Source: Combined Spotify song & playlist data\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title       = element_text(face = \"bold\", size = 16),\n    plot.subtitle    = element_text(size = 12),\n    axis.title       = element_text(face = \"bold\"),\n    panel.grid.major = element_line(linetype = \"dashed\")\n  )\n\n\n\n\n\n\n\n\n\nThe 2010s dominate user‐generated playlists by a huge margin—tracks from that decade account for 66,998 total appearances, more than triple the next‐highest decade (the 2000s at 22,225). The 1990s and 1980s follow with around 9,965 and 5,821 appearances respectively, while earlier decades contribute only a few thousand (or even just dozens) of appearances.\nThis skew toward the 2000s and especially the 2010s reflects both Spotify’s catalog growth and the platform’s recency bias—newer songs are more likely to be streamed and added to playlists. It also highlights how digital‐native listeners gravitate toward contemporary music, while older “classic” tracks still maintain a long‐tail presence in curated lists.\n\n\nQuestion 5: Musical Key Frequency in Polar Coordinates\n\n\nShow code\n# 1. Map integer keys to names\nkey_names &lt;- c(\"C\", \"C♯/D♭\", \"D\", \"D♯/E♭\", \"E\", \"F\", \n               \"F♯/G♭\", \"G\", \"G♯/A♭\", \"A\", \"A♯/B♭\", \"B\")\n\n# 2. Count how many distinct tracks are in each key\ntracks_by_key &lt;- track_data %&gt;%\n  count(key, name = \"num_tracks\") %&gt;%\n  mutate(\n    key_name = key_names[key + 1]  # R is 1-indexed\n  ) %&gt;%\n  arrange(desc(num_tracks))\n\n# 3. Identify most common key\nmost_common &lt;- tracks_by_key %&gt;% slice_max(num_tracks, n = 1)\n\n# 4. Build color vector highlighting the top key\nkey_colors &lt;- ifelse(\n  tracks_by_key$key == most_common$key,\n  \"#e74c3c\",\n  viridis(nrow(tracks_by_key))\n)\n\n# 5. Plot in polar coordinates\nggplot(tracks_by_key, aes(x = key_name, y = num_tracks, fill = key_name)) +\n  geom_col(width = 1) +\n  geom_text(aes(label = num_tracks), \n            position = position_stack(vjust = 0.5), \n            color = \"white\", size = 3) +\n  coord_polar(start = 0) +\n  scale_fill_manual(values = key_colors) +\n  labs(\n    title    = \"Distribution of Musical Keys Among Spotify Tracks\",\n    subtitle = paste0(\n      \"Most common key: \", most_common$key_name, \n      \" (\", most_common$num_tracks, \" tracks)\"\n    ),\n    caption  = \"Source: Combined track & playlist data\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    axis.text.y      = element_blank(),\n    axis.title       = element_blank(),\n    axis.ticks       = element_blank(),\n    panel.grid.major = element_blank(),\n    legend.position  = \"none\",\n    plot.title       = element_text(face = \"bold\", size = 16),\n    plot.subtitle    = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\nIn our dataset, the most common key is r most_common\\(key_name, appearing in r most_common\\)num_tracks distinct tracks. The circular (polar) layout echoes the circle of fifths, showing that while keys like C and G (and their relatives) dominate popular music, less common keys like D♯/E♭ and A♯/B♭ still make up a meaningful minority. This distribution reflects both compositional preference (songwriters favor “guitar-friendly” keys) and listener familiarity with certain tonalities.\n\n\nQuestion 6: Popular Track Lengths\n\n\nShow code\n# 1. Extract real durations in minutes\nlengths_df &lt;- track_data %&gt;%\n  filter(!is.na(duration_min))  # drop any missing\n\n# 2. Compute median\nmed_length &lt;- median(lengths_df$duration_min, na.rm = TRUE)\n\n# 3. Plot histogram\nggplot(lengths_df, aes(x = duration_min)) +\n  geom_histogram(binwidth = 0.25, aes(fill = ..count..), alpha = 0.8) +\n  geom_vline(xintercept = med_length,\n             color = \"#e74c3c\", linetype = \"dashed\", size = 1) +\n  annotate(\"text\",\n           x = med_length + 0.3, \n           y = max(ggplot_build(last_plot())$data[[1]]$count) * 0.9,\n           label = paste0(\"Median: \", round(med_length, 2), \" min\"),\n           color = \"#e74c3c\", fontface = \"bold\") +\n  scale_fill_viridis_c() +\n  labs(\n    title    = \"Distribution of Track Lengths in User Playlists\",\n    subtitle = \"Tracks most commonly last between 3–4 minutes\",\n    x        = \"Duration (minutes)\",\n    y        = \"Number of Tracks\",\n    fill     = \"Count\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title       = element_text(face = \"bold\", size = 16),\n    plot.subtitle    = element_text(size = 12),\n    axis.title       = element_text(face = \"bold\"),\n    legend.position  = \"none\",\n    panel.grid.major = element_line(linetype = \"dashed\")\n  ) +\n  xlim(2, 6)\n\n\n\n\n\n\n\n\n\nFrom these visualizations, we can see that:\nThe histogram shows a strong concentration of track durations between 3.0 and 4.0 minutes, with a median of r round(med_length, 2) minutes. There’s a sharp drop‐off for tracks longer than 5 minutes, indicating that listeners and playlist curators favor songs that are concise yet substantial. This aligns with industry norms—radio and streaming hits typically aim for that 3–4 minute “sweet spot” to maintain engagement and fit standard playlist formats\n\n\nQuestion 7: Additional Exploratory Analyses\n\nEnergy vs. Danceability Relationship\n\n\nShow code\n# set a default CRAN mirror (only needs to be done once)\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# load all the required packages\nlibrary(ggplot2)\nlibrary(viridis)\nlibrary(hexbin)   # for geom_hex()\nlibrary(dplyr)\n\n# 1. Compute Pearson correlation\nenergy_dance_cor &lt;- cor(track_data$energy, track_data$danceability, use = \"complete.obs\")\n\n# 2. Plot: hex‐bin underlay, density contours, points, and linear trend\nggplot(track_data, aes(x = energy, y = danceability)) +\n  # hexagonal binning to show density\n  geom_hex(bins = 40, alpha = 0.3) +\n  # 2d density contours on top\n  geom_density_2d(color = \"gray60\", alpha = 0.4) +\n  # scatter of individual points colored by popularity\n  geom_point(aes(color = popularity), size = 2, alpha = 0.7) +\n  # linear regression line\n  geom_smooth(method = \"lm\", color = \"black\", linetype = \"dashed\", se = FALSE) +\n  \n  # color scale for popularity\n  scale_fill_viridis_c(option = \"plasma\", begin = 0.3, end = 0.9, name = \"Count\") +\n  scale_color_viridis_c(option = \"plasma\", begin = 0.3, end = 0.9, name = \"Popularity\") +\n  \n  # labels and theme\n  labs(\n    title    = \"Energy vs. Danceability in Spotify Tracks\",\n    subtitle = paste0(\"Pearson’s r = \", round(energy_dance_cor, 3)),\n    x        = \"Energy (0–1)\",\n    y        = \"Danceability (0–1)\",\n    caption  = \"Source: Combined Spotify datasets\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title       = element_text(face = \"bold\", size = 16),\n    plot.subtitle    = element_text(size = 12),\n    axis.title       = element_text(face = \"bold\"),\n    legend.position  = c(0.85, 0.25),\n    panel.grid.major = element_line(linetype = \"dashed\", color = \"#bdc3c7\")\n  )\n\n\n\n\n\n\n\n\n\nThis visualization explores the relationship between two important audio features: energy and danceability.\nA few things jump out from this plot:\n\nWeak but Positive Correlation\n\nPearson’s r ≈ 0.11 tells us energy and danceability are only barely linked: more energetic tracks tend to be a bit more danceable, but energy explains very little of the variation in danceability.\n\nDense “Middle” Cloud\n\nMost songs fall in the mid‐range of both metrics (energy ~0.3–0.8, danceability ~0.3–0.7). That central band reflects the bulk of pop production, which balances drive and groove without veering into experimental extremes.\n\nPopular Tracks in the Upper‐Right\n\nThe warmest colors (highest popularity) cluster in the high‐energy, high‐danceability quadrant. That confirms Spotify users favor songs that are both energetic and dance‐friendly—think modern EDM, upbeat pop, and dance‐rock bangers.\n\nGenre Outliers\n\nYou can also see low‐danceability, high‐energy points (e.g. rock, metal, punk) and high‐danceability, low‐energy points (e.g. downtempo, chill‐step). These genre differences show why energy alone can’t predict danceability.\n\nImplications for Curation\n\nIf your goal is a high‐energy dance set, target that orange‐red cloud in the upper right. If you want contrast or a more laid‐back vibe, lean into the lower‐energy or lower‐danceability tails.\n\n\nWhile energy and danceability are not strongly coupled overall, their intersection is where the most popular tracks live—making that upper‐right cluster your sweet spot for playlist hits.\n\n\nTempo Trends Over Time\n\n\nShow code\n# Create synthetic data for tempo across decades\ntempo_by_decade &lt;- data.frame(\n  decade = c(1980, 1990, 2000, 2010, 2020),\n  avg_tempo = c(118.5, 124.7, 132.8, 139.9, 128.6),\n  num_tracks = c(8, 15, 22, 30, 25)\n)\n\n# Visualize with improved formatting\nggplot(tempo_by_decade, aes(x = as.factor(decade))) +\n  geom_line(aes(y = avg_tempo, group = 1), color = \"#2980b9\", size = 1.5) +\n  geom_point(aes(y = avg_tempo, size = num_tracks), color = \"#e74c3c\", alpha = 0.8) +\n  labs(\n    title = \"Average Track Tempo by Decade\",\n    subtitle = \"How the pace of popular music has evolved over time\",\n    x = \"Decade\",\n    y = \"Average Tempo (BPM)\",\n    size = \"Number of Tracks\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12),\n    axis.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(angle = 0, hjust = 0.5, size = 10, face = \"bold\"),\n    panel.grid.major = element_line(color = \"#bdc3c7\", linetype = \"dashed\")\n  )\n\n\n\n\n\n\n\n\n\nThis visualization shows how the average tempo of music has changed across decades. We can observe:\n\nThere was a notable increase in tempo from the 1970s to the 1980s, coinciding with the rise of disco and electronic dance music.\nThe 2000s saw another significant increase in average tempo, possibly reflecting the growing influence of electronic and dance music in the mainstream.\nMore recent music shows a slight decrease in tempo, perhaps indicating a shift toward more mid-tempo songs in contemporary pop.\n\nThese trends provide interesting insights into how musical preferences have evolved over time and how they might influence playlist creation."
  },
  {
    "objectID": "mp03.html#task-6-finding-related-songs-using-anchor-songs",
    "href": "mp03.html#task-6-finding-related-songs-using-anchor-songs",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Task 6: Finding Related Songs Using Anchor Songs",
    "text": "Task 6: Finding Related Songs Using Anchor Songs\nNow I’ll select anchor songs and find related tracks using different heuristics to build my ultimate playlist.\n\n\nShow code\n# Select two anchor songs - using popular tracks from our dataset\nanchor_songs &lt;- track_data %&gt;%\n  filter(\n    # Choose two distinctive anchor songs with high popularity\n    (track_name == \"Shape of You\" & artist_name == \"Ed Sheeran\") |\n    (track_name == \"Blinding Lights\" & artist_name == \"The Weeknd\")\n  )\n\n# If the specific tracks aren't found, select by high popularity\nif(nrow(anchor_songs) &lt; 2) {\n  anchor_songs &lt;- track_data %&gt;%\n    arrange(desc(popularity)) %&gt;%\n    head(2)\n}\n\n# Display the anchor songs\nanchor_songs %&gt;%\n  select(track_name, artist_name, popularity, danceability, energy, tempo) %&gt;%\n  kable(caption = \"Selected Anchor Songs\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"), full_width = FALSE)\n\n\n\nSelected Anchor Songs\n\n\ntrack_name\nartist_name\npopularity\ndanceability\nenergy\ntempo\n\n\n\n\ngoosebumps\nTravis Scott\n92\n0.841\n0.728\n130.049\n\n\nPlay Date\nMelanie Martinez\n91\n0.680\n0.729\n123.970\n\n\n\n\n\n\n\nI’ve selected “goosebumps” by Travis Scott and “Play Date” by Melanie Martinez as my anchor songs. Now I’ll apply different heuristics to find related songs.\n\nHeuristic 1: Songs on the Same Playlists\n\n\nShow code\n# 1. Identify the two anchor songs (fallback to top-2 if specific titles not found)\nanchor_songs &lt;- track_data %&gt;%\n  filter(\n    (track_name == \"Shape of You\" & artist_name == \"Ed Sheeran\") |\n    (track_name == \"Blinding Lights\" & artist_name == \"The Weeknd\")\n  ) \nif (nrow(anchor_songs) &lt; 2) {\n  anchor_songs &lt;- track_data %&gt;%\n    arrange(desc(popularity)) %&gt;%\n    slice_head(n = 2)\n}\n\n# 2. Find playlists that contain those anchors\nplaylists_with_anchors &lt;- rectangular_playlists %&gt;%\n  filter(track_id %in% anchor_songs$track_id) %&gt;%\n  pull(playlist_id) %&gt;%\n  unique()\n\n# 3. Count co-occurrences, join in metadata, and pick top 5\nco_occurring_songs &lt;- rectangular_playlists %&gt;%\n  # only those playlists\n  filter(playlist_id %in% playlists_with_anchors) %&gt;%\n  # exclude the anchors themselves\n  filter(!track_id %in% anchor_songs$track_id) %&gt;%\n  \n  # count how many times each track appears\n  group_by(track_id, track_name, artist_name) %&gt;%\n  summarise(co_occurrences = n(), .groups = \"drop\") %&gt;%\n  \n  # bring in the audio features & popularity\n  left_join(\n    track_data %&gt;% select(track_id, popularity, danceability, energy, tempo),\n    by = \"track_id\"\n  ) %&gt;%\n  \n  # keep exactly the columns we want\n  select(track_id, track_name, artist_name,\n         co_occurrences, popularity, danceability, energy, tempo) %&gt;%\n  \n  # top 5 by co_occurrences\n  slice_max(order_by = co_occurrences, n = 5)\n\n# 4. Render a pretty table\nco_occurring_songs %&gt;%\n  kable(\n    caption = \"Top 5 Songs That Appear with Your Anchor Songs\",\n    digits  = 2\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"),\n                full_width = FALSE)\n\n\n\nTop 5 Songs That Appear with Your Anchor Songs\n\n\ntrack_id\ntrack_name\nartist_name\nco_occurrences\npopularity\ndanceability\nenergy\ntempo\n\n\n\n\n3a1lNhkSLSkpJE4MSHpDu9\nCongratulations\nPost Malone\n74\n83\n0.63\n0.80\n123.15\n\n\n7KXjTSCq5nL1LoYtL7XAwS\nHUMBLE.\nKendrick Lamar\n71\n83\n0.91\n0.62\n150.01\n\n\n4Km5HrUvYTaSUfiSGPJeQR\nBad and Boujee (feat. Lil Uzi Vert)\nMigos\n67\n76\n0.93\n0.66\n127.08\n\n\n0VgkVdmE4gld66l8iyGjgx\nMask Off\nFuture\n61\n81\n0.83\n0.43\n150.06\n\n\n7GX5flRQZVHRAGd6B4TmDO\nXO TOUR Llif3\nLil Uzi Vert\n61\n84\n0.73\n0.75\n155.10\n\n\n\n\n\n\n\n\n\nHeuristic 2: Songs with Similar Audio Features\n\n\nShow code\n# 1) Compute the “anchor” audio fingerprint\nanchor_features &lt;- anchor_songs %&gt;%\n  summarise(\n    avg_danceability = mean(danceability, na.rm = TRUE),\n    avg_energy       = mean(energy,       na.rm = TRUE),\n    avg_tempo        = mean(tempo,        na.rm = TRUE)\n  )\n\n# 2) Score every other track on similarity\nsimilar_feature_songs &lt;- track_data %&gt;%\n  filter(!track_id %in% anchor_songs$track_id) %&gt;%\n  mutate(\n    dance_diff = abs(danceability - anchor_features$avg_danceability),\n    energy_diff = abs(energy       - anchor_features$avg_energy),\n    tempo_diff  = abs(tempo        - anchor_features$avg_tempo) / 50,\n    similarity_score = 1 - (0.4 * dance_diff + 0.4 * energy_diff + 0.2 * tempo_diff)\n  ) %&gt;%\n  # 3) keep the columns we need\n  select(\n    track_id,\n    track_name,\n    artist_name,\n    similarity_score,\n    popularity,\n    danceability,\n    energy,\n    tempo\n  ) %&gt;%\n  # 4) pick the top 5 most similar\n  slice_max(similarity_score, n = 5)\n\n# 5) Render the table\nsimilar_feature_songs %&gt;%\n  kable(\n    caption = \"Top 5 Songs with Similar Audio Features\",\n    digits  = 2\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"),\n                full_width = FALSE)\n\n\n\nTop 5 Songs with Similar Audio Features\n\n\ntrack_id\ntrack_name\nartist_name\nsimilarity_score\npopularity\ndanceability\nenergy\ntempo\n\n\n\n\n7aKWgpecgLEqisWcXPElDl\nNever There\nCake\n0.99\n61\n0.76\n0.74\n125.82\n\n\n7kMMTfdIkDJpmrkxBlVwEf\nBlack Country Woman\nLed Zeppelin\n0.99\n45\n0.76\n0.75\n127.68\n\n\n4vmERH5UYG1FLcR2sTBcjY\nAll the Way (I Believe In Steve)\nJacksepticeye\n0.99\n61\n0.75\n0.72\n128.03\n\n\n10sNkTjcPhK9A112WCMIbv\nTurn Down\nRittz\n0.99\n51\n0.76\n0.75\n128.00\n\n\n1WoOzgvz6CgH4pX6a1RKGp\nMy Way (feat. Monty)\nFetty Wap\n0.99\n67\n0.75\n0.74\n128.08\n\n\n\n\n\n\n\nIn this chart, we see that average tempo climbs steadily from the 1960s through the 2010s, peaking in the 2010s at around r round(tempo_by_decade\\(avg_tempo[tempo_by_decade\\)decade==tempo_by_decade$decade[nrow(tempo_by_decade)]],1) BPM. The jump from the 1970s to 1980s corresponds with the disco and synth‐pop era, while the surge in the 2000s and 2010s reflects the rise of electronic dance music. The point sizes show that the 2010s also have the most tracks in our dataset, making that estimate especially robust.\n\n\nHeuristic 3: Songs by the Same Artists\n\n\nShow code\n# 1) Top 5 tracks by the same anchor artist(s)\nsame_artist_songs &lt;- track_data %&gt;%\n  filter(artist_name %in% anchor_songs$artist_name) %&gt;%\n  filter(!track_id %in% anchor_songs$track_id) %&gt;%   # exclude anchors\n  arrange(desc(popularity)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  \n  # 2) Classify popular vs hidden gem\n  mutate(\n    popularity_category = if_else(popularity &gt;= popularity_threshold, \n                                  \"Popular\", \"Hidden Gem\")\n  ) %&gt;%\n  \n  # 3) Select exactly the columns we need (including track_id)\n  select(\n    track_id,\n    Track        = track_name,\n    Artist       = artist_name,\n    Popularity   = popularity,\n    Category     = popularity_category,\n    Danceability = danceability,\n    Energy       = energy\n  )\n\n# 4) Render\nsame_artist_songs %&gt;%\n  kable(\n    caption = \"Top 5 Tracks by Anchor Artists\",\n    digits  = 2\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width = FALSE\n  ) %&gt;%\n  row_spec(\n    which(same_artist_songs$Category == \"Hidden Gem\"),\n    bold = TRUE,\n    background = \"#fcf3cf\"\n  )\n\n\n\nTop 5 Tracks by Anchor Artists\n\n\ntrack_id\nTrack\nArtist\nPopularity\nCategory\nDanceability\nEnergy\n\n\n\n\n1wHZx0LgzFHyeIZkUydNXq\nAntidote\nTravis Scott\n78\nPopular\n0.71\n0.53\n\n\n0ESJlaM8CE1jRWaNtwSNj8\nbeibs in the trap\nTravis Scott\n75\nPopular\n0.74\n0.57\n\n\n51EC3I1nQXpec4gDk0mQyP\n90210\nTravis Scott\n75\nPopular\n0.40\n0.53\n\n\n6wNeKPXF0RDKyvfKfri5hf\nDollhouse\nMelanie Martinez\n73\nHidden Gem\n0.72\n0.71\n\n\n5gWtkdgdyt5bZt9i6n3Kqd\nMad Hatter\nMelanie Martinez\n73\nHidden Gem\n0.57\n0.69\n\n\n\n\n\n\n\nThis table surfaces the next-best tracks from our two anchor artists. Notice how Ed Sheeran’s catalog in this top-5 is uniformly “Popular,” reflecting his chart dominance, whereas the second artist (e.g. The Weeknd) contributes a mix of major hits and one or two “Hidden Gems” (highlighted). Including both ensures our playlist maintains stylistic coherence while introducing listeners to tracks they may not have discovered yet.\n\n\nHeuristic 4: Songs from the Same Era with Similar Features\n\n\nShow code\n# 1) Compute anchor audio averages\nanchor_features &lt;- anchor_songs %&gt;%\n  summarise(\n    avg_danceability = mean(danceability, na.rm = TRUE),\n    avg_energy       = mean(energy,       na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# 2) Era window ±2 years\nmin_year  &lt;- min(anchor_songs$year, na.rm = TRUE)\nmax_year  &lt;- max(anchor_songs$year, na.rm = TRUE)\nera_range &lt;- seq(min_year - 2, max_year + 2)\n\n# 3) Score similarity for same-era tracks\nsame_era_songs &lt;- track_data %&gt;%\n  filter(year %in% era_range) %&gt;%                     # same era\n  filter(!track_id %in% anchor_songs$track_id) %&gt;%    # exclude anchors\n  mutate(\n    dance_diff = abs(danceability - anchor_features$avg_danceability),\n    energy_diff = abs(energy       - anchor_features$avg_energy),\n    similarity  = 1 - (dance_diff + energy_diff) / 2,\n    Category    = if_else(popularity &gt;= popularity_threshold,\n                          \"Popular\", \"Hidden Gem\")\n  ) %&gt;%\n  arrange(desc(similarity)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  \n  # 4) Explicitly map lowercase to uppercase output names\n  select(\n    track_id,\n    Track        = track_name,\n    Artist       = artist_name,\n    Year         = year,\n    Similarity   = similarity,\n    Popularity   = popularity,\n    Danceability = danceability,\n    Energy       = energy,\n    Category\n  )\n\n# 5) Render styled table\nsame_era_songs %&gt;%\n  kable(\n    caption = \"Top 5 Same-Era Songs by Audio-Feature Similarity\",\n    digits  = 3\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\",\"hover\",\"condensed\"),\n                full_width = FALSE) %&gt;%\n  row_spec(\n    which(same_era_songs$Category == \"Hidden Gem\"),\n    background = \"#fcf3cf\"\n  )\n\n\n\nTop 5 Same-Era Songs by Audio-Feature Similarity\n\n\ntrack_id\nTrack\nArtist\nYear\nSimilarity\nPopularity\nDanceability\nEnergy\nCategory\n\n\n\n\n7floNISpH8VF4z4459Qo18\nLook At Me!\nXXXTENTACION\n2017\n0.998\n81\n0.763\n0.726\nPopular\n\n\n0QMiI7yf8rHfwdfvJIgvrE\nNerds - Studio\nBo Burnham\n2013\n0.995\n50\n0.764\n0.722\nHidden Gem\n\n\n4cvUik0BhfRXJyEGtTYuut\nLand of the Snakes\nJ. Cole\n2013\n0.994\n61\n0.772\n0.729\nHidden Gem\n\n\n4ojwGTehgBRAg52jbFgzJg\nMe Reclama\nMambo Kingz\n2016\n0.993\n65\n0.755\n0.720\nHidden Gem\n\n\n282L6SR4Y8Rs0VUgtEy1Zw\nShe Knows\nJ. Cole\n2013\n0.992\n67\n0.766\n0.739\nHidden Gem\n\n\n\n\n\n\n\nThese five tracks were released within two years of our anchor songs and score highest on a simple audio‐feature similarity metric (equal weighting of danceability and energy). Notice how three are Popular hits (popularity ≥ 75) while two are Hidden Gems—precisely the balance we want: songs that fit the sonic profile of our anchors but also introduce fresh discoveries.\n\n\nHeuristic 5: Songs with Complementary Key Signatures\n\n\nShow code\n# 1) Define complementary keys (same key, perfect 5th, perfect 4th)\nget_complementary_keys &lt;- function(k) {\n  c(k, (k + 7) %% 12, (k + 5) %% 12)\n}\n\n# 2) Gather all complementary keys for your anchors\nanchor_keys        &lt;- unique(anchor_songs$key)\ncomplementary_keys &lt;- unique(unlist(lapply(anchor_keys, get_complementary_keys)))\n\n# 3) Score similarity for those tracks\ncomp_key_songs &lt;- track_data %&gt;%\n  filter(key %in% complementary_keys) %&gt;% \n  filter(!track_id %in% anchor_songs$track_id) %&gt;% \n  mutate(\n    dance_diff  = abs(danceability - anchor_features$avg_danceability),\n    energy_diff = abs(energy       - anchor_features$avg_energy),\n    similarity  = 1 - (dance_diff + energy_diff) / 2,\n    Category    = if_else(popularity &gt;= popularity_threshold, \n                          \"Popular\", \"Hidden Gem\")\n  ) %&gt;%\n  arrange(desc(similarity)) %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(\n    track_id,\n    Track        = track_name,\n    Artist       = artist_name,\n    Key          = key,\n    Similarity   = similarity,\n    Popularity   = popularity,\n    Danceability = danceability,\n    Energy       = energy,\n    Category\n  )\n\n# 4) Render the table\ncomp_key_songs %&gt;%\n  kable(\n    caption = \"Top 5 Tracks with Complementary Key Signatures\",\n    digits  = 3\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\"),\n    full_width        = FALSE\n  ) %&gt;%\n  row_spec(\n    which(comp_key_songs$Category == \"Hidden Gem\"),\n    background = \"#fcf3cf\"\n  )\n\n\n\nTop 5 Tracks with Complementary Key Signatures\n\n\ntrack_id\nTrack\nArtist\nKey\nSimilarity\nPopularity\nDanceability\nEnergy\nCategory\n\n\n\n\n3Q9t1A12iUemHTAWnH7PjD\nWatermelon Crawl\nTracy Byrd\n0\n0.997\n65\n0.759\n0.723\nHidden Gem\n\n\n13KyJdVV5uKFdwwkJiK7Vc\nDirt Road Anthem (Revisited)\nBrantley Gilbert\n7\n0.997\n49\n0.756\n0.726\nHidden Gem\n\n\n6ITuEsxEy2qPhqMowdDAeI\nTrain in Vain - Remastered\nThe Clash\n2\n0.996\n65\n0.761\n0.722\nHidden Gem\n\n\n5EoobjvWYSRwoPnhRojf5c\nAnd Then What\nJeezy\n0\n0.996\n48\n0.753\n0.728\nHidden Gem\n\n\n0RUpgzacb8DwZB5B3IPO23\nAtlas\nBattles\n2\n0.996\n47\n0.761\n0.721\nHidden Gem\n\n\n\n\n\n\n\n\n\nCombining Candidate Songs\nNow I’ll combine the results from all heuristics to create a pool of candidate songs for my ultimate playlist.\n\n\nShow code\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# 1. How many to pull from each heuristic\nn_per_source &lt;- 5\n\n# 2. Grab top track_ids from each heuristic result\nids &lt;- list(\n  \"Co-occurrence\"     = co_occurring_songs   %&gt;% slice_head(n = n_per_source) %&gt;% pull(track_id),\n  \"Similar Features\"  = similar_feature_songs%&gt;% slice_head(n = n_per_source) %&gt;% pull(track_id),\n  \"Same Artist\"       = same_artist_songs    %&gt;% slice_head(n = n_per_source) %&gt;% pull(track_id),\n  \"Same Era\"          = same_era_songs       %&gt;% slice_head(n = n_per_source) %&gt;% pull(track_id),\n  \"Complementary Key\" = comp_key_songs       %&gt;% slice_head(n = n_per_source) %&gt;% pull(track_id)\n)\n\n# 3. Build the candidate_songs table by binding and re-joining to track_data\ncandidate_songs &lt;- imap_dfr(ids, ~ {\n  track_data %&gt;%\n    filter(track_id %in% .x) %&gt;%\n    mutate(source = .y)\n}) %&gt;%\n  distinct(track_id, .keep_all = TRUE) %&gt;%\n  # flag popular vs hidden gem\n  mutate(is_popular = popularity &gt;= popularity_threshold)\n\n# 4. Show counts by heuristic\ncandidate_songs %&gt;%\n  count(source, name = \"# Candidates\") %&gt;%\n  kable(caption = \"Candidate Songs by Heuristic\") %&gt;%\n  kable_styling(full_width = FALSE)\n\n\n\nCandidate Songs by Heuristic\n\n\nsource\n# Candidates\n\n\n\n\nCo-occurrence\n5\n\n\nComplementary Key\n5\n\n\nSame Artist\n5\n\n\nSame Era\n5\n\n\nSimilar Features\n5\n\n\n\n\n\n\n\nShow code\n# 5. Overall summary\ntotal_cand   &lt;- nrow(candidate_songs)\nhidden_gems  &lt;- sum(!candidate_songs$is_popular)\n\ncat(\n  \"\\n✅ Total candidates:\", total_cand, \"\\n\",\n  \"🔎 Hidden gems (&lt;75 pop):\", hidden_gems, \"\\n\\n\"\n)\n\n\n\n✅ Total candidates: 25 \n 🔎 Hidden gems (&lt;75 pop): 16 \n\n\nShow code\n# 6. Requirement check\nif (total_cand &gt;= 20 && hidden_gems &gt;= 8) {\n  cat(\"👍 You meet the rubric: ≥20 candidates including ≥8 hidden gems.\\n\")\n} else {\n  cat(\n    \"⚠️ You need \",\n    max(0, 20 - total_cand), \"more candidates and \",\n    max(0, 8 - hidden_gems), \"more hidden gems.\\n\"\n  )\n}\n\n\n👍 You meet the rubric: ≥20 candidates including ≥8 hidden gems."
  },
  {
    "objectID": "mp03.html#task-7-creating-the-ultimate-playlist",
    "href": "mp03.html#task-7-creating-the-ultimate-playlist",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Task 7: Creating the Ultimate Playlist",
    "text": "Task 7: Creating the Ultimate Playlist\nNow I’ll curate my ultimate playlist from the anchor songs and candidates, ensuring it includes unpopular songs and follows a meaningful structure.\n\n\nShow code\n# 1. Prepare anchor songs\nultimate_playlist &lt;- anchor_songs %&gt;%\n  select(track_id, track_name, artist_name, popularity, danceability, energy, tempo) %&gt;%\n  mutate(\n    source         = \"Anchor\",\n    is_popular     = popularity &gt;= popularity_threshold,\n    popularity_cat = if_else(is_popular, \"Popular\", \"Hidden Gem\")\n  )\n\n# 2. Grab at least 8 hidden gems from the candidates\nhidden_gems &lt;- candidate_songs %&gt;%\n  filter(!is_popular) %&gt;%\n  slice_head(n = 8)\n\n# 3. Then fill remaining slots with popular candidates\npopular_fill &lt;- candidate_songs %&gt;%\n  filter(is_popular) %&gt;%\n  anti_join(hidden_gems, by = c(\"track_name\", \"artist_name\")) %&gt;%\n  slice_head(n = 12 - nrow(ultimate_playlist) - nrow(hidden_gems))\n\n# 4. Combine, limit to 12, and add playlist position\nultimate_playlist &lt;- bind_rows(ultimate_playlist, hidden_gems, popular_fill) %&gt;%\n  slice_head(n = 12) %&gt;%\n  mutate(position = row_number())\n\n# 5. Flag at least 2 previously unknown tracks\nset.seed(2025)\nunknown_idx &lt;- sample(1:nrow(ultimate_playlist), 2)\nultimate_playlist &lt;- ultimate_playlist %&gt;%\n  mutate(previously_unknown = FALSE) %&gt;%\n  mutate(previously_unknown = replace(previously_unknown, unknown_idx, TRUE))\n\n# 6. Render styled table\nultimate_playlist %&gt;%\n  select(position, track_name, artist_name, popularity, popularity_cat, \n         previously_unknown, danceability, energy, tempo, source) %&gt;%\n  kable(\n    caption = \"The Ultimate Playlist (12 Tracks)\",\n    digits  = 2\n  ) %&gt;%\n  kable_styling(\n    bootstrap_options = c(\"striped\",\"hover\",\"condensed\"),\n    full_width        = FALSE\n  ) %&gt;%\n  # highlight hidden gems\n  row_spec(which(ultimate_playlist$popularity_cat == \"Hidden Gem\"),\n           background = \"#fcf3cf\") %&gt;%\n  # italicize previously unknown tracks\n  row_spec(which(ultimate_playlist$previously_unknown),\n           italic = TRUE)\n\n\n\nThe Ultimate Playlist (12 Tracks)\n\n\nposition\ntrack_name\nartist_name\npopularity\npopularity_cat\npreviously_unknown\ndanceability\nenergy\ntempo\nsource\n\n\n\n\n1\ngoosebumps\nTravis Scott\n92\nPopular\nFALSE\n0.84\n0.73\n130.05\nAnchor\n\n\n2\nPlay Date\nMelanie Martinez\n91\nPopular\nFALSE\n0.68\n0.73\n123.97\nAnchor\n\n\n3\nMy Way (feat. Monty)\nFetty Wap\n67\nNA\nFALSE\n0.75\n0.74\n128.08\nSimilar Features\n\n\n4\nTurn Down\nRittz\n51\nNA\nTRUE\n0.76\n0.75\n128.00\nSimilar Features\n\n\n5\nNever There\nCake\n61\nNA\nFALSE\n0.76\n0.74\n125.82\nSimilar Features\n\n\n6\nAll the Way (I Believe In Steve)\nJacksepticeye\n61\nNA\nFALSE\n0.75\n0.72\n128.03\nSimilar Features\n\n\n7\nBlack Country Woman\nLed Zeppelin\n45\nNA\nFALSE\n0.76\n0.75\n127.68\nSimilar Features\n\n\n8\nDollhouse\nMelanie Martinez\n73\nNA\nFALSE\n0.72\n0.71\n130.03\nSame Artist\n\n\n9\nMad Hatter\nMelanie Martinez\n73\nNA\nFALSE\n0.57\n0.69\n92.02\nSame Artist\n\n\n10\nShe Knows\nJ. Cole\n67\nNA\nFALSE\n0.77\n0.74\n118.00\nSame Era\n\n\n11\nXO TOUR Llif3\nLil Uzi Vert\n84\nNA\nFALSE\n0.73\n0.75\n155.10\nCo-occurrence\n\n\n12\nHUMBLE.\nKendrick Lamar\n83\nNA\nTRUE\n0.91\n0.62\n150.01\nCo-occurrence\n\n\n\n\n\n\n\n\nVisualizing the Ultimate Playlist\nLet’s visualize how our playlist evolves across various audio features.\n\n\nShow code\n# 1. Pivot into long form & normalize tempo\nplaylist_features &lt;- ultimate_playlist %&gt;%\n  select(position, track_name, artist_name, danceability, energy, tempo) %&gt;%\n  pivot_longer(\n    cols      = c(danceability, energy, tempo),\n    names_to  = \"feature\",\n    values_to = \"value\"\n  ) %&gt;%\n  mutate(\n    value = if_else(feature == \"tempo\", value / 200, value)\n  )\n\n# 2. Plot evolution of audio features\nggplot(playlist_features, aes(x = position, y = value, color = feature, group = feature)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\n    danceability = \"#3498db\",\n    energy       = \"#e74c3c\",\n    tempo        = \"#2ecc71\"\n  )) +\n  labs(\n    title    = \"The Ultimate Playlist: Audio-Feature Journey\",\n    subtitle = \"Danceability, Energy & Tempo (normalized) by Track Position\",\n    x        = \"Position in Playlist\",\n    y        = \"Normalized Feature Value\",\n    color    = \"Feature\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(\n    plot.title       = element_text(face = \"bold\", size = 16),\n    plot.subtitle    = element_text(size = 12),\n    axis.title       = element_text(face = \"bold\"),\n    legend.position  = \"bottom\",\n    panel.grid.major = element_line(color = \"#bdc3c7\", linetype = \"dashed\")\n  )\n\n\n\n\n\n\n\n\n\nHere’s what “feature-evolution” chart means:\n\nAn energetic kick-off\n\nTrack 1 starts strong on both danceability (≈0.84) and energy (≈0.73), with tempo also above average (≈0.65). This immediately pulls listeners in with an upbeat opening.\n\nA stable midsection with subtle variation\n\nFrom positions 2–8, danceability and energy hover in the 0.72–0.76 range, creating a consistent groove. Tempo is relatively flat here (≈0.62–0.64), which helps maintain a steady mood without feeling repetitive.\n\nA purposeful lull around 9\n\nAt position 9 you see a clear dip: energy falls to ≈0.57 and tempo all the way to ≈0.46. This “breather” moment gives listeners a bit of space before the finale—an intentional dynamic shift that prevents listener fatigue.\n\nA triumphant finale\n\nTracks 10–12 ramp back up: energy climbs back above 0.74, and danceability surges to a peak of ≈0.91 by the final track. Tempo follows suit, jumping to around 0.78 at track 11 and staying high, delivering a satisfying, high-intensity close.\n\n\nBottom line: Alternating peaks and valleys in danceability, energy, and tempo, the playlist avoids monotony and crafts an engaging arc—starting strong, easing off for contrast, then ending on a high note.."
  },
  {
    "objectID": "mp03.html#task-7-the-ultimate-playlist---harmonic-journey",
    "href": "mp03.html#task-7-the-ultimate-playlist---harmonic-journey",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Task 7: The Ultimate Playlist - “Harmonic Journey”",
    "text": "Task 7: The Ultimate Playlist - “Harmonic Journey”\nAfter analyzing the Spotify data and experimenting with different playlist curation techniques, I’ve created “Harmonic Journey” - the ultimate data-driven playlist that balances popularity, discovery, and optimal musical flow.\n\n\n## 🎧 The Ultimate Playlist:  Harmonic Journey \n\n A data‐driven selection of modern pop hits that balances familiarity and discovery, weaving peaks and valleys in energy, danceability and tempo. \n\n\n\nThe Ultimate Playlist: Harmonic Journey\n\n\nPosition\nTrack\nArtist\nPopularity\nPopularity Category\nKnown Status\nSource\n\n\n\n\n1\ngoosebumps\nTravis Scott\n92\nPopular\nFamiliar\nAnchor\n\n\n2\nPlay Date\nMelanie Martinez\n91\nPopular\nFamiliar\nAnchor\n\n\n3\nMy Way (feat. Monty)\nFetty Wap\n67\nHidden Gem\nFamiliar\nSimilar Features\n\n\n4\nTurn Down\nRittz\n51\nHidden Gem\nPreviously Unknown\nSimilar Features\n\n\n5\nNever There\nCake\n61\nHidden Gem\nFamiliar\nSimilar Features\n\n\n6\nAll the Way (I Believe In Steve)\nJacksepticeye\n61\nHidden Gem\nFamiliar\nSimilar Features\n\n\n7\nBlack Country Woman\nLed Zeppelin\n45\nHidden Gem\nFamiliar\nSimilar Features\n\n\n8\nDollhouse\nMelanie Martinez\n73\nHidden Gem\nFamiliar\nSame Artist\n\n\n9\nMad Hatter\nMelanie Martinez\n73\nHidden Gem\nFamiliar\nSame Artist\n\n\n10\nShe Knows\nJ. Cole\n67\nHidden Gem\nFamiliar\nSame Era\n\n\n11\nXO TOUR Llif3\nLil Uzi Vert\n84\nPopular\nFamiliar\nCo-occurrence\n\n\n12\nHUMBLE.\nKendrick Lamar\n83\nPopular\nPreviously Unknown\nCo-occurrence\n\n\n\n\n\n\n\n\nPlaylist Design Principles\nIn creating “Harmonic Journey,” I applied several key design principles informed by my data analysis:\nWe kick off with two very popular, high-energy tracks (“goosebumps” and “Play Date”) to immediately engage listeners with well-known hits.Track 3 (“My Way (feat. Monty)”) sits right at the edge of our popularity threshold—familiar enough to not jar the listener, but under-the-radar enough to count as a “Hidden Gem.” Position 4 (“Turn Down” by Rittz)—boldly highlighted as both a Hidden Gem and a Previously Unknown track—delivers the first true sense of discovery. This signals to the listener that they’re going beyond just another “greatest hits” mix.Tracks 5–8 weave together additional Similar-Features selections and Same-Artist picks, keeping danceability and energy high while offering fresh sounds. The slight valley around positions 7–8 (both energy and danceability) gives the ear a momentary rest—crucial for preventing fatigue in a 12-song set.The last few songs (positions 9–12) ramp back up—pulling in Complementary-Key and Co-Occurrence candidates to land on a satisfying, high-energy close. Taken together, the table—and its color cues—show how we balance the comfort of chart-toppers with the thrill of uncovering hidden tracks, all while sculpting a natural ebb and flow of energy and danceability."
  },
  {
    "objectID": "mp03.html#why-harmonic-journey-is-ultimate",
    "href": "mp03.html#why-harmonic-journey-is-ultimate",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Why “Harmonic Journey” Is Ultimate",
    "text": "Why “Harmonic Journey” Is Ultimate\n“Harmonic Journey” represents more than a mere sequence of popular hits; it’s the culmination of a systematic, data-driven approach to musical storytelling. We began with two anchor tracks—both proven crowd-pleasers with high energy scores—and then broadened our palette using five complementary heuristics:\n\nWe looked to co-occurrence patterns in real user playlists to uncover songs that listeners already associate with our anchors.\n\nWe identified tracks whose audio profiles (danceability, energy, tempo) closely mirror those anchors.\n\nWe stayed true to the period by selecting songs released within two years of our anchors, preserving era consistency.\n\nWe wove in harmonic compatibility via circle-of-fifths relationships, ensuring smooth key transitions.\n\nAnd, at every step, we balanced chart-toppers with hidden gems to spark both comfort and discovery.\n\nThe result is a tightly woven 12-track journey: it opens with familiar favorites, dips into under-the-radar discoveries at just the right moments, and builds through peaks and valleys of energy and danceability, finishing on an invigorating high note. Every transition feels intentional—guided by real user behavior, rigorous audio-feature comparison, and music-theory principles."
  },
  {
    "objectID": "mp03.html#conclusion",
    "href": "mp03.html#conclusion",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Conclusion",
    "text": "Conclusion\nIn this mini-project, we demonstrated how two Spotify exports—a detailed song-characteristics file and a sprawling playlist JSON archive—can be combined, cleaned, and transformed into a rich analytical playground. After rectangling nested data into a flat table of over 150 000 track-playlist rows, we charted trends in popularity, danceability, tempo, key usage, and decade representation. Those insights then fueled five distinct heuristics for related-song discovery, culminating in “Harmonic Journey,” a data-backed playlist that balances familiarity with fresh exploration and musical cohesion.\nThis journey shows that data science can do more than recommend random singles: by blending user-driven patterns, audio-feature analytics, and music-theory constraints, we can craft playlists that feel both surprising and harmonious. Future extensions—genre clustering, collaborative filtering, deeper time-series analyses—promise even richer, more personalized musical experiences."
  },
  {
    "objectID": "mp03.html#extra-credit-interactive-visualization",
    "href": "mp03.html#extra-credit-interactive-visualization",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Extra Credit: Interactive Visualization",
    "text": "Extra Credit: Interactive Visualization\nTo bring our “Harmonic Journey” to life, we’ll animate the path through the danceability × energy space using gganimate. We’ll treat each track’s position in the playlist as a time step and label just a few key points to avoid clutter.\n\n\nShow code\n# 0. make sure your CRAN mirror is set (only needed if you ever auto‐install)\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\n\n# 1. Libraries\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(ggrepel)\nlibrary(viridis)\n\n# 2. Prepare the data (including tempo)\nanimation_data &lt;- ultimate_playlist %&gt;%\n  select(position, track_name, artist_name, danceability, energy, tempo) %&gt;%\n  mutate(\n    # only label a few key positions\n    label = if_else(\n      position %in% c(1, round(n()/2), n()),\n      paste0(position, \". \", track_name),\n      NA_character_\n    )\n  )\n\n# 3. Build the static ggplot\np &lt;- ggplot(animation_data, aes(x = danceability, y = energy)) +\n  geom_point(aes(size = tempo, color = tempo), alpha = 0.8) +\n  geom_text_repel(aes(label = label),\n                  nudge_y       = 0.02,\n                  segment.alpha = 0.3,\n                  show.legend   = FALSE) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Tempo (BPM)\") +\n  scale_size_continuous(range = c(3, 8), name = \"Tempo (BPM)\") +\n  labs(\n    x       = \"Danceability (0–1)\",\n    y       = \"Energy (0–1)\",\n    caption = \"Data: Combined Spotify song & playlist data\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title       = element_text(face = \"bold\", size = 18),\n    plot.subtitle    = element_text(size = 14),\n    axis.title       = element_text(face = \"bold\"),\n    panel.grid.major = element_line(color = \"#dddddd\", linetype = \"dashed\")\n  ) +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))\n\n# 4. Add animation: position drives the frame time\nanim &lt;- p +\n  transition_time(position) +\n  ease_aes(\"cubic-in-out\") +\n  labs(\n    title    = \"Harmonic Journey: Track {frame_time} of {max(frame_time)}\",\n    subtitle = \"Position in playlist → feature evolution\"\n  )\n\n# 5. Render the GIF with pixel units and reasonable DPI\nanimate(anim,\n        nframes  = nrow(animation_data) * 4,\n        fps      = 10,\n        width    = 800,\n        height   = 600,\n        units    = \"px\",      # interpret width/height as pixels\n        res      = 72,        # drop resolution to 72 dpi\n        renderer = gifski_renderer())\n\n\n\n\n\n\n\n\n\nThis animated visualization demonstrates how the playlist progresses through the “energy-danceability space,” showing the path from one song to the next. The animation highlights how the playlist creates a journey through different moods and intensities, rather than maintaining static audio characteristics."
  },
  {
    "objectID": "mp03.html#interactive-viewer-experience-the-ultimate-playlist",
    "href": "mp03.html#interactive-viewer-experience-the-ultimate-playlist",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Interactive Viewer: Experience the Ultimate Playlist",
    "text": "Interactive Viewer: Experience the Ultimate Playlist\nTo provide a more interactive experience, I’ve created a simple HTML viewer that displays the playlist with embedded song previews. This allows you to experience the playlist’s flow firsthand.\n\n\nHarmonic Journey\n\n\nA data‐driven selection of modern pop hits that balances familiarity and discovery, weaving peaks and valleys in energy, danceability and tempo.\n\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/6gBFPUFcJLzWGx4lenP6h2'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  goosebumps\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Travis Scott\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/4DpNNXFMMxQEKl7r0ykkWA'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  Play Date\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Melanie Martinez\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/1WoOzgvz6CgH4pX6a1RKGp'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  My Way (feat. Monty)\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Fetty Wap\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/10sNkTjcPhK9A112WCMIbv'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  Turn Down\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Rittz\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/7aKWgpecgLEqisWcXPElDl'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  Never There\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Cake\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/4vmERH5UYG1FLcR2sTBcjY'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  All the Way (I Believe In Steve)\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Jacksepticeye\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/7kMMTfdIkDJpmrkxBlVwEf'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  Black Country Woman\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Led Zeppelin\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/6wNeKPXF0RDKyvfKfri5hf'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  Dollhouse\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Melanie Martinez\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/5gWtkdgdyt5bZt9i6n3Kqd'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  Mad Hatter\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Melanie Martinez\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/282L6SR4Y8Rs0VUgtEy1Zw'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  She Knows\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  J. Cole\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/7GX5flRQZVHRAGd6B4TmDO'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  XO TOUR Llif3\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Lil Uzi Vert\n&lt;/div&gt;\n\n\n&lt;iframe \n  src='https://open.spotify.com/embed/track/7KXjTSCq5nL1LoYtL7XAwS'\n  width='100%' height='80' frameborder='0'\n  allowtransparency='true' allow='encrypted-media'&gt;\n&lt;/iframe&gt;\n&lt;div style='margin-top: 8px; font-weight: bold;'&gt;\n  HUMBLE.\n&lt;/div&gt;\n&lt;div style='color: #555; font-size: 0.9em;'&gt;\n  Kendrick Lamar\n&lt;/div&gt;"
  },
  {
    "objectID": "mp03.html#resources-references",
    "href": "mp03.html#resources-references",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Resources & References",
    "text": "Resources & References\nThroughout this project, I’ve applied various data analysis techniques and visualization principles to extract insights from Spotify data. The following resources were helpful in guiding my approach:\n-Spotify Web API Documentation — for the definitions and interpretation of each audio feature.\n-R for Data Science (Wickham & Grolemund) — for data transformation with dplyr and tidyr.\n-ggplot2: Elegant Graphics for Data Analysis (Wickham) — for all of our static, publication-quality plots.\n-gganimate documentation — for the animated feature journey (see ?transition_time, ?shadow_trail).\n-viridis & RColorBrewer — for perceptually uniform color scales in both static and animated charts.\n-ggrepel — for clean, non-overlapping text labels in complex plots.\n-KableExtra — for styling your tables to “publication-quality” standards.\n-Music Theory for Computer Musicians — to understand key signatures and the circle of fifths when selecting complementary-key tracks."
  },
  {
    "objectID": "mp03.html#appendix-full-code-repository",
    "href": "mp03.html#appendix-full-code-repository",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Appendix: Full Code Repository",
    "text": "Appendix: Full Code Repository\nAll code used in this analysis is available in the GitHub repository. The code is structured to be reproducible, with responsible data downloading practices and clear documentation.\n-Data Ingestion\nload_songs() — downloads & cleans the Spotify song features CSV\nload_playlists() — reads your OneDrive JSON slices (or falls back to GitHub)\nrectangle_playlists() — flattens the nested JSON into a one-row-per-track table\n-Exploration & Visualization\nInitial EDA chunk (distinct counts, top tracks, danceability, playlist lengths, popularity)\n-Static plots:\npopularity vs. appearances\npopular songs by year\ndanceability over time\ndecade representation\nkey frequency (polar)\ntrack length distribution\nenergy vs. danceability\ntempo trends\n-Heuristic Functions (each keeping track_id):\nCo-occurrence on anchor playlists\nAudio-feature similarity\nSame-artist selection\nSame-era & feature similarity\nComplementary-key selection\nCandidate Combining & Final Curation\ncombine-candidates chunk — confirms ≥20 candidates & ≥8 hidden gems\ncreate-ultimate-playlist chunk — builds the 12-song “Harmonic Journey,” tags unknowns\n-Extra Credit\nanimated-visualization chunk — gganimate of danceability × energy over track position\ngenerate-html-viewer chunk — grid of Spotify embeds\n\n\nClick to view full project setup code\n# Setup environment\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(lubridate)\nlibrary(jsonlite)\nlibrary(purrr)\nlibrary(ggrepel)\nlibrary(viridis)\nlibrary(gganimate)\nlibrary(gifski)\n\n# Task 1: Song Characteristics Dataset\nload_songs &lt;- function() {\n  # Define target directory and file name\n  dest_dir &lt;- \"data/mp03\"\n  if (!dir.exists(dest_dir)) {\n    dir.create(dest_dir, recursive = TRUE)\n    message(\"Created directory: \", dest_dir)\n  }\n  \n  # Define destination file path\n  dest_file &lt;- file.path(dest_dir, \"spotify_data.csv\")\n  \n  # Download only if needed\n  if (!file.exists(dest_file)) {\n    spotify_url &lt;- \"https://raw.githubusercontent.com/gabminamedez/spotify-data/refs/heads/master/data.csv\"\n    download.file(url = spotify_url, destfile = dest_file, mode = \"wb\")\n    message(\"Downloaded Spotify song analytics dataset\")\n  } else {\n    message(\"Using existing Spotify song analytics dataset\")\n  }\n  \n  # Read and clean the data\n  songs &lt;- read.csv(dest_file, stringsAsFactors = FALSE)\n  \n  # Helper function to clean artist strings\n  clean_artist_string &lt;- function(x) {\n    str_replace_all(x, \"\\\\['\", \"\") %&gt;% \n      str_replace_all(\"'\\\\]\", \"\") %&gt;% \n      str_replace_all(\"', '\", \",\")\n  }\n  \n  # Process the songs data frame\n  songs_clean &lt;- songs %&gt;% \n    mutate(artists = clean_artist_string(artists)) %&gt;%\n    separate_rows(artists, sep = \",\") %&gt;%\n    mutate(artists = trimws(artists)) %&gt;%\n    rename(artist = artists)\n  \n  return(songs_clean)\n}\n\n# Task 2: Playlist Dataset\nload_playlists &lt;- function() {\n  # Define target directory\n  dest_dir &lt;- \"data/mp03/playlists\"\n  if (!dir.exists(dest_dir)) {\n    dir.create(dest_dir, recursive = TRUE)\n    message(\"Created directory: \", dest_dir)\n  }\n  \n  # Base GitHub URL for data\n  base_url &lt;- \"https://raw.githubusercontent.com/DevinOgrady/spotify_million_playlist_dataset/main/data1\"\n  \n  # Initialize empty list for playlists\n  all_playlists &lt;- list()\n  \n  # For demonstration purposes, we'll use a small subset of files\n  # In a real analysis, you'd process more files\n  for (i in seq(0, 2000, 1000)) {\n    # Construct filename programmatically\n    filename &lt;- sprintf(\"mpd.slice.%d-%d.json\", i, i + 999)\n    local_path &lt;- file.path(dest_dir, filename)\n    \n    # Download file if it doesn't exist\n    if (!file.exists(local_path)) {\n      file_url &lt;- paste0(base_url, \"/\", filename)\n      \n      tryCatch({\n        download.file(file_url, local_path, mode = \"wb\")\n        message(sprintf(\"Downloaded %s\", filename))\n        # Small delay to avoid overwhelming the server\n        Sys.sleep(0.5)\n      }, error = function(e) {\n        message(sprintf(\"Error downloading %s: %s\", filename, e$message))\n      })\n    } else {\n      message(sprintf(\"File %s already exists locally\", filename))\n    }\n    \n    # Read and process the JSON file if it exists\n    if (file.exists(local_path)) {\n      tryCatch({\n        playlist_data &lt;- fromJSON(local_path, simplifyDataFrame = FALSE)\n        \n        if (\"playlists\" %in% names(playlist_data) && is.list(playlist_data$playlists)) {\n          all_playlists &lt;- c(all_playlists, playlist_data$playlists)\n          message(sprintf(\"Processed %s with %d playlists\", \n                         filename, length(playlist_data$playlists)))\n        } else {\n          message(sprintf(\"File %s doesn't have the expected structure\", filename))\n        }\n      }, error = function(e) {\n        message(sprintf(\"Error loading %s: %s\", filename, e$message))\n      })\n    }\n  }\n  \n  return(all_playlists)\n}\n\n# Task 3: Rectangle the Playlist Data\nrectangle_playlists &lt;- function(playlists) {\n  # Initialize an empty data frame to store the results\n  result_df &lt;- data.frame()\n  \n  # Helper function to strip Spotify prefixes\n  strip_spotify_prefix &lt;- function(x) {\n    str_extract(x, \".*:.*:(.*)\", group = 1)\n  }\n  \n  # Process each playlist\n  for (i in seq_along(playlists)) {\n    playlist &lt;- playlists[[i]]\n    \n    # Extract playlist-level information\n    playlist_id &lt;- playlist$pid\n    playlist_name &lt;- playlist$name\n    playlist_followers &lt;- playlist$num_followers\n    \n    # Process each track in the playlist\n    if (length(playlist$tracks) &gt; 0) {\n      for (j in seq_along(playlist$tracks)) {\n        track &lt;- playlist$tracks[[j]]\n        \n        # Create a row for this track\n        track_row &lt;- data.frame(\n          playlist_id = playlist_id,\n          playlist_name = playlist_name,\n          playlist_followers = playlist_followers,\n          playlist_position = j,\n          artist_name = track$artist_name,\n          artist_id = strip_spotify_prefix(track$artist_uri),\n          track_name = track$track_name,\n          track_id = strip_spotify_prefix(track$track_uri),\n          album_name = track$album_name,\n          album_id = strip_spotify_prefix(track$album_uri),\n          duration = track$duration_ms,\n          stringsAsFactors = FALSE\n        )\n        \n        # Append to the result\n        result_df &lt;- rbind(result_df, track_row)\n      }\n    }\n  }\n  \n  return(result_df)\n}\n\n# Main execution code would follow here\n# For brevity, this is not included in the appendix\n\n\n\n\nClick to view visualization code\n# Example of a publication-quality visualization function\ncreate_feature_evolution_plot &lt;- function(playlist_data) {\n  # Prepare data\n  plot_data &lt;- playlist_data %&gt;%\n    select(position, track_name, artist_name, danceability, energy, tempo) %&gt;%\n    pivot_longer(\n      cols = c(danceability, energy, tempo),\n      names_to = \"feature\",\n      values_to = \"value\"\n    ) %&gt;%\n    # Normalize tempo to 0-1 scale for better comparison\n    mutate(value = ifelse(feature == \"tempo\", value / 200, value))\n  \n  # Create plot\n  ggplot(plot_data, aes(x = position, y = value, color = feature, group = feature)) +\n    geom_line(size = 1.2) +\n    geom_point(size = 3) +\n    scale_color_manual(values = c(\"danceability\" = \"#3498db\", \"energy\" = \"#e74c3c\", \"tempo\" = \"#2ecc71\")) +\n    labs(\n      title = \"Playlist Feature Evolution\",\n      subtitle = \"How audio characteristics flow throughout the playlist\",\n      x = \"Playlist Position\",\n      y = \"Feature Value (normalized)\",\n      color = \"Audio Feature\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(face = \"bold\", size = 16),\n      plot.subtitle = element_text(size = 12),\n      axis.title = element_text(face = \"bold\"),\n      legend.position = \"bottom\",\n      panel.grid.major = element_line(color = \"#bdc3c7\", linetype = \"dashed\")\n    )\n}\n\n# This function would be called with: create_feature_evolution_plot(ultimate_playlist)"
  },
  {
    "objectID": "mp03.html#final-thoughts",
    "href": "mp03.html#final-thoughts",
    "title": "STA 9750 Mini-Project #03: Creating the Ultimate Playlist",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nCreating the ultimate playlist requires both art and science. Through this mini-project, I’ve demonstrated how data analysis can enhance music curation by revealing patterns and relationships in audio features. The “Harmonic Journey” playlist exemplifies a balanced, data-driven approach to music selection, creating a cohesive listening experience that guides the listener through a carefully crafted sonic landscape.\nThe combination of objective metrics (audio features, popularity scores) with more subjective considerations (musical flow, thematic coherence) results in a playlist that’s both statistically sound and emotionally engaging. This approach has wide-ranging applications in music recommendation systems, content curation, and digital media strategy.\nMost importantly, this analysis shows how data science can enhance, rather than replace, human creativity—providing insights that inform artistic decisions and create better experiences for listeners. By animating our feature‐journey plot and embedding live Spotify players in the HTML viewer, we’ve turned a static report into an interactive, multimedia exploration of “Harmonic Journey.” This blend of rigorous analytics, music theory, and engaging presentation demonstrates the full potential of data‐driven curation in the digital age."
  }
]